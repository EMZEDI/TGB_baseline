{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Temporal Graph Benchmark","text":""},{"location":"#pip-install","title":"Pip Install","text":"<p>You can install TGB via pip <pre><code>pip install py-tgb\n</code></pre></p>"},{"location":"#links-and-datasets","title":"Links and Datasets","text":"<p>The project website can be found here.</p> <p>The API documentations can be found here.</p> <p>all dataset download links can be found at info.py</p> <p>TGB dataloader will also automatically download the dataset as well as the negative samples for the link property prediction datasets.</p>"},{"location":"#install-dependency","title":"Install dependency","text":"<p>Our implementation works with python &gt;= 3.9 and can be installed as follows</p> <ol> <li> <p>set up virtual environment (conda should work as well) <pre><code>python -m venv ~/tgb_env/\nsource ~/tgb_env/bin/activate\n</code></pre></p> </li> <li> <p>install external packages <pre><code>pip install pandas==1.5.3\npip install matplotlib==3.7.1\npip install clint==0.5.1\n</code></pre></p> </li> </ol> <p>install Pytorch and PyG dependencies (needed to run the examples) <pre><code>pip install torch==2.0.0 --index-url https://download.pytorch.org/whl/cu117\npip install torch_geometric==2.3.0\npip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.0.0+cu117.html\n</code></pre></p> <ol> <li>install local dependencies under root directory <code>/TGB</code> <pre><code>pip install -e .\n</code></pre></li> </ol>"},{"location":"#instruction-for-tracking-new-documentation-and-running-mkdocs-locally","title":"Instruction for tracking new documentation and running mkdocs locally","text":"<ol> <li> <p>first run the mkdocs server locally in your terminal  <pre><code>mkdocs serve\n</code></pre></p> </li> <li> <p>go to the local hosted web address similar to <pre><code>[14:18:13] Browser connected: http://127.0.0.1:8000/\n</code></pre></p> </li> </ol> <p>Example: to track documentation of a new hi.py file in tgb/edgeregression/hi.py</p> <ol> <li> <p>create docs/api/tgb.hi.md and add the following <pre><code># `tgb.edgeregression`\n\n::: tgb.edgeregression.hi\n</code></pre></p> </li> <li> <p>edit mkdocs.yml  <pre><code>nav:\n  - Overview: index.md\n  - About: about.md\n  - API:\n    other *.md files \n    - tgb.edgeregression: api/tgb.hi.md\n</code></pre></p> </li> </ol>"},{"location":"#creating-new-branch","title":"Creating new branch","text":"<pre><code>git fetch origin\n\ngit checkout -b test origin/test\n</code></pre>"},{"location":"#dependencies-for-mkdocs-documentation","title":"dependencies for mkdocs (documentation)","text":"<pre><code>pip install mkdocs\npip install mkdocs-material\npip install mkdocstrings-python\npip install mkdocs-jupyter\npip install notebook\n</code></pre>"},{"location":"#full-dependency-list","title":"full dependency list","text":"<p>Our implementation works with python &gt;= 3.9 and has the following dependencies <pre><code>pytorch == 2.0.0\ntorch-geometric == 2.3.0\ntorch-scatter==2.1.1\ntorch-sparse==0.6.17\ntorch-spline-conv==1.2.2\npandas==1.5.3\nclint==0.5.1\n</code></pre></p>"},{"location":"about/","title":"Temporal Graph Benchmark (TGB)","text":""},{"location":"about/#overview","title":"Overview","text":"<p>The TGB repo provides an automated ML pipeline for learning on a diverse set of temporal graph datasets:</p> <ul> <li> <p>automatic download of datasets from url</p> </li> <li> <p>processing the raw files into ML ready format</p> </li> <li> <p>support datasets in <code>numpy</code>, <code>Pytorch</code> and <code>PyG TemporalData</code> formats</p> </li> <li> <p>evaluation code for each dataset </p> </li> </ul>"},{"location":"api/tgb.linkproppred/","title":"<code>tgb.linkproppred</code>","text":"<p>Evaluator Module for Dynamic Link Prediction</p> <p>Sample negative edges for evaluation of dynamic link prediction Load already generated negative edges from file, batch them based on the positive edge, and return the evaluation set</p> <p>Sample and Generate negative edges that are going to be used for evaluation of a dynamic graph learning model Negative samples are generated and saved to files ONLY once;      other times, they should be loaded from file with instances of the <code>negative_sampler.py</code>.</p>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.dataset.LinkPropPredDataset","title":"<code>LinkPropPredDataset</code>","text":"<p>             Bases: <code>object</code></p> Source code in <code>tgb/linkproppred/dataset.py</code> <pre><code>class LinkPropPredDataset(object):\ndef __init__(\nself,\nname: str,\nroot: Optional[str] = \"datasets\",\nmeta_dict: Optional[dict] = None,\npreprocess: Optional[bool] = True,\n):\nr\"\"\"Dataset class for link prediction dataset. Stores meta information about each dataset such as evaluation metrics etc.\n        also automatically pre-processes the dataset.\n        Args:\n            name: name of the dataset\n            root: root directory to store the dataset folder\n            meta_dict: dictionary containing meta information about the dataset, should contain key 'dir_name' which is the name of the dataset folder\n            preprocess: whether to pre-process the dataset\n        \"\"\"\nself.name = name  ## original name\n# check if dataset url exist\nif self.name in DATA_URL_DICT:\nself.url = DATA_URL_DICT[self.name]\nelse:\nself.url = None\nprint(f\"Dataset {self.name} url not found, download not supported yet.\")\n# check if the evaluatioin metric are specified\nif self.name in DATA_EVAL_METRIC_DICT:\nself.metric = DATA_EVAL_METRIC_DICT[self.name]\nelse:\nself.metric = None\nprint(\nf\"Dataset {self.name} default evaluation metric not found, it is not supported yet.\"\n)\nroot = PROJ_DIR + root\nif meta_dict is None:\nself.dir_name = \"_\".join(name.split(\"-\"))  ## replace hyphen with underline\nmeta_dict = {\"dir_name\": self.dir_name}\nelse:\nself.dir_name = meta_dict[\"dir_name\"]\nself.root = osp.join(root, self.dir_name)\nself.meta_dict = meta_dict\nif \"fname\" not in self.meta_dict:\nself.meta_dict[\"fname\"] = self.root + \"/\" + self.name + \"_edgelist.csv\"\nself.meta_dict[\"nodefile\"] = None\nif name == \"tgbl-flight\":\nself.meta_dict[\"nodefile\"] = self.root + \"/\" + \"airport_node_feat.csv\"\nself.meta_dict[\"val_ns\"] = self.root + \"/\" + self.name + \"_val_ns.pkl\"\nself.meta_dict[\"test_ns\"] = self.root + \"/\" + self.name + \"_test_ns.pkl\"\n#! version check\nself.version_passed = True\nself._version_check()\n# initialize\nself._node_feat = None\nself._edge_feat = None\nself._full_data = None\nself._train_data = None\nself._val_data = None\nself._test_data = None\nself.download()\n# check if the root directory exists, if not create it\nif osp.isdir(self.root):\nprint(\"Dataset directory is \", self.root)\nelse:\n# os.makedirs(self.root)\nraise FileNotFoundError(f\"Directory not found at {self.root}\")\nif preprocess:\nself.pre_process()\nself.ns_sampler = NegativeEdgeSampler(\ndataset_name=self.name, strategy=\"hist_rnd\"\n)\ndef _version_check(self) -&gt; None:\nr\"\"\"Implement Version checks for dataset files\n        updates the file names based on the current version number\n        prompt the user to download the new version via self.version_passed variable\n        \"\"\"\nif (self.name in DATA_VERSION_DICT):\nversion = DATA_VERSION_DICT[self.name]\nelse:\nprint(f\"Dataset {self.name} version number not found.\")\nself.version_passed = False\nreturn None\nif (version &gt; 1):\n#* check if current version is outdated\nself.meta_dict[\"fname\"] = self.root + \"/\" + self.name + \"_edgelist_v\" + str(int(version)) + \".csv\"\nself.meta_dict[\"nodefile\"] = None\nif self.name == \"tgbl-flight\":\nself.meta_dict[\"nodefile\"] = self.root + \"/\" + \"airport_node_feat_v\" + str(int(version)) + \".csv\"\nself.meta_dict[\"val_ns\"] = self.root + \"/\" + self.name + \"_val_ns_v\" + str(int(version)) + \".pkl\"\nself.meta_dict[\"test_ns\"] = self.root + \"/\" + self.name + \"_test_ns_v\" + str(int(version)) + \".pkl\"\nif (not osp.exists(self.meta_dict[\"fname\"])):\nprint(f\"Dataset {self.name} version {int(version)} not found.\")\nprint(f\"Please download the latest version of the dataset.\")\nself.version_passed = False\nreturn None\ndef download(self):\n\"\"\"\n        downloads this dataset from url\n        check if files are already downloaded\n        \"\"\"\n# check if the file already exists\nif osp.exists(self.meta_dict[\"fname\"]):\nprint(\"raw file found, skipping download\")\nreturn\ninp = input(\n\"Will you download the dataset(s) now? (y/N)\\n\"\n).lower()  # ask if the user wants to download the dataset\nif inp == \"y\":\nprint(\nf\"{BColors.WARNING}Download started, this might take a while . . . {BColors.ENDC}\"\n)\nprint(f\"Dataset title: {self.name}\")\nif self.url is None:\nraise Exception(\"Dataset url not found, download not supported yet.\")\nelse:\nr = requests.get(self.url, stream=True)\n# download_dir = self.root + \"/\" + \"download\"\nif osp.isdir(self.root):\nprint(\"Dataset directory is \", self.root)\nelse:\nos.makedirs(self.root)\npath_download = self.root + \"/\" + self.name + \".zip\"\nwith open(path_download, \"wb\") as f:\ntotal_length = int(r.headers.get(\"content-length\"))\nfor chunk in progress.bar(\nr.iter_content(chunk_size=1024),\nexpected_size=(total_length / 1024) + 1,\n):\nif chunk:\nf.write(chunk)\nf.flush()\n# for unzipping the file\nwith zipfile.ZipFile(path_download, \"r\") as zip_ref:\nzip_ref.extractall(self.root)\nprint(f\"{BColors.OKGREEN}Download completed {BColors.ENDC}\")\nself.version_passed = True\nelse:\nraise Exception(\nBColors.FAIL + \"Data not found error, download \" + self.name + \" failed\"\n)\ndef generate_processed_files(self) -&gt; pd.DataFrame:\nr\"\"\"\n        turns raw data .csv file into a pandas data frame, stored on disc if not already\n        Returns:\n            df: pandas data frame\n        \"\"\"\nnode_feat = None\nif not osp.exists(self.meta_dict[\"fname\"]):\nraise FileNotFoundError(f\"File not found at {self.meta_dict['fname']}\")\nif self.meta_dict[\"nodefile\"] is not None:\nif not osp.exists(self.meta_dict[\"nodefile\"]):\nraise FileNotFoundError(\nf\"File not found at {self.meta_dict['nodefile']}\"\n)\nOUT_DF = self.root + \"/\" + \"ml_{}.pkl\".format(self.name)\nOUT_EDGE_FEAT = self.root + \"/\" + \"ml_{}.pkl\".format(self.name + \"_edge\")\nif self.meta_dict[\"nodefile\"] is not None:\nOUT_NODE_FEAT = self.root + \"/\" + \"ml_{}.pkl\".format(self.name + \"_node\")\nif (osp.exists(OUT_DF)) and (self.version_passed is True):\nprint(\"loading processed file\")\ndf = pd.read_pickle(OUT_DF)\nedge_feat = load_pkl(OUT_EDGE_FEAT)\nif self.meta_dict[\"nodefile\"] is not None:\nnode_feat = load_pkl(OUT_NODE_FEAT)\nelse:\nprint(\"file not processed, generating processed file\")\nif self.name == \"tgbl-flight\":\ndf, edge_feat, node_ids = csv_to_pd_data(self.meta_dict[\"fname\"])\nelif self.name == \"tgbl-coin\":\ndf, edge_feat, node_ids = csv_to_pd_data_sc(self.meta_dict[\"fname\"])\nelif self.name == \"tgbl-comment\":\ndf, edge_feat, node_ids = csv_to_pd_data_rc(self.meta_dict[\"fname\"])\nelif self.name == \"tgbl-review\":\ndf, edge_feat, node_ids = csv_to_pd_data_sc(self.meta_dict[\"fname\"])\nelif self.name == \"tgbl-wiki\":\ndf, edge_feat, node_ids = load_edgelist_wiki(self.meta_dict[\"fname\"])\nsave_pkl(edge_feat, OUT_EDGE_FEAT)\ndf.to_pickle(OUT_DF)\nif self.meta_dict[\"nodefile\"] is not None:\nnode_feat = process_node_feat(self.meta_dict[\"nodefile\"], node_ids)\nsave_pkl(node_feat, OUT_NODE_FEAT)\nreturn df, edge_feat, node_feat\ndef pre_process(self):\n\"\"\"\n        Pre-process the dataset and generates the splits, must be run before dataset properties can be accessed\n        generates the edge data and different train, val, test splits\n        \"\"\"\n# TODO for link prediction, y =1 because these are all true edges, edge feat = weight + edge feat\n# check if path to file is valid\ndf, edge_feat, node_feat = self.generate_processed_files()\nsources = np.array(df[\"u\"])\ndestinations = np.array(df[\"i\"])\ntimestamps = np.array(df[\"ts\"])\nedge_idxs = np.array(df[\"idx\"])\nweights = np.array(df[\"w\"])\nedge_label = np.ones(len(df))  # should be 1 for all pos edges\nself._edge_feat = edge_feat\nself._node_feat = node_feat\nfull_data = {\n\"sources\": sources,\n\"destinations\": destinations,\n\"timestamps\": timestamps,\n\"edge_idxs\": edge_idxs,\n\"edge_feat\": edge_feat,\n\"w\": weights,\n\"edge_label\": edge_label,\n}\nself._full_data = full_data\n_train_mask, _val_mask, _test_mask = self.generate_splits(full_data)\nself._train_mask = _train_mask\nself._val_mask = _val_mask\nself._test_mask = _test_mask\ndef generate_splits(\nself,\nfull_data: Dict[str, Any],\nval_ratio: float = 0.15,\ntest_ratio: float = 0.15,\n) -&gt; Tuple[Dict[str, Any], Dict[str, Any], Dict[str, Any]]:\nr\"\"\"Generates train, validation, and test splits from the full dataset\n        Args:\n            full_data: dictionary containing the full dataset\n            val_ratio: ratio of validation data\n            test_ratio: ratio of test data\n        Returns:\n            train_data: dictionary containing the training dataset\n            val_data: dictionary containing the validation dataset\n            test_data: dictionary containing the test dataset\n        \"\"\"\nval_time, test_time = list(\nnp.quantile(\nfull_data[\"timestamps\"],\n[(1 - val_ratio - test_ratio), (1 - test_ratio)],\n)\n)\ntimestamps = full_data[\"timestamps\"]\ntrain_mask = timestamps &lt;= val_time\nval_mask = np.logical_and(timestamps &lt;= test_time, timestamps &gt; val_time)\ntest_mask = timestamps &gt; test_time\nreturn train_mask, val_mask, test_mask\n@property\ndef eval_metric(self) -&gt; str:\n\"\"\"\n        the official evaluation metric for the dataset, loaded from info.py\n        Returns:\n            eval_metric: str, the evaluation metric\n        \"\"\"\nreturn self.metric\n@property\ndef negative_sampler(self) -&gt; NegativeEdgeSampler:\nr\"\"\"\n        Returns the negative sampler of the dataset, will load negative samples from disc\n        Returns:\n            negative_sampler: NegativeEdgeSampler\n        \"\"\"\nreturn self.ns_sampler\ndef load_val_ns(self) -&gt; None:\nr\"\"\"\n        load the negative samples for the validation set\n        \"\"\"\nself.ns_sampler.load_eval_set(\nfname=self.meta_dict[\"val_ns\"], split_mode=\"val\"\n)\ndef load_test_ns(self) -&gt; None:\nr\"\"\"\n        load the negative samples for the test set\n        \"\"\"\nself.ns_sampler.load_eval_set(\nfname=self.meta_dict[\"test_ns\"], split_mode=\"test\"\n)\n@property\ndef node_feat(self) -&gt; Optional[np.ndarray]:\nr\"\"\"\n        Returns the node features of the dataset with dim [N, feat_dim]\n        Returns:\n            node_feat: np.ndarray, [N, feat_dim] or None if there is no node feature\n        \"\"\"\nreturn self._node_feat\n@property\ndef edge_feat(self) -&gt; Optional[np.ndarray]:\nr\"\"\"\n        Returns the edge features of the dataset with dim [E, feat_dim]\n        Returns:\n            edge_feat: np.ndarray, [E, feat_dim] or None if there is no edge feature\n        \"\"\"\nreturn self._edge_feat\n@property\ndef full_data(self) -&gt; Dict[str, Any]:\nr\"\"\"\n        the full data of the dataset as a dictionary with keys: 'sources', 'destinations', 'timestamps', 'edge_idxs', 'edge_feat', 'w', 'edge_label',\n        Returns:\n            full_data: Dict[str, Any]\n        \"\"\"\nif self._full_data is None:\nraise ValueError(\n\"dataset has not been processed yet, please call pre_process() first\"\n)\nreturn self._full_data\n@property\ndef train_mask(self) -&gt; np.ndarray:\nr\"\"\"\n        Returns the train mask of the dataset\n        Returns:\n            train_mask: training masks\n        \"\"\"\nif self._train_mask is None:\nraise ValueError(\"training split hasn't been loaded\")\nreturn self._train_mask\n@property\ndef val_mask(self) -&gt; np.ndarray:\nr\"\"\"\n        Returns the validation mask of the dataset\n        Returns:\n            val_mask: Dict[str, Any]\n        \"\"\"\nif self._val_mask is None:\nraise ValueError(\"validation split hasn't been loaded\")\nreturn self._val_mask\n@property\ndef test_mask(self) -&gt; np.ndarray:\nr\"\"\"\n        Returns the test mask of the dataset:\n        Returns:\n            test_mask: Dict[str, Any]\n        \"\"\"\nif self._test_mask is None:\nraise ValueError(\"test split hasn't been loaded\")\nreturn self._test_mask\n</code></pre>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.dataset.LinkPropPredDataset.edge_feat","title":"<code>edge_feat: Optional[np.ndarray]</code>  <code>property</code>","text":"<p>Returns the edge features of the dataset with dim [E, feat_dim]</p> <p>Returns:</p> Name Type Description <code>edge_feat</code> <code>Optional[np.ndarray]</code> <p>np.ndarray, [E, feat_dim] or None if there is no edge feature</p>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.dataset.LinkPropPredDataset.eval_metric","title":"<code>eval_metric: str</code>  <code>property</code>","text":"<p>the official evaluation metric for the dataset, loaded from info.py</p> <p>Returns:</p> Name Type Description <code>eval_metric</code> <code>str</code> <p>str, the evaluation metric</p>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.dataset.LinkPropPredDataset.full_data","title":"<code>full_data: Dict[str, Any]</code>  <code>property</code>","text":"<p>the full data of the dataset as a dictionary with keys: 'sources', 'destinations', 'timestamps', 'edge_idxs', 'edge_feat', 'w', 'edge_label',</p> <p>Returns:</p> Name Type Description <code>full_data</code> <code>Dict[str, Any]</code> <p>Dict[str, Any]</p>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.dataset.LinkPropPredDataset.negative_sampler","title":"<code>negative_sampler: NegativeEdgeSampler</code>  <code>property</code>","text":"<p>Returns the negative sampler of the dataset, will load negative samples from disc</p> <p>Returns:</p> Name Type Description <code>negative_sampler</code> <code>NegativeEdgeSampler</code> <p>NegativeEdgeSampler</p>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.dataset.LinkPropPredDataset.node_feat","title":"<code>node_feat: Optional[np.ndarray]</code>  <code>property</code>","text":"<p>Returns the node features of the dataset with dim [N, feat_dim]</p> <p>Returns:</p> Name Type Description <code>node_feat</code> <code>Optional[np.ndarray]</code> <p>np.ndarray, [N, feat_dim] or None if there is no node feature</p>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.dataset.LinkPropPredDataset.test_mask","title":"<code>test_mask: np.ndarray</code>  <code>property</code>","text":"<p>Returns the test mask of the dataset:</p> <p>Returns:</p> Name Type Description <code>test_mask</code> <code>np.ndarray</code> <p>Dict[str, Any]</p>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.dataset.LinkPropPredDataset.train_mask","title":"<code>train_mask: np.ndarray</code>  <code>property</code>","text":"<p>Returns the train mask of the dataset</p> <p>Returns:</p> Name Type Description <code>train_mask</code> <code>np.ndarray</code> <p>training masks</p>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.dataset.LinkPropPredDataset.val_mask","title":"<code>val_mask: np.ndarray</code>  <code>property</code>","text":"<p>Returns the validation mask of the dataset</p> <p>Returns:</p> Name Type Description <code>val_mask</code> <code>np.ndarray</code> <p>Dict[str, Any]</p>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.dataset.LinkPropPredDataset.__init__","title":"<code>__init__(name, root='datasets', meta_dict=None, preprocess=True)</code>","text":"<p>Dataset class for link prediction dataset. Stores meta information about each dataset such as evaluation metrics etc. also automatically pre-processes the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>name of the dataset</p> required <code>root</code> <code>Optional[str]</code> <p>root directory to store the dataset folder</p> <code>'datasets'</code> <code>meta_dict</code> <code>Optional[dict]</code> <p>dictionary containing meta information about the dataset, should contain key 'dir_name' which is the name of the dataset folder</p> <code>None</code> <code>preprocess</code> <code>Optional[bool]</code> <p>whether to pre-process the dataset</p> <code>True</code> Source code in <code>tgb/linkproppred/dataset.py</code> <pre><code>def __init__(\nself,\nname: str,\nroot: Optional[str] = \"datasets\",\nmeta_dict: Optional[dict] = None,\npreprocess: Optional[bool] = True,\n):\nr\"\"\"Dataset class for link prediction dataset. Stores meta information about each dataset such as evaluation metrics etc.\n    also automatically pre-processes the dataset.\n    Args:\n        name: name of the dataset\n        root: root directory to store the dataset folder\n        meta_dict: dictionary containing meta information about the dataset, should contain key 'dir_name' which is the name of the dataset folder\n        preprocess: whether to pre-process the dataset\n    \"\"\"\nself.name = name  ## original name\n# check if dataset url exist\nif self.name in DATA_URL_DICT:\nself.url = DATA_URL_DICT[self.name]\nelse:\nself.url = None\nprint(f\"Dataset {self.name} url not found, download not supported yet.\")\n# check if the evaluatioin metric are specified\nif self.name in DATA_EVAL_METRIC_DICT:\nself.metric = DATA_EVAL_METRIC_DICT[self.name]\nelse:\nself.metric = None\nprint(\nf\"Dataset {self.name} default evaluation metric not found, it is not supported yet.\"\n)\nroot = PROJ_DIR + root\nif meta_dict is None:\nself.dir_name = \"_\".join(name.split(\"-\"))  ## replace hyphen with underline\nmeta_dict = {\"dir_name\": self.dir_name}\nelse:\nself.dir_name = meta_dict[\"dir_name\"]\nself.root = osp.join(root, self.dir_name)\nself.meta_dict = meta_dict\nif \"fname\" not in self.meta_dict:\nself.meta_dict[\"fname\"] = self.root + \"/\" + self.name + \"_edgelist.csv\"\nself.meta_dict[\"nodefile\"] = None\nif name == \"tgbl-flight\":\nself.meta_dict[\"nodefile\"] = self.root + \"/\" + \"airport_node_feat.csv\"\nself.meta_dict[\"val_ns\"] = self.root + \"/\" + self.name + \"_val_ns.pkl\"\nself.meta_dict[\"test_ns\"] = self.root + \"/\" + self.name + \"_test_ns.pkl\"\n#! version check\nself.version_passed = True\nself._version_check()\n# initialize\nself._node_feat = None\nself._edge_feat = None\nself._full_data = None\nself._train_data = None\nself._val_data = None\nself._test_data = None\nself.download()\n# check if the root directory exists, if not create it\nif osp.isdir(self.root):\nprint(\"Dataset directory is \", self.root)\nelse:\n# os.makedirs(self.root)\nraise FileNotFoundError(f\"Directory not found at {self.root}\")\nif preprocess:\nself.pre_process()\nself.ns_sampler = NegativeEdgeSampler(\ndataset_name=self.name, strategy=\"hist_rnd\"\n)\n</code></pre>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.dataset.LinkPropPredDataset.download","title":"<code>download()</code>","text":"<p>downloads this dataset from url check if files are already downloaded</p> Source code in <code>tgb/linkproppred/dataset.py</code> <pre><code>def download(self):\n\"\"\"\n    downloads this dataset from url\n    check if files are already downloaded\n    \"\"\"\n# check if the file already exists\nif osp.exists(self.meta_dict[\"fname\"]):\nprint(\"raw file found, skipping download\")\nreturn\ninp = input(\n\"Will you download the dataset(s) now? (y/N)\\n\"\n).lower()  # ask if the user wants to download the dataset\nif inp == \"y\":\nprint(\nf\"{BColors.WARNING}Download started, this might take a while . . . {BColors.ENDC}\"\n)\nprint(f\"Dataset title: {self.name}\")\nif self.url is None:\nraise Exception(\"Dataset url not found, download not supported yet.\")\nelse:\nr = requests.get(self.url, stream=True)\n# download_dir = self.root + \"/\" + \"download\"\nif osp.isdir(self.root):\nprint(\"Dataset directory is \", self.root)\nelse:\nos.makedirs(self.root)\npath_download = self.root + \"/\" + self.name + \".zip\"\nwith open(path_download, \"wb\") as f:\ntotal_length = int(r.headers.get(\"content-length\"))\nfor chunk in progress.bar(\nr.iter_content(chunk_size=1024),\nexpected_size=(total_length / 1024) + 1,\n):\nif chunk:\nf.write(chunk)\nf.flush()\n# for unzipping the file\nwith zipfile.ZipFile(path_download, \"r\") as zip_ref:\nzip_ref.extractall(self.root)\nprint(f\"{BColors.OKGREEN}Download completed {BColors.ENDC}\")\nself.version_passed = True\nelse:\nraise Exception(\nBColors.FAIL + \"Data not found error, download \" + self.name + \" failed\"\n)\n</code></pre>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.dataset.LinkPropPredDataset.generate_processed_files","title":"<code>generate_processed_files()</code>","text":"<p>turns raw data .csv file into a pandas data frame, stored on disc if not already</p> <p>Returns:</p> Name Type Description <code>df</code> <code>pd.DataFrame</code> <p>pandas data frame</p> Source code in <code>tgb/linkproppred/dataset.py</code> <pre><code>def generate_processed_files(self) -&gt; pd.DataFrame:\nr\"\"\"\n    turns raw data .csv file into a pandas data frame, stored on disc if not already\n    Returns:\n        df: pandas data frame\n    \"\"\"\nnode_feat = None\nif not osp.exists(self.meta_dict[\"fname\"]):\nraise FileNotFoundError(f\"File not found at {self.meta_dict['fname']}\")\nif self.meta_dict[\"nodefile\"] is not None:\nif not osp.exists(self.meta_dict[\"nodefile\"]):\nraise FileNotFoundError(\nf\"File not found at {self.meta_dict['nodefile']}\"\n)\nOUT_DF = self.root + \"/\" + \"ml_{}.pkl\".format(self.name)\nOUT_EDGE_FEAT = self.root + \"/\" + \"ml_{}.pkl\".format(self.name + \"_edge\")\nif self.meta_dict[\"nodefile\"] is not None:\nOUT_NODE_FEAT = self.root + \"/\" + \"ml_{}.pkl\".format(self.name + \"_node\")\nif (osp.exists(OUT_DF)) and (self.version_passed is True):\nprint(\"loading processed file\")\ndf = pd.read_pickle(OUT_DF)\nedge_feat = load_pkl(OUT_EDGE_FEAT)\nif self.meta_dict[\"nodefile\"] is not None:\nnode_feat = load_pkl(OUT_NODE_FEAT)\nelse:\nprint(\"file not processed, generating processed file\")\nif self.name == \"tgbl-flight\":\ndf, edge_feat, node_ids = csv_to_pd_data(self.meta_dict[\"fname\"])\nelif self.name == \"tgbl-coin\":\ndf, edge_feat, node_ids = csv_to_pd_data_sc(self.meta_dict[\"fname\"])\nelif self.name == \"tgbl-comment\":\ndf, edge_feat, node_ids = csv_to_pd_data_rc(self.meta_dict[\"fname\"])\nelif self.name == \"tgbl-review\":\ndf, edge_feat, node_ids = csv_to_pd_data_sc(self.meta_dict[\"fname\"])\nelif self.name == \"tgbl-wiki\":\ndf, edge_feat, node_ids = load_edgelist_wiki(self.meta_dict[\"fname\"])\nsave_pkl(edge_feat, OUT_EDGE_FEAT)\ndf.to_pickle(OUT_DF)\nif self.meta_dict[\"nodefile\"] is not None:\nnode_feat = process_node_feat(self.meta_dict[\"nodefile\"], node_ids)\nsave_pkl(node_feat, OUT_NODE_FEAT)\nreturn df, edge_feat, node_feat\n</code></pre>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.dataset.LinkPropPredDataset.generate_splits","title":"<code>generate_splits(full_data, val_ratio=0.15, test_ratio=0.15)</code>","text":"<p>Generates train, validation, and test splits from the full dataset</p> <p>Parameters:</p> Name Type Description Default <code>full_data</code> <code>Dict[str, Any]</code> <p>dictionary containing the full dataset</p> required <code>val_ratio</code> <code>float</code> <p>ratio of validation data</p> <code>0.15</code> <code>test_ratio</code> <code>float</code> <p>ratio of test data</p> <code>0.15</code> <p>Returns:</p> Name Type Description <code>train_data</code> <code>Dict[str, Any]</code> <p>dictionary containing the training dataset</p> <code>val_data</code> <code>Dict[str, Any]</code> <p>dictionary containing the validation dataset</p> <code>test_data</code> <code>Dict[str, Any]</code> <p>dictionary containing the test dataset</p> Source code in <code>tgb/linkproppred/dataset.py</code> <pre><code>def generate_splits(\nself,\nfull_data: Dict[str, Any],\nval_ratio: float = 0.15,\ntest_ratio: float = 0.15,\n) -&gt; Tuple[Dict[str, Any], Dict[str, Any], Dict[str, Any]]:\nr\"\"\"Generates train, validation, and test splits from the full dataset\n    Args:\n        full_data: dictionary containing the full dataset\n        val_ratio: ratio of validation data\n        test_ratio: ratio of test data\n    Returns:\n        train_data: dictionary containing the training dataset\n        val_data: dictionary containing the validation dataset\n        test_data: dictionary containing the test dataset\n    \"\"\"\nval_time, test_time = list(\nnp.quantile(\nfull_data[\"timestamps\"],\n[(1 - val_ratio - test_ratio), (1 - test_ratio)],\n)\n)\ntimestamps = full_data[\"timestamps\"]\ntrain_mask = timestamps &lt;= val_time\nval_mask = np.logical_and(timestamps &lt;= test_time, timestamps &gt; val_time)\ntest_mask = timestamps &gt; test_time\nreturn train_mask, val_mask, test_mask\n</code></pre>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.dataset.LinkPropPredDataset.load_test_ns","title":"<code>load_test_ns()</code>","text":"<p>load the negative samples for the test set</p> Source code in <code>tgb/linkproppred/dataset.py</code> <pre><code>def load_test_ns(self) -&gt; None:\nr\"\"\"\n    load the negative samples for the test set\n    \"\"\"\nself.ns_sampler.load_eval_set(\nfname=self.meta_dict[\"test_ns\"], split_mode=\"test\"\n)\n</code></pre>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.dataset.LinkPropPredDataset.load_val_ns","title":"<code>load_val_ns()</code>","text":"<p>load the negative samples for the validation set</p> Source code in <code>tgb/linkproppred/dataset.py</code> <pre><code>def load_val_ns(self) -&gt; None:\nr\"\"\"\n    load the negative samples for the validation set\n    \"\"\"\nself.ns_sampler.load_eval_set(\nfname=self.meta_dict[\"val_ns\"], split_mode=\"val\"\n)\n</code></pre>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.dataset.LinkPropPredDataset.pre_process","title":"<code>pre_process()</code>","text":"<p>Pre-process the dataset and generates the splits, must be run before dataset properties can be accessed generates the edge data and different train, val, test splits</p> Source code in <code>tgb/linkproppred/dataset.py</code> <pre><code>def pre_process(self):\n\"\"\"\n    Pre-process the dataset and generates the splits, must be run before dataset properties can be accessed\n    generates the edge data and different train, val, test splits\n    \"\"\"\n# TODO for link prediction, y =1 because these are all true edges, edge feat = weight + edge feat\n# check if path to file is valid\ndf, edge_feat, node_feat = self.generate_processed_files()\nsources = np.array(df[\"u\"])\ndestinations = np.array(df[\"i\"])\ntimestamps = np.array(df[\"ts\"])\nedge_idxs = np.array(df[\"idx\"])\nweights = np.array(df[\"w\"])\nedge_label = np.ones(len(df))  # should be 1 for all pos edges\nself._edge_feat = edge_feat\nself._node_feat = node_feat\nfull_data = {\n\"sources\": sources,\n\"destinations\": destinations,\n\"timestamps\": timestamps,\n\"edge_idxs\": edge_idxs,\n\"edge_feat\": edge_feat,\n\"w\": weights,\n\"edge_label\": edge_label,\n}\nself._full_data = full_data\n_train_mask, _val_mask, _test_mask = self.generate_splits(full_data)\nself._train_mask = _train_mask\nself._val_mask = _val_mask\nself._test_mask = _test_mask\n</code></pre>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.dataset_pyg.PyGLinkPropPredDataset","title":"<code>PyGLinkPropPredDataset</code>","text":"<p>             Bases: <code>Dataset</code></p> Source code in <code>tgb/linkproppred/dataset_pyg.py</code> <pre><code>class PyGLinkPropPredDataset(Dataset):\ndef __init__(\nself,\nname: str,\nroot: str,\ntransform: Optional[Callable] = None,\npre_transform: Optional[Callable] = None,\n):\nr\"\"\"\n        PyG wrapper for the LinkPropPredDataset\n        can return pytorch tensors for src,dst,t,msg,label\n        can return Temporal Data object\n        Parameters:\n            name: name of the dataset, passed to `LinkPropPredDataset`\n            root (string): Root directory where the dataset should be saved, passed to `LinkPropPredDataset`\n            transform (callable, optional): A function/transform that takes in an, not used in this case\n            pre_transform (callable, optional): A function/transform that takes in, not used in this case\n        \"\"\"\nself.name = name\nself.root = root\nself.dataset = LinkPropPredDataset(name=name, root=root)\nself._train_mask = torch.from_numpy(self.dataset.train_mask)\nself._val_mask = torch.from_numpy(self.dataset.val_mask)\nself._test_mask = torch.from_numpy(self.dataset.test_mask)\nsuper().__init__(root, transform, pre_transform)\nself._node_feat = self.dataset.node_feat\nif self._node_feat is None:\nself._node_feat = None\nelse:\nself._node_feat = torch.from_numpy(self._node_feat).float()\nself.process_data()\nself._ns_sampler = self.dataset.negative_sampler\n@property\ndef eval_metric(self) -&gt; str:\n\"\"\"\n        the official evaluation metric for the dataset, loaded from info.py\n        Returns:\n            eval_metric: str, the evaluation metric\n        \"\"\"\nreturn self.dataset.eval_metric\n@property\ndef negative_sampler(self) -&gt; NegativeEdgeSampler:\nr\"\"\"\n        Returns the negative sampler of the dataset, will load negative samples from disc\n        Returns:\n            negative_sampler: NegativeEdgeSampler\n        \"\"\"\nreturn self._ns_sampler\ndef load_val_ns(self) -&gt; None:\nr\"\"\"\n        load the negative samples for the validation set\n        \"\"\"\nself.dataset.load_val_ns()\ndef load_test_ns(self) -&gt; None:\nr\"\"\"\n        load the negative samples for the test set\n        \"\"\"\nself.dataset.load_test_ns()\n@property\ndef train_mask(self) -&gt; torch.Tensor:\nr\"\"\"\n        Returns the train mask of the dataset\n        Returns:\n            train_mask: the mask for edges in the training set\n        \"\"\"\nif self._train_mask is None:\nraise ValueError(\"training split hasn't been loaded\")\nreturn self._train_mask\n@property\ndef val_mask(self) -&gt; torch.Tensor:\nr\"\"\"\n        Returns the validation mask of the dataset\n        Returns:\n            val_mask: the mask for edges in the validation set\n        \"\"\"\nif self._val_mask is None:\nraise ValueError(\"validation split hasn't been loaded\")\nreturn self._val_mask\n@property\ndef test_mask(self) -&gt; torch.Tensor:\nr\"\"\"\n        Returns the test mask of the dataset:\n        Returns:\n            test_mask: the mask for edges in the test set\n        \"\"\"\nif self._test_mask is None:\nraise ValueError(\"test split hasn't been loaded\")\nreturn self._test_mask\n@property\ndef node_feat(self) -&gt; torch.Tensor:\nr\"\"\"\n        Returns the node features of the dataset\n        Returns:\n            node_feat: the node features\n        \"\"\"\nreturn self._node_feat\n@property\ndef src(self) -&gt; torch.Tensor:\nr\"\"\"\n        Returns the source nodes of the dataset\n        Returns:\n            src: the idx of the source nodes\n        \"\"\"\nreturn self._src\n@property\ndef dst(self) -&gt; torch.Tensor:\nr\"\"\"\n        Returns the destination nodes of the dataset\n        Returns:\n            dst: the idx of the destination nodes\n        \"\"\"\nreturn self._dst\n@property\ndef ts(self) -&gt; torch.Tensor:\nr\"\"\"\n        Returns the timestamps of the dataset\n        Returns:\n            ts: the timestamps of the edges\n        \"\"\"\nreturn self._ts\n@property\ndef edge_feat(self) -&gt; torch.Tensor:\nr\"\"\"\n        Returns the edge features of the dataset\n        Returns:\n            edge_feat: the edge features\n        \"\"\"\nreturn self._edge_feat\n@property\ndef edge_label(self) -&gt; torch.Tensor:\nr\"\"\"\n        Returns the edge labels of the dataset\n        Returns:\n            edge_label: the labels of the edges\n        \"\"\"\nreturn self._edge_label\ndef process_data(self) -&gt; None:\nr\"\"\"\n        convert the numpy arrays from dataset to pytorch tensors\n        \"\"\"\nsrc = torch.from_numpy(self.dataset.full_data[\"sources\"])\ndst = torch.from_numpy(self.dataset.full_data[\"destinations\"])\nts = torch.from_numpy(self.dataset.full_data[\"timestamps\"])\nmsg = torch.from_numpy(\nself.dataset.full_data[\"edge_feat\"]\n)  # use edge features here if available\nedge_label = torch.from_numpy(\nself.dataset.full_data[\"edge_label\"]\n)  # this is the label indicating if an edge is a true edge, always 1 for true edges\n# * first check typing for all tensors\n# source tensor must be of type int64\n# warnings.warn(\"sources tensor is not of type int64 or int32, forcing conversion\")\nif src.dtype != torch.int64:\nsrc = src.long()\n# destination tensor must be of type int64\nif dst.dtype != torch.int64:\ndst = dst.long()\n# timestamp tensor must be of type int64\nif ts.dtype != torch.int64:\nts = ts.long()\n# message tensor must be of type float32\nif msg.dtype != torch.float32:\nmsg = msg.float()\nself._src = src\nself._dst = dst\nself._ts = ts\nself._edge_label = edge_label\nself._edge_feat = msg\ndef get_TemporalData(self) -&gt; TemporalData:\n\"\"\"\n        return the TemporalData object for the entire dataset\n        \"\"\"\ndata = TemporalData(\nsrc=self._src,\ndst=self._dst,\nt=self._ts,\nmsg=self._edge_feat,\ny=self._edge_label,\n)\nreturn data\ndef len(self) -&gt; int:\n\"\"\"\n        size of the dataset\n        Returns:\n            size: int\n        \"\"\"\nreturn self._src.shape[0]\ndef get(self, idx: int) -&gt; TemporalData:\n\"\"\"\n        construct temporal data object for a single edge\n        Parameters:\n            idx: index of the edge\n        Returns:\n            data: TemporalData object\n        \"\"\"\ndata = TemporalData(\nsrc=self._src[idx],\ndst=self._dst[idx],\nt=self._ts[idx],\nmsg=self._edge_feat[idx],\ny=self._edge_label[idx],\n)\nreturn data\ndef __repr__(self) -&gt; str:\nreturn f\"{self.name.capitalize()}()\"\n</code></pre>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.dataset_pyg.PyGLinkPropPredDataset.dst","title":"<code>dst: torch.Tensor</code>  <code>property</code>","text":"<p>Returns the destination nodes of the dataset</p> <p>Returns:</p> Name Type Description <code>dst</code> <code>torch.Tensor</code> <p>the idx of the destination nodes</p>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.dataset_pyg.PyGLinkPropPredDataset.edge_feat","title":"<code>edge_feat: torch.Tensor</code>  <code>property</code>","text":"<p>Returns the edge features of the dataset</p> <p>Returns:</p> Name Type Description <code>edge_feat</code> <code>torch.Tensor</code> <p>the edge features</p>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.dataset_pyg.PyGLinkPropPredDataset.edge_label","title":"<code>edge_label: torch.Tensor</code>  <code>property</code>","text":"<p>Returns the edge labels of the dataset</p> <p>Returns:</p> Name Type Description <code>edge_label</code> <code>torch.Tensor</code> <p>the labels of the edges</p>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.dataset_pyg.PyGLinkPropPredDataset.eval_metric","title":"<code>eval_metric: str</code>  <code>property</code>","text":"<p>the official evaluation metric for the dataset, loaded from info.py</p> <p>Returns:</p> Name Type Description <code>eval_metric</code> <code>str</code> <p>str, the evaluation metric</p>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.dataset_pyg.PyGLinkPropPredDataset.negative_sampler","title":"<code>negative_sampler: NegativeEdgeSampler</code>  <code>property</code>","text":"<p>Returns the negative sampler of the dataset, will load negative samples from disc</p> <p>Returns:</p> Name Type Description <code>negative_sampler</code> <code>NegativeEdgeSampler</code> <p>NegativeEdgeSampler</p>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.dataset_pyg.PyGLinkPropPredDataset.node_feat","title":"<code>node_feat: torch.Tensor</code>  <code>property</code>","text":"<p>Returns the node features of the dataset</p> <p>Returns:</p> Name Type Description <code>node_feat</code> <code>torch.Tensor</code> <p>the node features</p>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.dataset_pyg.PyGLinkPropPredDataset.src","title":"<code>src: torch.Tensor</code>  <code>property</code>","text":"<p>Returns the source nodes of the dataset</p> <p>Returns:</p> Name Type Description <code>src</code> <code>torch.Tensor</code> <p>the idx of the source nodes</p>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.dataset_pyg.PyGLinkPropPredDataset.test_mask","title":"<code>test_mask: torch.Tensor</code>  <code>property</code>","text":"<p>Returns the test mask of the dataset:</p> <p>Returns:</p> Name Type Description <code>test_mask</code> <code>torch.Tensor</code> <p>the mask for edges in the test set</p>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.dataset_pyg.PyGLinkPropPredDataset.train_mask","title":"<code>train_mask: torch.Tensor</code>  <code>property</code>","text":"<p>Returns the train mask of the dataset</p> <p>Returns:</p> Name Type Description <code>train_mask</code> <code>torch.Tensor</code> <p>the mask for edges in the training set</p>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.dataset_pyg.PyGLinkPropPredDataset.ts","title":"<code>ts: torch.Tensor</code>  <code>property</code>","text":"<p>Returns the timestamps of the dataset</p> <p>Returns:</p> Name Type Description <code>ts</code> <code>torch.Tensor</code> <p>the timestamps of the edges</p>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.dataset_pyg.PyGLinkPropPredDataset.val_mask","title":"<code>val_mask: torch.Tensor</code>  <code>property</code>","text":"<p>Returns the validation mask of the dataset</p> <p>Returns:</p> Name Type Description <code>val_mask</code> <code>torch.Tensor</code> <p>the mask for edges in the validation set</p>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.dataset_pyg.PyGLinkPropPredDataset.__init__","title":"<code>__init__(name, root, transform=None, pre_transform=None)</code>","text":"<p>PyG wrapper for the LinkPropPredDataset can return pytorch tensors for src,dst,t,msg,label can return Temporal Data object</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>name of the dataset, passed to <code>LinkPropPredDataset</code></p> required <code>root</code> <code>string</code> <p>Root directory where the dataset should be saved, passed to <code>LinkPropPredDataset</code></p> required <code>transform</code> <code>callable</code> <p>A function/transform that takes in an, not used in this case</p> <code>None</code> <code>pre_transform</code> <code>callable</code> <p>A function/transform that takes in, not used in this case</p> <code>None</code> Source code in <code>tgb/linkproppred/dataset_pyg.py</code> <pre><code>def __init__(\nself,\nname: str,\nroot: str,\ntransform: Optional[Callable] = None,\npre_transform: Optional[Callable] = None,\n):\nr\"\"\"\n    PyG wrapper for the LinkPropPredDataset\n    can return pytorch tensors for src,dst,t,msg,label\n    can return Temporal Data object\n    Parameters:\n        name: name of the dataset, passed to `LinkPropPredDataset`\n        root (string): Root directory where the dataset should be saved, passed to `LinkPropPredDataset`\n        transform (callable, optional): A function/transform that takes in an, not used in this case\n        pre_transform (callable, optional): A function/transform that takes in, not used in this case\n    \"\"\"\nself.name = name\nself.root = root\nself.dataset = LinkPropPredDataset(name=name, root=root)\nself._train_mask = torch.from_numpy(self.dataset.train_mask)\nself._val_mask = torch.from_numpy(self.dataset.val_mask)\nself._test_mask = torch.from_numpy(self.dataset.test_mask)\nsuper().__init__(root, transform, pre_transform)\nself._node_feat = self.dataset.node_feat\nif self._node_feat is None:\nself._node_feat = None\nelse:\nself._node_feat = torch.from_numpy(self._node_feat).float()\nself.process_data()\nself._ns_sampler = self.dataset.negative_sampler\n</code></pre>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.dataset_pyg.PyGLinkPropPredDataset.get","title":"<code>get(idx)</code>","text":"<p>construct temporal data object for a single edge</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>index of the edge</p> required <p>Returns:</p> Name Type Description <code>data</code> <code>TemporalData</code> <p>TemporalData object</p> Source code in <code>tgb/linkproppred/dataset_pyg.py</code> <pre><code>def get(self, idx: int) -&gt; TemporalData:\n\"\"\"\n    construct temporal data object for a single edge\n    Parameters:\n        idx: index of the edge\n    Returns:\n        data: TemporalData object\n    \"\"\"\ndata = TemporalData(\nsrc=self._src[idx],\ndst=self._dst[idx],\nt=self._ts[idx],\nmsg=self._edge_feat[idx],\ny=self._edge_label[idx],\n)\nreturn data\n</code></pre>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.dataset_pyg.PyGLinkPropPredDataset.get_TemporalData","title":"<code>get_TemporalData()</code>","text":"<p>return the TemporalData object for the entire dataset</p> Source code in <code>tgb/linkproppred/dataset_pyg.py</code> <pre><code>def get_TemporalData(self) -&gt; TemporalData:\n\"\"\"\n    return the TemporalData object for the entire dataset\n    \"\"\"\ndata = TemporalData(\nsrc=self._src,\ndst=self._dst,\nt=self._ts,\nmsg=self._edge_feat,\ny=self._edge_label,\n)\nreturn data\n</code></pre>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.dataset_pyg.PyGLinkPropPredDataset.len","title":"<code>len()</code>","text":"<p>size of the dataset</p> <p>Returns:</p> Name Type Description <code>size</code> <code>int</code> <p>int</p> Source code in <code>tgb/linkproppred/dataset_pyg.py</code> <pre><code>def len(self) -&gt; int:\n\"\"\"\n    size of the dataset\n    Returns:\n        size: int\n    \"\"\"\nreturn self._src.shape[0]\n</code></pre>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.dataset_pyg.PyGLinkPropPredDataset.load_test_ns","title":"<code>load_test_ns()</code>","text":"<p>load the negative samples for the test set</p> Source code in <code>tgb/linkproppred/dataset_pyg.py</code> <pre><code>def load_test_ns(self) -&gt; None:\nr\"\"\"\n    load the negative samples for the test set\n    \"\"\"\nself.dataset.load_test_ns()\n</code></pre>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.dataset_pyg.PyGLinkPropPredDataset.load_val_ns","title":"<code>load_val_ns()</code>","text":"<p>load the negative samples for the validation set</p> Source code in <code>tgb/linkproppred/dataset_pyg.py</code> <pre><code>def load_val_ns(self) -&gt; None:\nr\"\"\"\n    load the negative samples for the validation set\n    \"\"\"\nself.dataset.load_val_ns()\n</code></pre>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.dataset_pyg.PyGLinkPropPredDataset.process_data","title":"<code>process_data()</code>","text":"<p>convert the numpy arrays from dataset to pytorch tensors</p> Source code in <code>tgb/linkproppred/dataset_pyg.py</code> <pre><code>def process_data(self) -&gt; None:\nr\"\"\"\n    convert the numpy arrays from dataset to pytorch tensors\n    \"\"\"\nsrc = torch.from_numpy(self.dataset.full_data[\"sources\"])\ndst = torch.from_numpy(self.dataset.full_data[\"destinations\"])\nts = torch.from_numpy(self.dataset.full_data[\"timestamps\"])\nmsg = torch.from_numpy(\nself.dataset.full_data[\"edge_feat\"]\n)  # use edge features here if available\nedge_label = torch.from_numpy(\nself.dataset.full_data[\"edge_label\"]\n)  # this is the label indicating if an edge is a true edge, always 1 for true edges\n# * first check typing for all tensors\n# source tensor must be of type int64\n# warnings.warn(\"sources tensor is not of type int64 or int32, forcing conversion\")\nif src.dtype != torch.int64:\nsrc = src.long()\n# destination tensor must be of type int64\nif dst.dtype != torch.int64:\ndst = dst.long()\n# timestamp tensor must be of type int64\nif ts.dtype != torch.int64:\nts = ts.long()\n# message tensor must be of type float32\nif msg.dtype != torch.float32:\nmsg = msg.float()\nself._src = src\nself._dst = dst\nself._ts = ts\nself._edge_label = edge_label\nself._edge_feat = msg\n</code></pre>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.evaluate.Evaluator","title":"<code>Evaluator</code>","text":"<p>             Bases: <code>object</code></p> <p>Evaluator for Link Property Prediction</p> Source code in <code>tgb/linkproppred/evaluate.py</code> <pre><code>class Evaluator(object):\nr\"\"\"Evaluator for Link Property Prediction \"\"\"\ndef __init__(self, name: str, k_value: int = 10):\nr\"\"\"\n        Parameters:\n            name: name of the dataset\n            k_value: the desired 'k' value for calculating metric@k\n        \"\"\"\nself.name = name\nself.k_value = k_value  # for computing `hits@k`\nself.valid_metric_list = ['hits@', 'mrr']\nif self.name not in DATA_EVAL_METRIC_DICT:\nraise NotImplementedError(\"Dataset not supported\")\ndef _parse_and_check_input(self, input_dict):\nr\"\"\"\n        Check whether the input has the appropriate format\n        Parametrers:\n            input_dict: a dictionary containing \"y_pred_pos\", \"y_pred_neg\", and \"eval_metric\"\n            note: \"eval_metric\" should be a list including one or more of the followin metrics: [\"hits@\", \"mrr\"]\n        Returns:\n            y_pred_pos: positive predicted scores\n            y_pred_neg: negative predicted scores\n        \"\"\"\nif \"eval_metric\" not in input_dict:\nraise RuntimeError(\"Missing key of eval_metric!\")\nfor eval_metric in input_dict[\"eval_metric\"]:\nif eval_metric in self.valid_metric_list:\nif \"y_pred_pos\" not in input_dict:\nraise RuntimeError(\"Missing key of y_true\")\nif \"y_pred_neg\" not in input_dict:\nraise RuntimeError(\"Missing key of y_pred\")\ny_pred_pos, y_pred_neg = input_dict[\"y_pred_pos\"], input_dict[\"y_pred_neg\"]\n# converting to numpy on cpu\nif torch is not None and isinstance(y_pred_pos, torch.Tensor):\ny_pred_pos = y_pred_pos.detach().cpu().numpy()\nif torch is not None and isinstance(y_pred_neg, torch.Tensor):\ny_pred_neg = y_pred_neg.detach().cpu().numpy()\n# check type and shape\nif not isinstance(y_pred_pos, np.ndarray) or not isinstance(y_pred_neg, np.ndarray):\nraise RuntimeError(\n\"Arguments to Evaluator need to be either numpy ndarray or torch tensor!\"\n)\nelse:\nprint(\n\"ERROR: The evaluation metric should be in:\", self.valid_metric_list\n)\nraise ValueError(\"Unsupported eval metric %s \" % (eval_metric))\nself.eval_metric = input_dict[\"eval_metric\"]\nreturn y_pred_pos, y_pred_neg\ndef _eval_hits_and_mrr(self, y_pred_pos, y_pred_neg, type_info, k_value):\nr\"\"\"\n        compute hist@k and mrr\n        reference:\n            - https://github.com/snap-stanford/ogb/blob/d5c11d91c9e1c22ed090a2e0bbda3fe357de66e7/ogb/linkproppred/evaluate.py#L214\n        Parameters:\n            y_pred_pos: positive predicted scores\n            y_pred_neg: negative predicted scores\n            type_info: type of the predicted scores; could be 'torch' or 'numpy'\n            k_value: the desired 'k' value for calculating metric@k\n        Returns:\n            a dictionary containing the computed performance metrics\n        \"\"\"\nif type_info == 'torch':\n# calculate ranks\ny_pred_pos = y_pred_pos.view(-1, 1)\n# optimistic rank: \"how many negatives have a larger score than the positive?\"\n# ~&gt; the positive is ranked first among those with equal score\noptimistic_rank = (y_pred_neg &gt; y_pred_pos).sum(dim=1)\n# pessimistic rank: \"how many negatives have at least the positive score?\"\n# ~&gt; the positive is ranked last among those with equal score\npessimistic_rank = (y_pred_neg &gt;= y_pred_pos).sum(dim=1)\nranking_list = 0.5 * (optimistic_rank + pessimistic_rank) + 1\nhitsK_list = (ranking_list &lt;= k_value).to(torch.float)\nmrr_list = 1./ranking_list.to(torch.float)\nreturn {\nf'hits@{k_value}': hitsK_list.mean(),\n'mrr': mrr_list.mean()\n}\nelse:\ny_pred_pos = y_pred_pos.reshape(-1, 1)\noptimistic_rank = (y_pred_neg &gt;= y_pred_pos).sum()\npessimistic_rank = (y_pred_neg &gt; y_pred_pos).sum()\nranking_list = 0.5 * (optimistic_rank + pessimistic_rank) + 1\nhitsK_list = (ranking_list &lt;= k_value).astype(np.float32)\nmrr_list = 1./ranking_list.astype(np.float32)\nreturn {\nf'hits@{k_value}': hitsK_list.mean(),\n'mrr': mrr_list.mean()\n}\ndef eval(self, \ninput_dict: dict, \nverbose: bool = False) -&gt; dict:\nr\"\"\"\n        evaluate the link prediction task\n        this method is callable through an instance of this object to compute the metric\n        Parameters:\n            input_dict: a dictionary containing \"y_pred_pos\", \"y_pred_neg\", and \"eval_metric\"\n                        the performance metric is calculated for the provided scores\n            verbose: whether to print out the computed metric\n        Returns:\n            perf_dict: a dictionary containing the computed performance metric\n        \"\"\"\ny_pred_pos, y_pred_neg = self._parse_and_check_input(input_dict)  # convert the predictions to numpy\nperf_dict = self._eval_hits_and_mrr(y_pred_pos, y_pred_neg, type_info='numpy', k_value=self.k_value)\nreturn perf_dict\n</code></pre>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.evaluate.Evaluator.__init__","title":"<code>__init__(name, k_value=10)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>name of the dataset</p> required <code>k_value</code> <code>int</code> <p>the desired 'k' value for calculating metric@k</p> <code>10</code> Source code in <code>tgb/linkproppred/evaluate.py</code> <pre><code>def __init__(self, name: str, k_value: int = 10):\nr\"\"\"\n    Parameters:\n        name: name of the dataset\n        k_value: the desired 'k' value for calculating metric@k\n    \"\"\"\nself.name = name\nself.k_value = k_value  # for computing `hits@k`\nself.valid_metric_list = ['hits@', 'mrr']\nif self.name not in DATA_EVAL_METRIC_DICT:\nraise NotImplementedError(\"Dataset not supported\")\n</code></pre>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.evaluate.Evaluator.eval","title":"<code>eval(input_dict, verbose=False)</code>","text":"<p>evaluate the link prediction task this method is callable through an instance of this object to compute the metric</p> <p>Parameters:</p> Name Type Description Default <code>input_dict</code> <code>dict</code> <p>a dictionary containing \"y_pred_pos\", \"y_pred_neg\", and \"eval_metric\"         the performance metric is calculated for the provided scores</p> required <code>verbose</code> <code>bool</code> <p>whether to print out the computed metric</p> <code>False</code> <p>Returns:</p> Name Type Description <code>perf_dict</code> <code>dict</code> <p>a dictionary containing the computed performance metric</p> Source code in <code>tgb/linkproppred/evaluate.py</code> <pre><code>def eval(self, \ninput_dict: dict, \nverbose: bool = False) -&gt; dict:\nr\"\"\"\n    evaluate the link prediction task\n    this method is callable through an instance of this object to compute the metric\n    Parameters:\n        input_dict: a dictionary containing \"y_pred_pos\", \"y_pred_neg\", and \"eval_metric\"\n                    the performance metric is calculated for the provided scores\n        verbose: whether to print out the computed metric\n    Returns:\n        perf_dict: a dictionary containing the computed performance metric\n    \"\"\"\ny_pred_pos, y_pred_neg = self._parse_and_check_input(input_dict)  # convert the predictions to numpy\nperf_dict = self._eval_hits_and_mrr(y_pred_pos, y_pred_neg, type_info='numpy', k_value=self.k_value)\nreturn perf_dict\n</code></pre>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.negative_sampler.NegativeEdgeSampler","title":"<code>NegativeEdgeSampler</code>","text":"<p>             Bases: <code>object</code></p> Source code in <code>tgb/linkproppred/negative_sampler.py</code> <pre><code>class NegativeEdgeSampler(object):\ndef __init__(\nself,\ndataset_name: str,\nstrategy: str = \"hist_rnd\",\n) -&gt; None:\nr\"\"\"\n        Negative Edge Sampler\n            Loads and query the negative batches based on the positive batches provided.\n        constructor for the negative edge sampler class\n        Parameters:\n            dataset_name: name of the dataset\n            strategy: specifies which set of negatives should be loaded;\n                    can be 'rnd' or 'hist_rnd'\n        Returns:\n            None\n        \"\"\"\nself.dataset_name = dataset_name\nassert strategy in [\n\"rnd\",\n\"hist_rnd\",\n], \"The supported strategies are `rnd` or `hist_rnd`!\"\nself.strategy = strategy\nself.eval_set = {}\ndef load_eval_set(\nself,\nfname: str,\nsplit_mode: str = \"val\",\n) -&gt; None:\nr\"\"\"\n        Load the evaluation set from disk, can be either val or test set ns samples\n        Parameters:\n            fname: the file name of the evaluation ns on disk\n            split_mode: the split mode of the evaluation set, can be either `val` or `test`\n        Returns:\n            None\n        \"\"\"\nassert split_mode in [\n\"val\",\n\"test\",\n], \"Invalid split-mode! It should be `val`, `test`\"\nif not os.path.exists(fname):\nraise FileNotFoundError(f\"File not found at {fname}\")\nself.eval_set[split_mode] = load_pkl(fname)\ndef reset_eval_set(self, \nsplit_mode: str = \"test\",\n) -&gt; None:\nr\"\"\"\n        Reset evaluation set\n        Parameters:\n            split_mode: specifies whether to generate negative edges for 'validation' or 'test' splits\n        Returns:\n            None\n        \"\"\"\nassert split_mode in [\n\"val\",\n\"test\",\n], \"Invalid split-mode! It should be `val`, `test`!\"\nself.eval_set[split_mode] = None\ndef query_batch(self, \npos_src: Tensor, \npos_dst: Tensor, \npos_timestamp: Tensor, \nsplit_mode: str = \"test\") -&gt; list:\nr\"\"\"\n        For each positive edge in the `pos_batch`, return a list of negative edges\n        `split_mode` specifies whether the valiation or test evaluation set should be retrieved.\n        Parameters:\n            pos_src: list of positive source nodes\n            pos_dst: list of positive destination nodes\n            pos_timestamp: list of timestamps of the positive edges\n            split_mode: specifies whether to generate negative edges for 'validation' or 'test' splits\n        Returns:\n            neg_samples: a list of list; each internal list contains the set of negative edges that\n                        should be evaluated against each positive edge.\n        \"\"\"\nassert split_mode in [\n\"val\",\n\"test\",\n], \"Invalid split-mode! It should be `val`, `test`!\"\nif self.eval_set[split_mode] == None:\nraise ValueError(\nf\"Evaluation set is None! You should load the {split_mode} evaluation set first!\"\n)\n# check the argument types...\nif torch is not None and isinstance(pos_src, torch.Tensor):\npos_src = pos_src.detach().cpu().numpy()\nif torch is not None and isinstance(pos_dst, torch.Tensor):\npos_dst = pos_dst.detach().cpu().numpy()\nif torch is not None and isinstance(pos_timestamp, torch.Tensor):\npos_timestamp = pos_timestamp.detach().cpu().numpy()\nif not isinstance(pos_src, np.ndarray) or not isinstance(pos_dst, np.ndarray) or not(pos_timestamp, np.ndarray):\nraise RuntimeError(\n\"pos_src, pos_dst, and pos_timestamp need to be either numpy ndarray or torch tensor!\"\n)\nneg_samples = []\nfor pos_s, pos_d, pos_t in zip(pos_src, pos_dst, pos_timestamp):\nif (pos_s, pos_d, pos_t) not in self.eval_set[split_mode]:\nraise ValueError(\nf\"The edge ({pos_s}, {pos_d}, {pos_t}) is not in the '{split_mode}' evaluation set! Please check the implementation.\"\n)\nelse:\nneg_samples.append(\n[\nint(neg_dst)\nfor neg_dst in self.eval_set[split_mode][(pos_s, pos_d, pos_t)]\n]\n)\nreturn neg_samples\n</code></pre>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.negative_sampler.NegativeEdgeSampler.__init__","title":"<code>__init__(dataset_name, strategy='hist_rnd')</code>","text":"<p>Negative Edge Sampler     Loads and query the negative batches based on the positive batches provided. constructor for the negative edge sampler class</p> <p>Parameters:</p> Name Type Description Default <code>dataset_name</code> <code>str</code> <p>name of the dataset</p> required <code>strategy</code> <code>str</code> <p>specifies which set of negatives should be loaded;     can be 'rnd' or 'hist_rnd'</p> <code>'hist_rnd'</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>tgb/linkproppred/negative_sampler.py</code> <pre><code>def __init__(\nself,\ndataset_name: str,\nstrategy: str = \"hist_rnd\",\n) -&gt; None:\nr\"\"\"\n    Negative Edge Sampler\n        Loads and query the negative batches based on the positive batches provided.\n    constructor for the negative edge sampler class\n    Parameters:\n        dataset_name: name of the dataset\n        strategy: specifies which set of negatives should be loaded;\n                can be 'rnd' or 'hist_rnd'\n    Returns:\n        None\n    \"\"\"\nself.dataset_name = dataset_name\nassert strategy in [\n\"rnd\",\n\"hist_rnd\",\n], \"The supported strategies are `rnd` or `hist_rnd`!\"\nself.strategy = strategy\nself.eval_set = {}\n</code></pre>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.negative_sampler.NegativeEdgeSampler.load_eval_set","title":"<code>load_eval_set(fname, split_mode='val')</code>","text":"<p>Load the evaluation set from disk, can be either val or test set ns samples</p> <p>Parameters:</p> Name Type Description Default <code>fname</code> <code>str</code> <p>the file name of the evaluation ns on disk</p> required <code>split_mode</code> <code>str</code> <p>the split mode of the evaluation set, can be either <code>val</code> or <code>test</code></p> <code>'val'</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>tgb/linkproppred/negative_sampler.py</code> <pre><code>def load_eval_set(\nself,\nfname: str,\nsplit_mode: str = \"val\",\n) -&gt; None:\nr\"\"\"\n    Load the evaluation set from disk, can be either val or test set ns samples\n    Parameters:\n        fname: the file name of the evaluation ns on disk\n        split_mode: the split mode of the evaluation set, can be either `val` or `test`\n    Returns:\n        None\n    \"\"\"\nassert split_mode in [\n\"val\",\n\"test\",\n], \"Invalid split-mode! It should be `val`, `test`\"\nif not os.path.exists(fname):\nraise FileNotFoundError(f\"File not found at {fname}\")\nself.eval_set[split_mode] = load_pkl(fname)\n</code></pre>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.negative_sampler.NegativeEdgeSampler.query_batch","title":"<code>query_batch(pos_src, pos_dst, pos_timestamp, split_mode='test')</code>","text":"<p>For each positive edge in the <code>pos_batch</code>, return a list of negative edges <code>split_mode</code> specifies whether the valiation or test evaluation set should be retrieved.</p> <p>Parameters:</p> Name Type Description Default <code>pos_src</code> <code>Tensor</code> <p>list of positive source nodes</p> required <code>pos_dst</code> <code>Tensor</code> <p>list of positive destination nodes</p> required <code>pos_timestamp</code> <code>Tensor</code> <p>list of timestamps of the positive edges</p> required <code>split_mode</code> <code>str</code> <p>specifies whether to generate negative edges for 'validation' or 'test' splits</p> <code>'test'</code> <p>Returns:</p> Name Type Description <code>neg_samples</code> <code>list</code> <p>a list of list; each internal list contains the set of negative edges that         should be evaluated against each positive edge.</p> Source code in <code>tgb/linkproppred/negative_sampler.py</code> <pre><code>def query_batch(self, \npos_src: Tensor, \npos_dst: Tensor, \npos_timestamp: Tensor, \nsplit_mode: str = \"test\") -&gt; list:\nr\"\"\"\n    For each positive edge in the `pos_batch`, return a list of negative edges\n    `split_mode` specifies whether the valiation or test evaluation set should be retrieved.\n    Parameters:\n        pos_src: list of positive source nodes\n        pos_dst: list of positive destination nodes\n        pos_timestamp: list of timestamps of the positive edges\n        split_mode: specifies whether to generate negative edges for 'validation' or 'test' splits\n    Returns:\n        neg_samples: a list of list; each internal list contains the set of negative edges that\n                    should be evaluated against each positive edge.\n    \"\"\"\nassert split_mode in [\n\"val\",\n\"test\",\n], \"Invalid split-mode! It should be `val`, `test`!\"\nif self.eval_set[split_mode] == None:\nraise ValueError(\nf\"Evaluation set is None! You should load the {split_mode} evaluation set first!\"\n)\n# check the argument types...\nif torch is not None and isinstance(pos_src, torch.Tensor):\npos_src = pos_src.detach().cpu().numpy()\nif torch is not None and isinstance(pos_dst, torch.Tensor):\npos_dst = pos_dst.detach().cpu().numpy()\nif torch is not None and isinstance(pos_timestamp, torch.Tensor):\npos_timestamp = pos_timestamp.detach().cpu().numpy()\nif not isinstance(pos_src, np.ndarray) or not isinstance(pos_dst, np.ndarray) or not(pos_timestamp, np.ndarray):\nraise RuntimeError(\n\"pos_src, pos_dst, and pos_timestamp need to be either numpy ndarray or torch tensor!\"\n)\nneg_samples = []\nfor pos_s, pos_d, pos_t in zip(pos_src, pos_dst, pos_timestamp):\nif (pos_s, pos_d, pos_t) not in self.eval_set[split_mode]:\nraise ValueError(\nf\"The edge ({pos_s}, {pos_d}, {pos_t}) is not in the '{split_mode}' evaluation set! Please check the implementation.\"\n)\nelse:\nneg_samples.append(\n[\nint(neg_dst)\nfor neg_dst in self.eval_set[split_mode][(pos_s, pos_d, pos_t)]\n]\n)\nreturn neg_samples\n</code></pre>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.negative_sampler.NegativeEdgeSampler.reset_eval_set","title":"<code>reset_eval_set(split_mode='test')</code>","text":"<p>Reset evaluation set</p> <p>Parameters:</p> Name Type Description Default <code>split_mode</code> <code>str</code> <p>specifies whether to generate negative edges for 'validation' or 'test' splits</p> <code>'test'</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>tgb/linkproppred/negative_sampler.py</code> <pre><code>def reset_eval_set(self, \nsplit_mode: str = \"test\",\n) -&gt; None:\nr\"\"\"\n    Reset evaluation set\n    Parameters:\n        split_mode: specifies whether to generate negative edges for 'validation' or 'test' splits\n    Returns:\n        None\n    \"\"\"\nassert split_mode in [\n\"val\",\n\"test\",\n], \"Invalid split-mode! It should be `val`, `test`!\"\nself.eval_set[split_mode] = None\n</code></pre>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.negative_generator.NegativeEdgeGenerator","title":"<code>NegativeEdgeGenerator</code>","text":"<p>             Bases: <code>object</code></p> Source code in <code>tgb/linkproppred/negative_generator.py</code> <pre><code>class NegativeEdgeGenerator(object):\ndef __init__(\nself,\ndataset_name: str,\nfirst_dst_id: int,\nlast_dst_id: int,\nnum_neg_e: int = 100,  # number of negative edges sampled per positive edges --&gt; make it constant =&gt; 1000\nstrategy: str = \"rnd\",\nrnd_seed: int = 123,\nhist_ratio: float = 0.5,\nhistorical_data: TemporalData = None,\n) -&gt; None:\nr\"\"\"\n        Negative Edge Sampler class\n        this is a class for generating negative samples for a specific datasets\n        the set of the positive samples are provided, the negative samples are generated with specific strategies \n        and are saved for consistent evaluation across different methods\n        negative edges are sampled with 'oen_vs_many' strategy.\n        it is assumed that the destination nodes are indexed sequentially with 'first_dst_id' \n        and 'last_dst_id' being the first and last index, respectively.\n        Parameters:\n            dataset_name: name of the dataset\n            first_dst_id: identity of the first destination node\n            last_dst_id: indentity of the last destination node\n            num_neg_e: number of negative edges being generated per each positive edge\n            strategy: how to generate negative edges; can be 'rnd' or 'hist_rnd'\n            rnd_seed: random seed for consistency\n            hist_ratio: if the startegy is 'hist_rnd', how much of the negatives are historical\n            historical_data: previous records of the positive edges\n        Returns:\n            None\n        \"\"\"\nself.rnd_seed = rnd_seed\nnp.random.seed(self.rnd_seed)\nself.dataset_name = dataset_name\nself.first_dst_id = first_dst_id\nself.last_dst_id = last_dst_id\nself.num_neg_e = num_neg_e\nassert strategy in [\n\"rnd\",\n\"hist_rnd\",\n], \"The supported strategies are `rnd` or `hist_rnd`!\"\nself.strategy = strategy\nif self.strategy == \"hist_rnd\":\nassert (\nhistorical_data != None\n), \"Train data should be passed when `hist_rnd` strategy is selected.\"\nself.hist_ratio = hist_ratio\nself.historical_data = historical_data\ndef generate_negative_samples(self, \ndata: TemporalData, \nsplit_mode: str, \npartial_path: str,\n) -&gt; None:\nr\"\"\"\n        Generate negative samples\n        Parameters:\n            data: an object containing positive edges information\n            split_mode: specifies whether to generate negative edges for 'validation' or 'test' splits\n            partial_path: in which directory save the generated negatives\n        \"\"\"\n# file name for saving or loading...\nfilename = (\npartial_path\n+ \"/\"\n+ self.dataset_name\n+ \"_\"\n+ split_mode\n+ \"_\"\n+ \"ns\"\n+ \".pkl\"\n)\nif self.strategy == \"rnd\":\nself.generate_negative_samples_rnd(data, split_mode, filename)\nelif self.strategy == \"hist_rnd\":\nself.generate_negative_samples_hist_rnd(\nself.historical_data, data, split_mode, filename\n)\nelse:\nraise ValueError(\"Unsupported negative sample generation strategy!\")\ndef generate_negative_samples_rnd(self, \ndata: TemporalData, \nsplit_mode: str, \nfilename: str,\n) -&gt; None:\nr\"\"\"\n        Generate negative samples based on the `HIST-RND` strategy:\n            - for each positive edge, sample a batch of negative edges from all possible edges with the same source node\n            - filter actual positive edges\n        Parameters:\n            data: an object containing positive edges information\n            split_mode: specifies whether to generate negative edges for 'validation' or 'test' splits\n            filename: name of the file containing the generated negative edges\n        \"\"\"\nprint(\nf\"INFO: Negative Sampling Strategy: {self.strategy}, Data Split: {split_mode}\"\n)\nassert split_mode in [\n\"val\",\n\"test\",\n], \"Invalid split-mode! It should be `val` or `test`!\"\nif os.path.exists(filename):\nprint(\nf\"INFO: negative samples for '{split_mode}' evaluation are already generated!\"\n)\nelse:\nprint(f\"INFO: Generating negative samples for '{split_mode}' evaluation!\")\n# retrieve the information from the batch\npos_src, pos_dst, pos_timestamp = (\ndata.src.cpu().numpy(),\ndata.dst.cpu().numpy(),\ndata.t.cpu().numpy(),\n)\n# all possible destinations\nall_dst = np.arange(self.first_dst_id, self.last_dst_id + 1)\nevaluation_set = {}\n# generate a list of negative destinations for each positive edge\npos_edge_tqdm = tqdm(\nzip(pos_src, pos_dst, pos_timestamp), total=len(pos_src)\n)\nfor (\npos_s,\npos_d,\npos_t,\n) in pos_edge_tqdm:\nt_mask = pos_timestamp == pos_t\nsrc_mask = pos_src == pos_s\nfn_mask = np.logical_and(t_mask, src_mask)\npos_e_dst_same_src = pos_dst[fn_mask]\nfiltered_all_dst = np.setdiff1d(all_dst, pos_e_dst_same_src)\n'''\n                when num_neg_e is larger than all possible destinations simple return all possible destinations\n                '''\nif (self.num_neg_e &gt; len(filtered_all_dst)):\nneg_d_arr = filtered_all_dst\nelse:\nneg_d_arr = np.random.choice(\nfiltered_all_dst, self.num_neg_e, replace=False) #never replace negatives\nevaluation_set[(pos_s, pos_d, pos_t)] = neg_d_arr\n# save the generated evaluation set to disk\nsave_pkl(evaluation_set, filename)\ndef generate_historical_edge_set(self, \nhistorical_data: TemporalData,\n) -&gt; tuple:\nr\"\"\"\n        Generate the set of edges seen durign training or validation\n        ONLY `train_data` should be passed as historical data; i.e., the HISTORICAL negative edges should be selected from training data only.\n        Parameters:\n            historical_data: contains the positive edges observed previously\n        Returns:\n            historical_edges: distict historical positive edges\n            hist_edge_set_per_node: historical edges observed for each node\n        \"\"\"\nsources = historical_data.src.cpu().numpy()\ndestinations = historical_data.dst.cpu().numpy()\nhistorical_edges = {}\nhist_e_per_node = {}\nfor src, dst in zip(sources, destinations):\n# edge-centric\nif (src, dst) not in historical_edges:\nhistorical_edges[(src, dst)] = 1\n# node-centric\nif src not in hist_e_per_node:\nhist_e_per_node[src] = [dst]\nelse:\nhist_e_per_node[src].append(dst)\nhist_edge_set_per_node = {}\nfor src, dst_list in hist_e_per_node.items():\nhist_edge_set_per_node[src] = np.array(list(set(dst_list)))\nreturn historical_edges, hist_edge_set_per_node\ndef generate_negative_samples_hist_rnd(\nself, \nhistorical_data : TemporalData, \ndata: TemporalData, \nsplit_mode: str, \nfilename: str,\n) -&gt; None:\nr\"\"\"\n        Generate negative samples based on the `HIST-RND` strategy:\n            - up to 50% of the negative samples are selected from the set of edges seen during the training with the same source node.\n            - the rest of the negative edges are randomly sampled with the fixed source node.\n        Parameters:\n            historical_data: contains the history of the observed positive edges including \n                            distinct positive edges and edges observed for each positive node\n            data: an object containing positive edges information\n            split_mode: specifies whether to generate negative edges for 'validation' or 'test' splits\n            filename: name of the file to save generated negative edges\n        Returns:\n            None\n        \"\"\"\nprint(\nf\"INFO: Negative Sampling Strategy: {self.strategy}, Data Split: {split_mode}\"\n)\nassert split_mode in [\n\"val\",\n\"test\",\n], \"Invalid split-mode! It should be `val` or `test`!\"\nif os.path.exists(filename):\nprint(\nf\"INFO: negative samples for '{split_mode}' evaluation are already generated!\"\n)\nelse:\nprint(f\"INFO: Generating negative samples for '{split_mode}' evaluation!\")\n# retrieve the information from the batch\npos_src, pos_dst, pos_timestamp = (\ndata.src.cpu().numpy(),\ndata.dst.cpu().numpy(),\ndata.t.cpu().numpy(),\n)\npos_ts_edge_dict = {} #{ts: {src: [dsts]}}\npos_edge_tqdm = tqdm(\nzip(pos_src, pos_dst, pos_timestamp), total=len(pos_src)\n)\nfor (\npos_s,\npos_d,\npos_t,\n) in pos_edge_tqdm:\nif (pos_t not in pos_ts_edge_dict):\npos_ts_edge_dict[pos_t] = {pos_s: [pos_d]}\nelse:\nif (pos_s not in pos_ts_edge_dict[pos_t]):\npos_ts_edge_dict[pos_t][pos_s] = [pos_d]\nelse:\npos_ts_edge_dict[pos_t][pos_s].append(pos_d)\n# all possible destinations\nall_dst = np.arange(self.first_dst_id, self.last_dst_id + 1)\n# get seen edge history\n(\nhistorical_edges,\nhist_edge_set_per_node,\n) = self.generate_historical_edge_set(historical_data)\n# sample historical edges\nmax_num_hist_neg_e = int(self.num_neg_e * self.hist_ratio)\nevaluation_set = {}\n# generate a list of negative destinations for each positive edge\npos_edge_tqdm = tqdm(\nzip(pos_src, pos_dst, pos_timestamp), total=len(pos_src)\n)\nfor (\npos_s,\npos_d,\npos_t,\n) in pos_edge_tqdm:\npos_e_dst_same_src = np.array(pos_ts_edge_dict[pos_t][pos_s])\n# sample historical edges\nnum_hist_neg_e = 0\nneg_hist_dsts = np.array([])\nseen_dst = []\nif pos_s in hist_edge_set_per_node:\nseen_dst = hist_edge_set_per_node[pos_s]\nif len(seen_dst) &gt;= 1:\nfiltered_all_seen_dst = np.setdiff1d(seen_dst, pos_e_dst_same_src)\n#filtered_all_seen_dst = seen_dst #! no collision check\nnum_hist_neg_e = (\nmax_num_hist_neg_e\nif max_num_hist_neg_e &lt;= len(filtered_all_seen_dst)\nelse len(filtered_all_seen_dst)\n)\nneg_hist_dsts = np.random.choice(\nfiltered_all_seen_dst, num_hist_neg_e, replace=False\n)\n# sample random edges\nif (len(seen_dst) &gt;= 1):\ninvalid_dst = np.concatenate((np.array(pos_e_dst_same_src), seen_dst))\nelse:\ninvalid_dst = np.array(pos_e_dst_same_src)\nfiltered_all_rnd_dst = np.setdiff1d(all_dst, invalid_dst)\nnum_rnd_neg_e = self.num_neg_e - num_hist_neg_e\n'''\n                when num_neg_e is larger than all possible destinations simple return all possible destinations\n                '''\nif (num_rnd_neg_e &gt; len(filtered_all_rnd_dst)):\nneg_rnd_dsts = filtered_all_rnd_dst\nelse:\nneg_rnd_dsts = np.random.choice(\nfiltered_all_rnd_dst, num_rnd_neg_e, replace=False\n)\n# concatenate the two sets: historical and random\nneg_dst_arr = np.concatenate((neg_hist_dsts, neg_rnd_dsts))\nevaluation_set[(pos_s, pos_d, pos_t)] = neg_dst_arr\n# save the generated evaluation set to disk\nsave_pkl(evaluation_set, filename)\n</code></pre>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.negative_generator.NegativeEdgeGenerator.__init__","title":"<code>__init__(dataset_name, first_dst_id, last_dst_id, num_neg_e=100, strategy='rnd', rnd_seed=123, hist_ratio=0.5, historical_data=None)</code>","text":"<p>Negative Edge Sampler class this is a class for generating negative samples for a specific datasets the set of the positive samples are provided, the negative samples are generated with specific strategies  and are saved for consistent evaluation across different methods negative edges are sampled with 'oen_vs_many' strategy. it is assumed that the destination nodes are indexed sequentially with 'first_dst_id'  and 'last_dst_id' being the first and last index, respectively.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_name</code> <code>str</code> <p>name of the dataset</p> required <code>first_dst_id</code> <code>int</code> <p>identity of the first destination node</p> required <code>last_dst_id</code> <code>int</code> <p>indentity of the last destination node</p> required <code>num_neg_e</code> <code>int</code> <p>number of negative edges being generated per each positive edge</p> <code>100</code> <code>strategy</code> <code>str</code> <p>how to generate negative edges; can be 'rnd' or 'hist_rnd'</p> <code>'rnd'</code> <code>rnd_seed</code> <code>int</code> <p>random seed for consistency</p> <code>123</code> <code>hist_ratio</code> <code>float</code> <p>if the startegy is 'hist_rnd', how much of the negatives are historical</p> <code>0.5</code> <code>historical_data</code> <code>TemporalData</code> <p>previous records of the positive edges</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>tgb/linkproppred/negative_generator.py</code> <pre><code>def __init__(\nself,\ndataset_name: str,\nfirst_dst_id: int,\nlast_dst_id: int,\nnum_neg_e: int = 100,  # number of negative edges sampled per positive edges --&gt; make it constant =&gt; 1000\nstrategy: str = \"rnd\",\nrnd_seed: int = 123,\nhist_ratio: float = 0.5,\nhistorical_data: TemporalData = None,\n) -&gt; None:\nr\"\"\"\n    Negative Edge Sampler class\n    this is a class for generating negative samples for a specific datasets\n    the set of the positive samples are provided, the negative samples are generated with specific strategies \n    and are saved for consistent evaluation across different methods\n    negative edges are sampled with 'oen_vs_many' strategy.\n    it is assumed that the destination nodes are indexed sequentially with 'first_dst_id' \n    and 'last_dst_id' being the first and last index, respectively.\n    Parameters:\n        dataset_name: name of the dataset\n        first_dst_id: identity of the first destination node\n        last_dst_id: indentity of the last destination node\n        num_neg_e: number of negative edges being generated per each positive edge\n        strategy: how to generate negative edges; can be 'rnd' or 'hist_rnd'\n        rnd_seed: random seed for consistency\n        hist_ratio: if the startegy is 'hist_rnd', how much of the negatives are historical\n        historical_data: previous records of the positive edges\n    Returns:\n        None\n    \"\"\"\nself.rnd_seed = rnd_seed\nnp.random.seed(self.rnd_seed)\nself.dataset_name = dataset_name\nself.first_dst_id = first_dst_id\nself.last_dst_id = last_dst_id\nself.num_neg_e = num_neg_e\nassert strategy in [\n\"rnd\",\n\"hist_rnd\",\n], \"The supported strategies are `rnd` or `hist_rnd`!\"\nself.strategy = strategy\nif self.strategy == \"hist_rnd\":\nassert (\nhistorical_data != None\n), \"Train data should be passed when `hist_rnd` strategy is selected.\"\nself.hist_ratio = hist_ratio\nself.historical_data = historical_data\n</code></pre>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.negative_generator.NegativeEdgeGenerator.generate_historical_edge_set","title":"<code>generate_historical_edge_set(historical_data)</code>","text":"<p>Generate the set of edges seen durign training or validation</p> <p>ONLY <code>train_data</code> should be passed as historical data; i.e., the HISTORICAL negative edges should be selected from training data only.</p> <p>Parameters:</p> Name Type Description Default <code>historical_data</code> <code>TemporalData</code> <p>contains the positive edges observed previously</p> required <p>Returns:</p> Name Type Description <code>historical_edges</code> <code>tuple</code> <p>distict historical positive edges</p> <code>hist_edge_set_per_node</code> <code>tuple</code> <p>historical edges observed for each node</p> Source code in <code>tgb/linkproppred/negative_generator.py</code> <pre><code>def generate_historical_edge_set(self, \nhistorical_data: TemporalData,\n) -&gt; tuple:\nr\"\"\"\n    Generate the set of edges seen durign training or validation\n    ONLY `train_data` should be passed as historical data; i.e., the HISTORICAL negative edges should be selected from training data only.\n    Parameters:\n        historical_data: contains the positive edges observed previously\n    Returns:\n        historical_edges: distict historical positive edges\n        hist_edge_set_per_node: historical edges observed for each node\n    \"\"\"\nsources = historical_data.src.cpu().numpy()\ndestinations = historical_data.dst.cpu().numpy()\nhistorical_edges = {}\nhist_e_per_node = {}\nfor src, dst in zip(sources, destinations):\n# edge-centric\nif (src, dst) not in historical_edges:\nhistorical_edges[(src, dst)] = 1\n# node-centric\nif src not in hist_e_per_node:\nhist_e_per_node[src] = [dst]\nelse:\nhist_e_per_node[src].append(dst)\nhist_edge_set_per_node = {}\nfor src, dst_list in hist_e_per_node.items():\nhist_edge_set_per_node[src] = np.array(list(set(dst_list)))\nreturn historical_edges, hist_edge_set_per_node\n</code></pre>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.negative_generator.NegativeEdgeGenerator.generate_negative_samples","title":"<code>generate_negative_samples(data, split_mode, partial_path)</code>","text":"<p>Generate negative samples</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>TemporalData</code> <p>an object containing positive edges information</p> required <code>split_mode</code> <code>str</code> <p>specifies whether to generate negative edges for 'validation' or 'test' splits</p> required <code>partial_path</code> <code>str</code> <p>in which directory save the generated negatives</p> required Source code in <code>tgb/linkproppred/negative_generator.py</code> <pre><code>def generate_negative_samples(self, \ndata: TemporalData, \nsplit_mode: str, \npartial_path: str,\n) -&gt; None:\nr\"\"\"\n    Generate negative samples\n    Parameters:\n        data: an object containing positive edges information\n        split_mode: specifies whether to generate negative edges for 'validation' or 'test' splits\n        partial_path: in which directory save the generated negatives\n    \"\"\"\n# file name for saving or loading...\nfilename = (\npartial_path\n+ \"/\"\n+ self.dataset_name\n+ \"_\"\n+ split_mode\n+ \"_\"\n+ \"ns\"\n+ \".pkl\"\n)\nif self.strategy == \"rnd\":\nself.generate_negative_samples_rnd(data, split_mode, filename)\nelif self.strategy == \"hist_rnd\":\nself.generate_negative_samples_hist_rnd(\nself.historical_data, data, split_mode, filename\n)\nelse:\nraise ValueError(\"Unsupported negative sample generation strategy!\")\n</code></pre>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.negative_generator.NegativeEdgeGenerator.generate_negative_samples_hist_rnd","title":"<code>generate_negative_samples_hist_rnd(historical_data, data, split_mode, filename)</code>","text":"<p>Generate negative samples based on the <code>HIST-RND</code> strategy:     - up to 50% of the negative samples are selected from the set of edges seen during the training with the same source node.     - the rest of the negative edges are randomly sampled with the fixed source node.</p> <p>Parameters:</p> Name Type Description Default <code>historical_data</code> <code>TemporalData</code> <p>contains the history of the observed positive edges including              distinct positive edges and edges observed for each positive node</p> required <code>data</code> <code>TemporalData</code> <p>an object containing positive edges information</p> required <code>split_mode</code> <code>str</code> <p>specifies whether to generate negative edges for 'validation' or 'test' splits</p> required <code>filename</code> <code>str</code> <p>name of the file to save generated negative edges</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>tgb/linkproppred/negative_generator.py</code> <pre><code>def generate_negative_samples_hist_rnd(\nself, \nhistorical_data : TemporalData, \ndata: TemporalData, \nsplit_mode: str, \nfilename: str,\n) -&gt; None:\nr\"\"\"\n    Generate negative samples based on the `HIST-RND` strategy:\n        - up to 50% of the negative samples are selected from the set of edges seen during the training with the same source node.\n        - the rest of the negative edges are randomly sampled with the fixed source node.\n    Parameters:\n        historical_data: contains the history of the observed positive edges including \n                        distinct positive edges and edges observed for each positive node\n        data: an object containing positive edges information\n        split_mode: specifies whether to generate negative edges for 'validation' or 'test' splits\n        filename: name of the file to save generated negative edges\n    Returns:\n        None\n    \"\"\"\nprint(\nf\"INFO: Negative Sampling Strategy: {self.strategy}, Data Split: {split_mode}\"\n)\nassert split_mode in [\n\"val\",\n\"test\",\n], \"Invalid split-mode! It should be `val` or `test`!\"\nif os.path.exists(filename):\nprint(\nf\"INFO: negative samples for '{split_mode}' evaluation are already generated!\"\n)\nelse:\nprint(f\"INFO: Generating negative samples for '{split_mode}' evaluation!\")\n# retrieve the information from the batch\npos_src, pos_dst, pos_timestamp = (\ndata.src.cpu().numpy(),\ndata.dst.cpu().numpy(),\ndata.t.cpu().numpy(),\n)\npos_ts_edge_dict = {} #{ts: {src: [dsts]}}\npos_edge_tqdm = tqdm(\nzip(pos_src, pos_dst, pos_timestamp), total=len(pos_src)\n)\nfor (\npos_s,\npos_d,\npos_t,\n) in pos_edge_tqdm:\nif (pos_t not in pos_ts_edge_dict):\npos_ts_edge_dict[pos_t] = {pos_s: [pos_d]}\nelse:\nif (pos_s not in pos_ts_edge_dict[pos_t]):\npos_ts_edge_dict[pos_t][pos_s] = [pos_d]\nelse:\npos_ts_edge_dict[pos_t][pos_s].append(pos_d)\n# all possible destinations\nall_dst = np.arange(self.first_dst_id, self.last_dst_id + 1)\n# get seen edge history\n(\nhistorical_edges,\nhist_edge_set_per_node,\n) = self.generate_historical_edge_set(historical_data)\n# sample historical edges\nmax_num_hist_neg_e = int(self.num_neg_e * self.hist_ratio)\nevaluation_set = {}\n# generate a list of negative destinations for each positive edge\npos_edge_tqdm = tqdm(\nzip(pos_src, pos_dst, pos_timestamp), total=len(pos_src)\n)\nfor (\npos_s,\npos_d,\npos_t,\n) in pos_edge_tqdm:\npos_e_dst_same_src = np.array(pos_ts_edge_dict[pos_t][pos_s])\n# sample historical edges\nnum_hist_neg_e = 0\nneg_hist_dsts = np.array([])\nseen_dst = []\nif pos_s in hist_edge_set_per_node:\nseen_dst = hist_edge_set_per_node[pos_s]\nif len(seen_dst) &gt;= 1:\nfiltered_all_seen_dst = np.setdiff1d(seen_dst, pos_e_dst_same_src)\n#filtered_all_seen_dst = seen_dst #! no collision check\nnum_hist_neg_e = (\nmax_num_hist_neg_e\nif max_num_hist_neg_e &lt;= len(filtered_all_seen_dst)\nelse len(filtered_all_seen_dst)\n)\nneg_hist_dsts = np.random.choice(\nfiltered_all_seen_dst, num_hist_neg_e, replace=False\n)\n# sample random edges\nif (len(seen_dst) &gt;= 1):\ninvalid_dst = np.concatenate((np.array(pos_e_dst_same_src), seen_dst))\nelse:\ninvalid_dst = np.array(pos_e_dst_same_src)\nfiltered_all_rnd_dst = np.setdiff1d(all_dst, invalid_dst)\nnum_rnd_neg_e = self.num_neg_e - num_hist_neg_e\n'''\n            when num_neg_e is larger than all possible destinations simple return all possible destinations\n            '''\nif (num_rnd_neg_e &gt; len(filtered_all_rnd_dst)):\nneg_rnd_dsts = filtered_all_rnd_dst\nelse:\nneg_rnd_dsts = np.random.choice(\nfiltered_all_rnd_dst, num_rnd_neg_e, replace=False\n)\n# concatenate the two sets: historical and random\nneg_dst_arr = np.concatenate((neg_hist_dsts, neg_rnd_dsts))\nevaluation_set[(pos_s, pos_d, pos_t)] = neg_dst_arr\n# save the generated evaluation set to disk\nsave_pkl(evaluation_set, filename)\n</code></pre>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.negative_generator.NegativeEdgeGenerator.generate_negative_samples_rnd","title":"<code>generate_negative_samples_rnd(data, split_mode, filename)</code>","text":"<p>Generate negative samples based on the <code>HIST-RND</code> strategy:     - for each positive edge, sample a batch of negative edges from all possible edges with the same source node     - filter actual positive edges</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>TemporalData</code> <p>an object containing positive edges information</p> required <code>split_mode</code> <code>str</code> <p>specifies whether to generate negative edges for 'validation' or 'test' splits</p> required <code>filename</code> <code>str</code> <p>name of the file containing the generated negative edges</p> required Source code in <code>tgb/linkproppred/negative_generator.py</code> <pre><code>def generate_negative_samples_rnd(self, \ndata: TemporalData, \nsplit_mode: str, \nfilename: str,\n) -&gt; None:\nr\"\"\"\n    Generate negative samples based on the `HIST-RND` strategy:\n        - for each positive edge, sample a batch of negative edges from all possible edges with the same source node\n        - filter actual positive edges\n    Parameters:\n        data: an object containing positive edges information\n        split_mode: specifies whether to generate negative edges for 'validation' or 'test' splits\n        filename: name of the file containing the generated negative edges\n    \"\"\"\nprint(\nf\"INFO: Negative Sampling Strategy: {self.strategy}, Data Split: {split_mode}\"\n)\nassert split_mode in [\n\"val\",\n\"test\",\n], \"Invalid split-mode! It should be `val` or `test`!\"\nif os.path.exists(filename):\nprint(\nf\"INFO: negative samples for '{split_mode}' evaluation are already generated!\"\n)\nelse:\nprint(f\"INFO: Generating negative samples for '{split_mode}' evaluation!\")\n# retrieve the information from the batch\npos_src, pos_dst, pos_timestamp = (\ndata.src.cpu().numpy(),\ndata.dst.cpu().numpy(),\ndata.t.cpu().numpy(),\n)\n# all possible destinations\nall_dst = np.arange(self.first_dst_id, self.last_dst_id + 1)\nevaluation_set = {}\n# generate a list of negative destinations for each positive edge\npos_edge_tqdm = tqdm(\nzip(pos_src, pos_dst, pos_timestamp), total=len(pos_src)\n)\nfor (\npos_s,\npos_d,\npos_t,\n) in pos_edge_tqdm:\nt_mask = pos_timestamp == pos_t\nsrc_mask = pos_src == pos_s\nfn_mask = np.logical_and(t_mask, src_mask)\npos_e_dst_same_src = pos_dst[fn_mask]\nfiltered_all_dst = np.setdiff1d(all_dst, pos_e_dst_same_src)\n'''\n            when num_neg_e is larger than all possible destinations simple return all possible destinations\n            '''\nif (self.num_neg_e &gt; len(filtered_all_dst)):\nneg_d_arr = filtered_all_dst\nelse:\nneg_d_arr = np.random.choice(\nfiltered_all_dst, self.num_neg_e, replace=False) #never replace negatives\nevaluation_set[(pos_s, pos_d, pos_t)] = neg_d_arr\n# save the generated evaluation set to disk\nsave_pkl(evaluation_set, filename)\n</code></pre>"},{"location":"api/tgb.nodeproppred/","title":"<code>tgb.nodeproppred</code>","text":""},{"location":"api/tgb.nodeproppred/#tgb.nodeproppred.dataset.NodePropPredDataset","title":"<code>NodePropPredDataset</code>","text":"<p>             Bases: <code>object</code></p> Source code in <code>tgb/nodeproppred/dataset.py</code> <pre><code>class NodePropPredDataset(object):\ndef __init__(\nself,\nname: str,\nroot: Optional[str] = \"datasets\",\nmeta_dict: Optional[dict] = None,\npreprocess: Optional[bool] = True,\n) -&gt; None:\nr\"\"\"Dataset class for the node property prediction task. Stores meta information about each dataset such as evaluation metrics etc.\n        also automatically pre-processes the dataset.\n        [!] node property prediction datasets requires the following:\n        self.meta_dict[\"fname\"]: path to the edge list file\n        self.meta_dict[\"nodefile\"]: path to the node label file\n        Parameters:\n            name: name of the dataset\n            root: root directory to store the dataset folder\n            meta_dict: dictionary containing meta information about the dataset, should contain key 'dir_name' which is the name of the dataset folder\n            preprocess: whether to pre-process the dataset\n        Returns:\n            None\n        \"\"\"\nself.name = name  ## original name\n# check if dataset url exist\nif self.name in DATA_URL_DICT:\nself.url = DATA_URL_DICT[self.name]\nelse:\nself.url = None\nprint(f\"Dataset {self.name} url not found, download not supported yet.\")\n# check if the evaluatioin metric are specified\nif self.name in DATA_EVAL_METRIC_DICT:\nself.metric = DATA_EVAL_METRIC_DICT[self.name]\nelse:\nself.metric = None\nprint(\nf\"Dataset {self.name} default evaluation metric not found, it is not supported yet.\"\n)\nroot = PROJ_DIR + root\nif meta_dict is None:\nself.dir_name = \"_\".join(name.split(\"-\"))  ## replace hyphen with underline\nmeta_dict = {\"dir_name\": self.dir_name}\nelse:\nself.dir_name = meta_dict[\"dir_name\"]\nself.root = osp.join(root, self.dir_name)\nself.meta_dict = meta_dict\nif \"fname\" not in self.meta_dict:\nself.meta_dict[\"fname\"] = self.root + \"/\" + self.name + \"_edgelist.csv\"\nself.meta_dict[\"nodefile\"] = self.root + \"/\" + self.name + \"_node_labels.csv\"\n#! version check\nself.version_passed = True\nself._version_check()\nself._num_classes = DATA_NUM_CLASSES[self.name]\n# initialize\nself._node_feat = None\nself._edge_feat = None\nself._full_data = None\nself.download()\n# check if the root directory exists, if not create it\nif osp.isdir(self.root):\nprint(\"Dataset directory is \", self.root)\nelse:\nraise FileNotFoundError(f\"Directory not found at {self.root}\")\nif preprocess:\nself.pre_process()\nself.label_ts_idx = 0  # index for which node lables to return now\ndef _version_check(self) -&gt; None:\nr\"\"\"Implement Version checks for dataset files\n        updates the file names based on the current version number\n        prompt the user to download the new version via self.version_passed variable\n        \"\"\"\nif (self.name in DATA_VERSION_DICT):\nversion = DATA_VERSION_DICT[self.name]\nelse:\nprint(f\"Dataset {self.name} version number not found.\")\nself.version_passed = False\nreturn None\nif (version &gt; 1):\n#* check if current version is outdated\nself.meta_dict[\"fname\"] = self.root + \"/\" + self.name + \"_edgelist_v\" + str(int(version)) + \".csv\"\nself.meta_dict[\"nodefile\"] = self.root + \"/\" + self.name + \"_node_labels_v\" + str(int(version)) + \".csv\"\nif (not osp.exists(self.meta_dict[\"fname\"])):\nprint(f\"Dataset {self.name} version {int(version)} not found.\")\nprint(f\"Please download the latest version of the dataset.\")\nself.version_passed = False\nreturn None\ndef download(self) -&gt; None:\nr\"\"\"\n        downloads this dataset from url\n        check if files are already downloaded\n        Returns:\n            None\n        \"\"\"\n# check if the file already exists\nif osp.exists(self.meta_dict[\"fname\"]) and osp.exists(\nself.meta_dict[\"nodefile\"]\n):\nprint(\"raw file found, skipping download\")\nreturn\nelse:\ninp = input(\n\"Will you download the dataset(s) now? (y/N)\\n\"\n).lower()  # ask if the user wants to download the dataset\nif inp == \"y\":\nprint(\nf\"{BColors.WARNING}Download started, this might take a while . . . {BColors.ENDC}\"\n)\nprint(f\"Dataset title: {self.name}\")\nif self.url is None:\nraise Exception(\n\"Dataset url not found, download not supported yet.\"\n)\nelse:\nr = requests.get(self.url, stream=True)\nif osp.isdir(self.root):\nprint(\"Dataset directory is \", self.root)\nelse:\nos.makedirs(self.root)\npath_download = self.root + \"/\" + self.name + \".zip\"\nwith open(path_download, \"wb\") as f:\ntotal_length = int(r.headers.get(\"content-length\"))\nfor chunk in progress.bar(\nr.iter_content(chunk_size=1024),\nexpected_size=(total_length / 1024) + 1,\n):\nif chunk:\nf.write(chunk)\nf.flush()\n# for unzipping the file\nwith zipfile.ZipFile(path_download, \"r\") as zip_ref:\nzip_ref.extractall(self.root)\nprint(f\"{BColors.OKGREEN}Download completed {BColors.ENDC}\")\nelse:\nraise Exception(\nBColors.FAIL\n+ \"Data not found error, download \"\n+ self.name\n+ \" failed\"\n)\ndef generate_processed_files(\nself,\n) -&gt; Tuple[pd.DataFrame, Dict[int, Dict[str, Any]]]:\nr\"\"\"\n        returns an edge list of pandas data frame\n        Returns:\n            df: pandas data frame storing the temporal edge list\n            node_label_dict: dictionary with key as timestamp and item as dictionary of node labels\n        \"\"\"\nOUT_DF = self.root + \"/\" + \"ml_{}.pkl\".format(self.name)\nOUT_NODE_DF = self.root + \"/\" + \"ml_{}_node.pkl\".format(self.name)\nOUT_LABEL_DF = self.root + \"/\" + \"ml_{}_label.pkl\".format(self.name)\nOUT_EDGE_FEAT = self.root + \"/\" + \"ml_{}.pkl\".format(self.name + \"_edge\")\n# * logic for large datasets, as node label file is too big to store on disc\nif self.name == \"tgbn-reddit\" or self.name == \"tgbn-token\":\nif osp.exists(OUT_DF) and osp.exists(OUT_NODE_DF) and osp.exists(OUT_EDGE_FEAT):\ndf = pd.read_pickle(OUT_DF)\nedge_feat = load_pkl(OUT_EDGE_FEAT)\nif (self.name == \"tgbn-token\"):\n#! taking log normalization for numerical stability\nprint (\"applying log normalization for weights in tgbn-token\")\nedge_feat[:,0] = np.log(edge_feat[:,0])\nnode_ids = load_pkl(OUT_NODE_DF)\nlabels_dict = load_pkl(OUT_LABEL_DF)\nnode_label_dict = load_label_dict(\nself.meta_dict[\"nodefile\"], node_ids, labels_dict\n)\nreturn df, node_label_dict, edge_feat\n# * load the preprocessed file if possible\nif osp.exists(OUT_DF) and osp.exists(OUT_NODE_DF) and osp.exists(OUT_EDGE_FEAT):\nprint(\"loading processed file\")\ndf = pd.read_pickle(OUT_DF)\nnode_label_dict = load_pkl(OUT_NODE_DF)\nedge_feat = load_pkl(OUT_EDGE_FEAT)\nelse:  # * process the file\nprint(\"file not processed, generating processed file\")\nif self.name == \"tgbn-reddit\":\ndf, edge_feat, node_ids, labels_dict = load_edgelist_sr(\nself.meta_dict[\"fname\"], label_size=self._num_classes\n)\nelif self.name == \"tgbn-token\":\ndf, edge_feat, node_ids, labels_dict = load_edgelist_token(\nself.meta_dict[\"fname\"], label_size=self._num_classes\n)\nelif self.name == \"tgbn-genre\":\ndf, edge_feat, node_ids, labels_dict = load_edgelist_datetime(\nself.meta_dict[\"fname\"], label_size=self._num_classes\n)\nelif self.name == \"tgbn-trade\":\ndf, edge_feat, node_ids = load_edgelist_trade(\nself.meta_dict[\"fname\"], label_size=self._num_classes\n)\ndf.to_pickle(OUT_DF)\nsave_pkl(edge_feat, OUT_EDGE_FEAT)\nif self.name == \"tgbn-trade\":\nnode_label_dict = load_trade_label_dict(\nself.meta_dict[\"nodefile\"], node_ids\n)\nelse:\nnode_label_dict = load_label_dict(\nself.meta_dict[\"nodefile\"], node_ids, labels_dict\n)\nif (\nself.name != \"tgbn-reddit\" and self.name != \"tgbn-token\"\n):  # don't save subreddits on disc, the node label file is too big\nsave_pkl(node_label_dict, OUT_NODE_DF)\nelse:\nsave_pkl(node_ids, OUT_NODE_DF)\nsave_pkl(labels_dict, OUT_LABEL_DF)\nprint(\"file processed and saved\")\nreturn df, node_label_dict, edge_feat\ndef pre_process(self) -&gt; None:\n\"\"\"\n        Pre-process the dataset and generates the splits, must be run before dataset properties can be accessed\n        Returns:\n            None\n        \"\"\"\n# first check if all files exist\nif (\"fname\" not in self.meta_dict) or (\"nodefile\" not in self.meta_dict):\nraise Exception(\"meta_dict does not contain all required filenames\")\ndf, node_label_dict, edge_feat = self.generate_processed_files()\nsources = np.array(df[\"u\"])\ndestinations = np.array(df[\"i\"])\ntimestamps = np.array(df[\"ts\"])\nedge_idxs = np.array(df[\"idx\"])\nedge_label = np.ones(sources.shape[0])\n#self._edge_feat = np.array(df[\"w\"])\nself._edge_feat = edge_feat\nfull_data = {\n\"sources\": sources,\n\"destinations\": destinations,\n\"timestamps\": timestamps,\n\"edge_idxs\": edge_idxs,\n\"edge_feat\": self._edge_feat,\n\"edge_label\": edge_label,\n}\nself._full_data = full_data\n# storing the split masks\n_train_mask, _val_mask, _test_mask = self.generate_splits(full_data)\nself._train_mask = _train_mask\nself._val_mask = _val_mask\nself._test_mask = _test_mask\nself.label_dict = node_label_dict\nself.label_ts = np.array(list(node_label_dict.keys()))\nself.label_ts = np.sort(self.label_ts)\ndef generate_splits(\nself,\nfull_data: Dict[str, Any],\nval_ratio: float = 0.15,\ntest_ratio: float = 0.15,\n) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\nr\"\"\"\n        Generates train, validation, and test splits from the full dataset\n        Parameters:\n            full_data: dictionary containing the full dataset\n            val_ratio: ratio of validation data\n            test_ratio: ratio of test data\n        Returns:\n            train_mask: boolean mask for training data\n            val_mask: boolean mask for validation data\n            test_mask: boolean mask for test data\n        \"\"\"\nval_time, test_time = list(\nnp.quantile(\nfull_data[\"timestamps\"],\n[(1 - val_ratio - test_ratio), (1 - test_ratio)],\n)\n)\ntimestamps = full_data[\"timestamps\"]\ntrain_mask = timestamps &lt;= val_time\nval_mask = np.logical_and(timestamps &lt;= test_time, timestamps &gt; val_time)\ntest_mask = timestamps &gt; test_time\nreturn train_mask, val_mask, test_mask\ndef find_next_labels_batch(\nself,\ncur_t: int,\n) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\nr\"\"\"\n        this returns the node labels closest to cur_t (for that given day)\n        Parameters:\n            cur_t: current timestamp of the batch of edges\n        Returns:\n            ts: timestamp of the node labels\n            source_idx: node ids\n            labels: the stacked label vectors\n        \"\"\"\nif self.label_ts_idx &gt;= (self.label_ts.shape[0]):\n# for query that are after the last batch of labels\nreturn None\nelse:\nts = self.label_ts[self.label_ts_idx]\nif cur_t &gt;= ts:\nself.label_ts_idx += 1  # move to the next ts\n# {ts: {node_id: label_vec}}\nnode_ids = np.array(list(self.label_dict[ts].keys()))\nnode_labels = []\nfor key in self.label_dict[ts]:\nnode_labels.append(np.array(self.label_dict[ts][key]))\nnode_labels = np.stack(node_labels, axis=0)\nlabel_ts = np.full(node_ids.shape[0], ts, dtype=\"int\")\nreturn (label_ts, node_ids, node_labels)\nelse:\nreturn None\ndef reset_label_time(self) -&gt; None:\nr\"\"\"\n        reset the pointer for node label once the entire dataset has been iterated once\n        Returns:\n            None\n        \"\"\"\nself.label_ts_idx = 0\ndef return_label_ts(self) -&gt; int:\n\"\"\"\n        return the current label timestamp that the pointer is at\n        Returns:\n            ts: int, the timestamp of the node labels\n        \"\"\"\nif (self.label_ts_idx &gt;= self.label_ts.shape[0]):\nreturn self.label_ts[-1]\nelse:\nreturn self.label_ts[self.label_ts_idx]\n@property\ndef num_classes(self) -&gt; int:\n\"\"\"\n        number of classes in the node label\n        Returns:\n            num_classes: int, number of classes\n        \"\"\"\nreturn self._num_classes\n@property\ndef eval_metric(self) -&gt; str:\n\"\"\"\n        the official evaluation metric for the dataset, loaded from info.py\n        Returns:\n            eval_metric: str, the evaluation metric\n        \"\"\"\nreturn self.metric\n# TODO not sure needed, to be removed\n@property\ndef node_feat(self) -&gt; Optional[np.ndarray]:\nr\"\"\"\n        Returns the node features of the dataset with dim [N, feat_dim]\n        Returns:\n            node_feat: np.ndarray, [N, feat_dim] or None if there is no node feature\n        \"\"\"\nreturn self._node_feat\n# TODO not sure needed, to be removed\n@property\ndef edge_feat(self) -&gt; Optional[np.ndarray]:\nr\"\"\"\n        Returns the edge features of the dataset with dim [E, feat_dim]\n        Returns:\n            edge_feat: np.ndarray, [E, feat_dim] or None if there is no edge feature\n        \"\"\"\nreturn self._edge_feat\n@property\ndef full_data(self) -&gt; Dict[str, Any]:\nr\"\"\"\n        the full data of the dataset as a dictionary with keys: 'sources', 'destinations', 'timestamps', 'edge_idxs', 'edge_feat', 'w', 'edge_label',\n        Returns:\n            full_data: Dict[str, Any]\n        \"\"\"\nif self._full_data is None:\nraise ValueError(\n\"dataset has not been processed yet, please call pre_process() first\"\n)\nreturn self._full_data\n@property\ndef train_mask(self) -&gt; np.ndarray:\nr\"\"\"\n        Returns the train mask of the dataset\n        Returns:\n            train_mask\n        \"\"\"\nif self._train_mask is None:\nraise ValueError(\"training split hasn't been loaded\")\nreturn self._train_mask\n@property\ndef val_mask(self) -&gt; np.ndarray:\nr\"\"\"\n        Returns the validation mask of the dataset\n        Returns:\n            val_mask: Dict[str, Any]\n        \"\"\"\nif self._val_mask is None:\nraise ValueError(\"validation split hasn't been loaded\")\nreturn self._val_mask\n@property\ndef test_mask(self) -&gt; np.ndarray:\nr\"\"\"\n        Returns the test mask of the dataset:\n        Returns:\n            test_mask: Dict[str, Any]\n        \"\"\"\nif self._test_mask is None:\nraise ValueError(\"test split hasn't been loaded\")\nreturn self._test_mask\n</code></pre>"},{"location":"api/tgb.nodeproppred/#tgb.nodeproppred.dataset.NodePropPredDataset.edge_feat","title":"<code>edge_feat: Optional[np.ndarray]</code>  <code>property</code>","text":"<p>Returns the edge features of the dataset with dim [E, feat_dim]</p> <p>Returns:</p> Name Type Description <code>edge_feat</code> <code>Optional[np.ndarray]</code> <p>np.ndarray, [E, feat_dim] or None if there is no edge feature</p>"},{"location":"api/tgb.nodeproppred/#tgb.nodeproppred.dataset.NodePropPredDataset.eval_metric","title":"<code>eval_metric: str</code>  <code>property</code>","text":"<p>the official evaluation metric for the dataset, loaded from info.py</p> <p>Returns:</p> Name Type Description <code>eval_metric</code> <code>str</code> <p>str, the evaluation metric</p>"},{"location":"api/tgb.nodeproppred/#tgb.nodeproppred.dataset.NodePropPredDataset.full_data","title":"<code>full_data: Dict[str, Any]</code>  <code>property</code>","text":"<p>the full data of the dataset as a dictionary with keys: 'sources', 'destinations', 'timestamps', 'edge_idxs', 'edge_feat', 'w', 'edge_label',</p> <p>Returns:</p> Name Type Description <code>full_data</code> <code>Dict[str, Any]</code> <p>Dict[str, Any]</p>"},{"location":"api/tgb.nodeproppred/#tgb.nodeproppred.dataset.NodePropPredDataset.node_feat","title":"<code>node_feat: Optional[np.ndarray]</code>  <code>property</code>","text":"<p>Returns the node features of the dataset with dim [N, feat_dim]</p> <p>Returns:</p> Name Type Description <code>node_feat</code> <code>Optional[np.ndarray]</code> <p>np.ndarray, [N, feat_dim] or None if there is no node feature</p>"},{"location":"api/tgb.nodeproppred/#tgb.nodeproppred.dataset.NodePropPredDataset.num_classes","title":"<code>num_classes: int</code>  <code>property</code>","text":"<p>number of classes in the node label</p> <p>Returns:</p> Name Type Description <code>num_classes</code> <code>int</code> <p>int, number of classes</p>"},{"location":"api/tgb.nodeproppred/#tgb.nodeproppred.dataset.NodePropPredDataset.test_mask","title":"<code>test_mask: np.ndarray</code>  <code>property</code>","text":"<p>Returns the test mask of the dataset:</p> <p>Returns:</p> Name Type Description <code>test_mask</code> <code>np.ndarray</code> <p>Dict[str, Any]</p>"},{"location":"api/tgb.nodeproppred/#tgb.nodeproppred.dataset.NodePropPredDataset.train_mask","title":"<code>train_mask: np.ndarray</code>  <code>property</code>","text":"<p>Returns the train mask of the dataset</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>train_mask</p>"},{"location":"api/tgb.nodeproppred/#tgb.nodeproppred.dataset.NodePropPredDataset.val_mask","title":"<code>val_mask: np.ndarray</code>  <code>property</code>","text":"<p>Returns the validation mask of the dataset</p> <p>Returns:</p> Name Type Description <code>val_mask</code> <code>np.ndarray</code> <p>Dict[str, Any]</p>"},{"location":"api/tgb.nodeproppred/#tgb.nodeproppred.dataset.NodePropPredDataset.__init__","title":"<code>__init__(name, root='datasets', meta_dict=None, preprocess=True)</code>","text":"<p>Dataset class for the node property prediction task. Stores meta information about each dataset such as evaluation metrics etc. also automatically pre-processes the dataset. [!] node property prediction datasets requires the following: self.meta_dict[\"fname\"]: path to the edge list file self.meta_dict[\"nodefile\"]: path to the node label file</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>name of the dataset</p> required <code>root</code> <code>Optional[str]</code> <p>root directory to store the dataset folder</p> <code>'datasets'</code> <code>meta_dict</code> <code>Optional[dict]</code> <p>dictionary containing meta information about the dataset, should contain key 'dir_name' which is the name of the dataset folder</p> <code>None</code> <code>preprocess</code> <code>Optional[bool]</code> <p>whether to pre-process the dataset</p> <code>True</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>tgb/nodeproppred/dataset.py</code> <pre><code>def __init__(\nself,\nname: str,\nroot: Optional[str] = \"datasets\",\nmeta_dict: Optional[dict] = None,\npreprocess: Optional[bool] = True,\n) -&gt; None:\nr\"\"\"Dataset class for the node property prediction task. Stores meta information about each dataset such as evaluation metrics etc.\n    also automatically pre-processes the dataset.\n    [!] node property prediction datasets requires the following:\n    self.meta_dict[\"fname\"]: path to the edge list file\n    self.meta_dict[\"nodefile\"]: path to the node label file\n    Parameters:\n        name: name of the dataset\n        root: root directory to store the dataset folder\n        meta_dict: dictionary containing meta information about the dataset, should contain key 'dir_name' which is the name of the dataset folder\n        preprocess: whether to pre-process the dataset\n    Returns:\n        None\n    \"\"\"\nself.name = name  ## original name\n# check if dataset url exist\nif self.name in DATA_URL_DICT:\nself.url = DATA_URL_DICT[self.name]\nelse:\nself.url = None\nprint(f\"Dataset {self.name} url not found, download not supported yet.\")\n# check if the evaluatioin metric are specified\nif self.name in DATA_EVAL_METRIC_DICT:\nself.metric = DATA_EVAL_METRIC_DICT[self.name]\nelse:\nself.metric = None\nprint(\nf\"Dataset {self.name} default evaluation metric not found, it is not supported yet.\"\n)\nroot = PROJ_DIR + root\nif meta_dict is None:\nself.dir_name = \"_\".join(name.split(\"-\"))  ## replace hyphen with underline\nmeta_dict = {\"dir_name\": self.dir_name}\nelse:\nself.dir_name = meta_dict[\"dir_name\"]\nself.root = osp.join(root, self.dir_name)\nself.meta_dict = meta_dict\nif \"fname\" not in self.meta_dict:\nself.meta_dict[\"fname\"] = self.root + \"/\" + self.name + \"_edgelist.csv\"\nself.meta_dict[\"nodefile\"] = self.root + \"/\" + self.name + \"_node_labels.csv\"\n#! version check\nself.version_passed = True\nself._version_check()\nself._num_classes = DATA_NUM_CLASSES[self.name]\n# initialize\nself._node_feat = None\nself._edge_feat = None\nself._full_data = None\nself.download()\n# check if the root directory exists, if not create it\nif osp.isdir(self.root):\nprint(\"Dataset directory is \", self.root)\nelse:\nraise FileNotFoundError(f\"Directory not found at {self.root}\")\nif preprocess:\nself.pre_process()\nself.label_ts_idx = 0  # index for which node lables to return now\n</code></pre>"},{"location":"api/tgb.nodeproppred/#tgb.nodeproppred.dataset.NodePropPredDataset.download","title":"<code>download()</code>","text":"<p>downloads this dataset from url check if files are already downloaded</p> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>tgb/nodeproppred/dataset.py</code> <pre><code>def download(self) -&gt; None:\nr\"\"\"\n    downloads this dataset from url\n    check if files are already downloaded\n    Returns:\n        None\n    \"\"\"\n# check if the file already exists\nif osp.exists(self.meta_dict[\"fname\"]) and osp.exists(\nself.meta_dict[\"nodefile\"]\n):\nprint(\"raw file found, skipping download\")\nreturn\nelse:\ninp = input(\n\"Will you download the dataset(s) now? (y/N)\\n\"\n).lower()  # ask if the user wants to download the dataset\nif inp == \"y\":\nprint(\nf\"{BColors.WARNING}Download started, this might take a while . . . {BColors.ENDC}\"\n)\nprint(f\"Dataset title: {self.name}\")\nif self.url is None:\nraise Exception(\n\"Dataset url not found, download not supported yet.\"\n)\nelse:\nr = requests.get(self.url, stream=True)\nif osp.isdir(self.root):\nprint(\"Dataset directory is \", self.root)\nelse:\nos.makedirs(self.root)\npath_download = self.root + \"/\" + self.name + \".zip\"\nwith open(path_download, \"wb\") as f:\ntotal_length = int(r.headers.get(\"content-length\"))\nfor chunk in progress.bar(\nr.iter_content(chunk_size=1024),\nexpected_size=(total_length / 1024) + 1,\n):\nif chunk:\nf.write(chunk)\nf.flush()\n# for unzipping the file\nwith zipfile.ZipFile(path_download, \"r\") as zip_ref:\nzip_ref.extractall(self.root)\nprint(f\"{BColors.OKGREEN}Download completed {BColors.ENDC}\")\nelse:\nraise Exception(\nBColors.FAIL\n+ \"Data not found error, download \"\n+ self.name\n+ \" failed\"\n)\n</code></pre>"},{"location":"api/tgb.nodeproppred/#tgb.nodeproppred.dataset.NodePropPredDataset.find_next_labels_batch","title":"<code>find_next_labels_batch(cur_t)</code>","text":"<p>this returns the node labels closest to cur_t (for that given day)</p> <p>Parameters:</p> Name Type Description Default <code>cur_t</code> <code>int</code> <p>current timestamp of the batch of edges</p> required <p>Returns:</p> Name Type Description <code>ts</code> <code>np.ndarray</code> <p>timestamp of the node labels</p> <code>source_idx</code> <code>np.ndarray</code> <p>node ids</p> <code>labels</code> <code>np.ndarray</code> <p>the stacked label vectors</p> Source code in <code>tgb/nodeproppred/dataset.py</code> <pre><code>def find_next_labels_batch(\nself,\ncur_t: int,\n) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\nr\"\"\"\n    this returns the node labels closest to cur_t (for that given day)\n    Parameters:\n        cur_t: current timestamp of the batch of edges\n    Returns:\n        ts: timestamp of the node labels\n        source_idx: node ids\n        labels: the stacked label vectors\n    \"\"\"\nif self.label_ts_idx &gt;= (self.label_ts.shape[0]):\n# for query that are after the last batch of labels\nreturn None\nelse:\nts = self.label_ts[self.label_ts_idx]\nif cur_t &gt;= ts:\nself.label_ts_idx += 1  # move to the next ts\n# {ts: {node_id: label_vec}}\nnode_ids = np.array(list(self.label_dict[ts].keys()))\nnode_labels = []\nfor key in self.label_dict[ts]:\nnode_labels.append(np.array(self.label_dict[ts][key]))\nnode_labels = np.stack(node_labels, axis=0)\nlabel_ts = np.full(node_ids.shape[0], ts, dtype=\"int\")\nreturn (label_ts, node_ids, node_labels)\nelse:\nreturn None\n</code></pre>"},{"location":"api/tgb.nodeproppred/#tgb.nodeproppred.dataset.NodePropPredDataset.generate_processed_files","title":"<code>generate_processed_files()</code>","text":"<p>returns an edge list of pandas data frame</p> <p>Returns:</p> Name Type Description <code>df</code> <code>pd.DataFrame</code> <p>pandas data frame storing the temporal edge list</p> <code>node_label_dict</code> <code>Dict[int, Dict[str, Any]]</code> <p>dictionary with key as timestamp and item as dictionary of node labels</p> Source code in <code>tgb/nodeproppred/dataset.py</code> <pre><code>def generate_processed_files(\nself,\n) -&gt; Tuple[pd.DataFrame, Dict[int, Dict[str, Any]]]:\nr\"\"\"\n    returns an edge list of pandas data frame\n    Returns:\n        df: pandas data frame storing the temporal edge list\n        node_label_dict: dictionary with key as timestamp and item as dictionary of node labels\n    \"\"\"\nOUT_DF = self.root + \"/\" + \"ml_{}.pkl\".format(self.name)\nOUT_NODE_DF = self.root + \"/\" + \"ml_{}_node.pkl\".format(self.name)\nOUT_LABEL_DF = self.root + \"/\" + \"ml_{}_label.pkl\".format(self.name)\nOUT_EDGE_FEAT = self.root + \"/\" + \"ml_{}.pkl\".format(self.name + \"_edge\")\n# * logic for large datasets, as node label file is too big to store on disc\nif self.name == \"tgbn-reddit\" or self.name == \"tgbn-token\":\nif osp.exists(OUT_DF) and osp.exists(OUT_NODE_DF) and osp.exists(OUT_EDGE_FEAT):\ndf = pd.read_pickle(OUT_DF)\nedge_feat = load_pkl(OUT_EDGE_FEAT)\nif (self.name == \"tgbn-token\"):\n#! taking log normalization for numerical stability\nprint (\"applying log normalization for weights in tgbn-token\")\nedge_feat[:,0] = np.log(edge_feat[:,0])\nnode_ids = load_pkl(OUT_NODE_DF)\nlabels_dict = load_pkl(OUT_LABEL_DF)\nnode_label_dict = load_label_dict(\nself.meta_dict[\"nodefile\"], node_ids, labels_dict\n)\nreturn df, node_label_dict, edge_feat\n# * load the preprocessed file if possible\nif osp.exists(OUT_DF) and osp.exists(OUT_NODE_DF) and osp.exists(OUT_EDGE_FEAT):\nprint(\"loading processed file\")\ndf = pd.read_pickle(OUT_DF)\nnode_label_dict = load_pkl(OUT_NODE_DF)\nedge_feat = load_pkl(OUT_EDGE_FEAT)\nelse:  # * process the file\nprint(\"file not processed, generating processed file\")\nif self.name == \"tgbn-reddit\":\ndf, edge_feat, node_ids, labels_dict = load_edgelist_sr(\nself.meta_dict[\"fname\"], label_size=self._num_classes\n)\nelif self.name == \"tgbn-token\":\ndf, edge_feat, node_ids, labels_dict = load_edgelist_token(\nself.meta_dict[\"fname\"], label_size=self._num_classes\n)\nelif self.name == \"tgbn-genre\":\ndf, edge_feat, node_ids, labels_dict = load_edgelist_datetime(\nself.meta_dict[\"fname\"], label_size=self._num_classes\n)\nelif self.name == \"tgbn-trade\":\ndf, edge_feat, node_ids = load_edgelist_trade(\nself.meta_dict[\"fname\"], label_size=self._num_classes\n)\ndf.to_pickle(OUT_DF)\nsave_pkl(edge_feat, OUT_EDGE_FEAT)\nif self.name == \"tgbn-trade\":\nnode_label_dict = load_trade_label_dict(\nself.meta_dict[\"nodefile\"], node_ids\n)\nelse:\nnode_label_dict = load_label_dict(\nself.meta_dict[\"nodefile\"], node_ids, labels_dict\n)\nif (\nself.name != \"tgbn-reddit\" and self.name != \"tgbn-token\"\n):  # don't save subreddits on disc, the node label file is too big\nsave_pkl(node_label_dict, OUT_NODE_DF)\nelse:\nsave_pkl(node_ids, OUT_NODE_DF)\nsave_pkl(labels_dict, OUT_LABEL_DF)\nprint(\"file processed and saved\")\nreturn df, node_label_dict, edge_feat\n</code></pre>"},{"location":"api/tgb.nodeproppred/#tgb.nodeproppred.dataset.NodePropPredDataset.generate_splits","title":"<code>generate_splits(full_data, val_ratio=0.15, test_ratio=0.15)</code>","text":"<p>Generates train, validation, and test splits from the full dataset</p> <p>Parameters:</p> Name Type Description Default <code>full_data</code> <code>Dict[str, Any]</code> <p>dictionary containing the full dataset</p> required <code>val_ratio</code> <code>float</code> <p>ratio of validation data</p> <code>0.15</code> <code>test_ratio</code> <code>float</code> <p>ratio of test data</p> <code>0.15</code> <p>Returns:</p> Name Type Description <code>train_mask</code> <code>np.ndarray</code> <p>boolean mask for training data</p> <code>val_mask</code> <code>np.ndarray</code> <p>boolean mask for validation data</p> <code>test_mask</code> <code>np.ndarray</code> <p>boolean mask for test data</p> Source code in <code>tgb/nodeproppred/dataset.py</code> <pre><code>def generate_splits(\nself,\nfull_data: Dict[str, Any],\nval_ratio: float = 0.15,\ntest_ratio: float = 0.15,\n) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\nr\"\"\"\n    Generates train, validation, and test splits from the full dataset\n    Parameters:\n        full_data: dictionary containing the full dataset\n        val_ratio: ratio of validation data\n        test_ratio: ratio of test data\n    Returns:\n        train_mask: boolean mask for training data\n        val_mask: boolean mask for validation data\n        test_mask: boolean mask for test data\n    \"\"\"\nval_time, test_time = list(\nnp.quantile(\nfull_data[\"timestamps\"],\n[(1 - val_ratio - test_ratio), (1 - test_ratio)],\n)\n)\ntimestamps = full_data[\"timestamps\"]\ntrain_mask = timestamps &lt;= val_time\nval_mask = np.logical_and(timestamps &lt;= test_time, timestamps &gt; val_time)\ntest_mask = timestamps &gt; test_time\nreturn train_mask, val_mask, test_mask\n</code></pre>"},{"location":"api/tgb.nodeproppred/#tgb.nodeproppred.dataset.NodePropPredDataset.pre_process","title":"<code>pre_process()</code>","text":"<p>Pre-process the dataset and generates the splits, must be run before dataset properties can be accessed</p> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>tgb/nodeproppred/dataset.py</code> <pre><code>def pre_process(self) -&gt; None:\n\"\"\"\n    Pre-process the dataset and generates the splits, must be run before dataset properties can be accessed\n    Returns:\n        None\n    \"\"\"\n# first check if all files exist\nif (\"fname\" not in self.meta_dict) or (\"nodefile\" not in self.meta_dict):\nraise Exception(\"meta_dict does not contain all required filenames\")\ndf, node_label_dict, edge_feat = self.generate_processed_files()\nsources = np.array(df[\"u\"])\ndestinations = np.array(df[\"i\"])\ntimestamps = np.array(df[\"ts\"])\nedge_idxs = np.array(df[\"idx\"])\nedge_label = np.ones(sources.shape[0])\n#self._edge_feat = np.array(df[\"w\"])\nself._edge_feat = edge_feat\nfull_data = {\n\"sources\": sources,\n\"destinations\": destinations,\n\"timestamps\": timestamps,\n\"edge_idxs\": edge_idxs,\n\"edge_feat\": self._edge_feat,\n\"edge_label\": edge_label,\n}\nself._full_data = full_data\n# storing the split masks\n_train_mask, _val_mask, _test_mask = self.generate_splits(full_data)\nself._train_mask = _train_mask\nself._val_mask = _val_mask\nself._test_mask = _test_mask\nself.label_dict = node_label_dict\nself.label_ts = np.array(list(node_label_dict.keys()))\nself.label_ts = np.sort(self.label_ts)\n</code></pre>"},{"location":"api/tgb.nodeproppred/#tgb.nodeproppred.dataset.NodePropPredDataset.reset_label_time","title":"<code>reset_label_time()</code>","text":"<p>reset the pointer for node label once the entire dataset has been iterated once</p> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>tgb/nodeproppred/dataset.py</code> <pre><code>def reset_label_time(self) -&gt; None:\nr\"\"\"\n    reset the pointer for node label once the entire dataset has been iterated once\n    Returns:\n        None\n    \"\"\"\nself.label_ts_idx = 0\n</code></pre>"},{"location":"api/tgb.nodeproppred/#tgb.nodeproppred.dataset.NodePropPredDataset.return_label_ts","title":"<code>return_label_ts()</code>","text":"<p>return the current label timestamp that the pointer is at</p> <p>Returns:</p> Name Type Description <code>ts</code> <code>int</code> <p>int, the timestamp of the node labels</p> Source code in <code>tgb/nodeproppred/dataset.py</code> <pre><code>def return_label_ts(self) -&gt; int:\n\"\"\"\n    return the current label timestamp that the pointer is at\n    Returns:\n        ts: int, the timestamp of the node labels\n    \"\"\"\nif (self.label_ts_idx &gt;= self.label_ts.shape[0]):\nreturn self.label_ts[-1]\nelse:\nreturn self.label_ts[self.label_ts_idx]\n</code></pre>"},{"location":"api/tgb.nodeproppred/#tgb.nodeproppred.dataset_pyg.PyGNodePropPredDataset","title":"<code>PyGNodePropPredDataset</code>","text":"<p>             Bases: <code>InMemoryDataset</code></p> <p>PyG wrapper for the NodePropPredDataset can return pytorch tensors for src,dst,t,msg,label can return Temporal Data object also query the node labels corresponding to a timestamp from edge batch</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>name of the dataset, passed to <code>NodePropPredDataset</code></p> required <code>root</code> <code>string</code> <p>Root directory where the dataset should be saved.</p> required <code>transform</code> <code>callable</code> <p>A function/transform that takes in an</p> <code>None</code> <code>pre_transform</code> <code>callable</code> <p>A function/transform that takes in</p> <code>None</code> Source code in <code>tgb/nodeproppred/dataset_pyg.py</code> <pre><code>class PyGNodePropPredDataset(InMemoryDataset):\nr\"\"\"\n    PyG wrapper for the NodePropPredDataset\n    can return pytorch tensors for src,dst,t,msg,label\n    can return Temporal Data object\n    also query the node labels corresponding to a timestamp from edge batch\n    Parameters:\n        name: name of the dataset, passed to `NodePropPredDataset`\n        root (string): Root directory where the dataset should be saved.\n        transform (callable, optional): A function/transform that takes in an\n        pre_transform (callable, optional): A function/transform that takes in\n    \"\"\"\ndef __init__(\nself,\nname: str,\nroot: str,\ntransform: Optional[Callable] = None,\npre_transform: Optional[Callable] = None,\n):\nself.name = name\nself.root = root\nself.dataset = NodePropPredDataset(name=name, root=root)\nself._train_mask = torch.from_numpy(self.dataset.train_mask)\nself._val_mask = torch.from_numpy(self.dataset.val_mask)\nself._test_mask = torch.from_numpy(self.dataset.test_mask)\nself.__num_classes = self.dataset.num_classes\nsuper().__init__(root, transform, pre_transform)\nself.process_data()\n@property\ndef num_classes(self) -&gt; int:\n\"\"\"\n        how many classes are in the node label\n        Returns:\n            num_classes: int\n        \"\"\"\nreturn self.__num_classes\n@property\ndef eval_metric(self) -&gt; str:\n\"\"\"\n        the official evaluation metric for the dataset, loaded from info.py\n        Returns:\n            eval_metric: str, the evaluation metric\n        \"\"\"\nreturn self.dataset.eval_metric\n@property\ndef train_mask(self) -&gt; torch.Tensor:\nr\"\"\"\n        Returns the train mask of the dataset\n        Returns:\n            train_mask: the mask for edges in the training set\n        \"\"\"\nif self._train_mask is None:\nraise ValueError(\"training split hasn't been loaded\")\nreturn self._train_mask\n@property\ndef val_mask(self) -&gt; torch.Tensor:\nr\"\"\"\n        Returns the validation mask of the dataset\n        Returns:\n            val_mask: the mask for edges in the validation set\n        \"\"\"\nif self._val_mask is None:\nraise ValueError(\"validation split hasn't been loaded\")\nreturn self._val_mask\n@property\ndef test_mask(self) -&gt; torch.Tensor:\nr\"\"\"\n        Returns the test mask of the dataset:\n        Returns:\n            test_mask: the mask for edges in the test set\n        \"\"\"\nif self._test_mask is None:\nraise ValueError(\"test split hasn't been loaded\")\nreturn self._test_mask\n@property\ndef src(self) -&gt; torch.Tensor:\nr\"\"\"\n        Returns the source nodes of the dataset\n        Returns:\n            src: the idx of the source nodes\n        \"\"\"\nreturn self._src\n@property\ndef dst(self) -&gt; torch.Tensor:\nr\"\"\"\n        Returns the destination nodes of the dataset\n        Returns:\n            dst: the idx of the destination nodes\n        \"\"\"\nreturn self._dst\n@property\ndef ts(self) -&gt; torch.Tensor:\nr\"\"\"\n        Returns the timestamps of the dataset\n        Returns:\n            ts: the timestamps of the edges\n        \"\"\"\nreturn self._ts\n@property\ndef edge_feat(self) -&gt; torch.Tensor:\nr\"\"\"\n        Returns the edge features of the dataset\n        Returns:\n            edge_feat: the edge features\n        \"\"\"\nreturn self._edge_feat\n@property\ndef edge_label(self) -&gt; torch.Tensor:\nr\"\"\"\n        Returns the edge labels of the dataset\n        Returns:\n            edge_label: the labels of the edges (all one tensor)\n        \"\"\"\nreturn self._edge_label\ndef process_data(self):\n\"\"\"\n        convert data to pytorch tensors\n        \"\"\"\nsrc = torch.from_numpy(self.dataset.full_data[\"sources\"])\ndst = torch.from_numpy(self.dataset.full_data[\"destinations\"])\nt = torch.from_numpy(self.dataset.full_data[\"timestamps\"])\nedge_label = torch.from_numpy(self.dataset.full_data[\"edge_label\"])\nmsg = torch.from_numpy(self.dataset.full_data[\"edge_feat\"])\n# msg = torch.from_numpy(self.dataset.full_data[\"edge_feat\"]).reshape(\n#     [-1, 1]\n# ) \n# * check typing\nif src.dtype != torch.int64:\nsrc = src.long()\nif dst.dtype != torch.int64:\ndst = dst.long()\nif t.dtype != torch.int64:\nt = t.long()\nif msg.dtype != torch.float32:\nmsg = msg.float()\nself._src = src\nself._dst = dst\nself._ts = t\nself._edge_label = edge_label\nself._edge_feat = msg\ndef get_TemporalData(\nself,\n) -&gt; TemporalData:\n\"\"\"\n        return the TemporalData object for the entire dataset\n        Returns:\n            data: TemporalData object storing the edgelist\n        \"\"\"\ndata = TemporalData(\nsrc=self._src,\ndst=self._dst,\nt=self._ts,\nmsg=self._edge_feat,\ny=self._edge_label,\n)\nreturn data\ndef reset_label_time(self) -&gt; None:\n\"\"\"\n        reset the pointer for the node labels, should be done per epoch\n        \"\"\"\nself.dataset.reset_label_time()\ndef get_node_label(self, cur_t):\n\"\"\"\n        return the node labels for the current timestamp\n        \"\"\"\nlabel_tuple = self.dataset.find_next_labels_batch(cur_t)\nif label_tuple is None:\nreturn None\nlabel_ts, label_srcs, labels = label_tuple[0], label_tuple[1], label_tuple[2]\nlabel_ts = torch.from_numpy(label_ts).long()\nlabel_srcs = torch.from_numpy(label_srcs).long()\nlabels = torch.from_numpy(labels).to(torch.float32)\nreturn label_ts, label_srcs, labels\ndef get_label_time(self) -&gt; int:\n\"\"\"\n        return the timestamps of the current node labels\n        Returns:\n            t: time of the current node labels\n        \"\"\"\nreturn self.dataset.return_label_ts()\ndef len(self) -&gt; int:\n\"\"\"\n        size of the dataset\n        Returns:\n            size: int\n        \"\"\"\nreturn self._src.shape[0]\ndef get(self, idx: int) -&gt; TemporalData:\n\"\"\"\n        construct temporal data object for a single edge\n        Parameters:\n            idx: index of the edge\n        Returns:\n            data: TemporalData object\n        \"\"\"\ndata = TemporalData(\nsrc=self._src[idx],\ndst=self._dst[idx],\nt=self._ts[idx],\nmsg=self._edge_feat[idx],\ny=self._edge_label[idx],\n)\nreturn data\ndef __repr__(self) -&gt; str:\nreturn f\"{self.name.capitalize()}()\"\n</code></pre>"},{"location":"api/tgb.nodeproppred/#tgb.nodeproppred.dataset_pyg.PyGNodePropPredDataset.dst","title":"<code>dst: torch.Tensor</code>  <code>property</code>","text":"<p>Returns the destination nodes of the dataset</p> <p>Returns:</p> Name Type Description <code>dst</code> <code>torch.Tensor</code> <p>the idx of the destination nodes</p>"},{"location":"api/tgb.nodeproppred/#tgb.nodeproppred.dataset_pyg.PyGNodePropPredDataset.edge_feat","title":"<code>edge_feat: torch.Tensor</code>  <code>property</code>","text":"<p>Returns the edge features of the dataset</p> <p>Returns:</p> Name Type Description <code>edge_feat</code> <code>torch.Tensor</code> <p>the edge features</p>"},{"location":"api/tgb.nodeproppred/#tgb.nodeproppred.dataset_pyg.PyGNodePropPredDataset.edge_label","title":"<code>edge_label: torch.Tensor</code>  <code>property</code>","text":"<p>Returns the edge labels of the dataset</p> <p>Returns:</p> Name Type Description <code>edge_label</code> <code>torch.Tensor</code> <p>the labels of the edges (all one tensor)</p>"},{"location":"api/tgb.nodeproppred/#tgb.nodeproppred.dataset_pyg.PyGNodePropPredDataset.eval_metric","title":"<code>eval_metric: str</code>  <code>property</code>","text":"<p>the official evaluation metric for the dataset, loaded from info.py</p> <p>Returns:</p> Name Type Description <code>eval_metric</code> <code>str</code> <p>str, the evaluation metric</p>"},{"location":"api/tgb.nodeproppred/#tgb.nodeproppred.dataset_pyg.PyGNodePropPredDataset.num_classes","title":"<code>num_classes: int</code>  <code>property</code>","text":"<p>how many classes are in the node label</p> <p>Returns:</p> Name Type Description <code>num_classes</code> <code>int</code> <p>int</p>"},{"location":"api/tgb.nodeproppred/#tgb.nodeproppred.dataset_pyg.PyGNodePropPredDataset.src","title":"<code>src: torch.Tensor</code>  <code>property</code>","text":"<p>Returns the source nodes of the dataset</p> <p>Returns:</p> Name Type Description <code>src</code> <code>torch.Tensor</code> <p>the idx of the source nodes</p>"},{"location":"api/tgb.nodeproppred/#tgb.nodeproppred.dataset_pyg.PyGNodePropPredDataset.test_mask","title":"<code>test_mask: torch.Tensor</code>  <code>property</code>","text":"<p>Returns the test mask of the dataset:</p> <p>Returns:</p> Name Type Description <code>test_mask</code> <code>torch.Tensor</code> <p>the mask for edges in the test set</p>"},{"location":"api/tgb.nodeproppred/#tgb.nodeproppred.dataset_pyg.PyGNodePropPredDataset.train_mask","title":"<code>train_mask: torch.Tensor</code>  <code>property</code>","text":"<p>Returns the train mask of the dataset</p> <p>Returns:</p> Name Type Description <code>train_mask</code> <code>torch.Tensor</code> <p>the mask for edges in the training set</p>"},{"location":"api/tgb.nodeproppred/#tgb.nodeproppred.dataset_pyg.PyGNodePropPredDataset.ts","title":"<code>ts: torch.Tensor</code>  <code>property</code>","text":"<p>Returns the timestamps of the dataset</p> <p>Returns:</p> Name Type Description <code>ts</code> <code>torch.Tensor</code> <p>the timestamps of the edges</p>"},{"location":"api/tgb.nodeproppred/#tgb.nodeproppred.dataset_pyg.PyGNodePropPredDataset.val_mask","title":"<code>val_mask: torch.Tensor</code>  <code>property</code>","text":"<p>Returns the validation mask of the dataset</p> <p>Returns:</p> Name Type Description <code>val_mask</code> <code>torch.Tensor</code> <p>the mask for edges in the validation set</p>"},{"location":"api/tgb.nodeproppred/#tgb.nodeproppred.dataset_pyg.PyGNodePropPredDataset.get","title":"<code>get(idx)</code>","text":"<p>construct temporal data object for a single edge</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>index of the edge</p> required <p>Returns:</p> Name Type Description <code>data</code> <code>TemporalData</code> <p>TemporalData object</p> Source code in <code>tgb/nodeproppred/dataset_pyg.py</code> <pre><code>def get(self, idx: int) -&gt; TemporalData:\n\"\"\"\n    construct temporal data object for a single edge\n    Parameters:\n        idx: index of the edge\n    Returns:\n        data: TemporalData object\n    \"\"\"\ndata = TemporalData(\nsrc=self._src[idx],\ndst=self._dst[idx],\nt=self._ts[idx],\nmsg=self._edge_feat[idx],\ny=self._edge_label[idx],\n)\nreturn data\n</code></pre>"},{"location":"api/tgb.nodeproppred/#tgb.nodeproppred.dataset_pyg.PyGNodePropPredDataset.get_TemporalData","title":"<code>get_TemporalData()</code>","text":"<p>return the TemporalData object for the entire dataset</p> <p>Returns:</p> Name Type Description <code>data</code> <code>TemporalData</code> <p>TemporalData object storing the edgelist</p> Source code in <code>tgb/nodeproppred/dataset_pyg.py</code> <pre><code>def get_TemporalData(\nself,\n) -&gt; TemporalData:\n\"\"\"\n    return the TemporalData object for the entire dataset\n    Returns:\n        data: TemporalData object storing the edgelist\n    \"\"\"\ndata = TemporalData(\nsrc=self._src,\ndst=self._dst,\nt=self._ts,\nmsg=self._edge_feat,\ny=self._edge_label,\n)\nreturn data\n</code></pre>"},{"location":"api/tgb.nodeproppred/#tgb.nodeproppred.dataset_pyg.PyGNodePropPredDataset.get_label_time","title":"<code>get_label_time()</code>","text":"<p>return the timestamps of the current node labels</p> <p>Returns:</p> Name Type Description <code>t</code> <code>int</code> <p>time of the current node labels</p> Source code in <code>tgb/nodeproppred/dataset_pyg.py</code> <pre><code>def get_label_time(self) -&gt; int:\n\"\"\"\n    return the timestamps of the current node labels\n    Returns:\n        t: time of the current node labels\n    \"\"\"\nreturn self.dataset.return_label_ts()\n</code></pre>"},{"location":"api/tgb.nodeproppred/#tgb.nodeproppred.dataset_pyg.PyGNodePropPredDataset.get_node_label","title":"<code>get_node_label(cur_t)</code>","text":"<p>return the node labels for the current timestamp</p> Source code in <code>tgb/nodeproppred/dataset_pyg.py</code> <pre><code>def get_node_label(self, cur_t):\n\"\"\"\n    return the node labels for the current timestamp\n    \"\"\"\nlabel_tuple = self.dataset.find_next_labels_batch(cur_t)\nif label_tuple is None:\nreturn None\nlabel_ts, label_srcs, labels = label_tuple[0], label_tuple[1], label_tuple[2]\nlabel_ts = torch.from_numpy(label_ts).long()\nlabel_srcs = torch.from_numpy(label_srcs).long()\nlabels = torch.from_numpy(labels).to(torch.float32)\nreturn label_ts, label_srcs, labels\n</code></pre>"},{"location":"api/tgb.nodeproppred/#tgb.nodeproppred.dataset_pyg.PyGNodePropPredDataset.len","title":"<code>len()</code>","text":"<p>size of the dataset</p> <p>Returns:</p> Name Type Description <code>size</code> <code>int</code> <p>int</p> Source code in <code>tgb/nodeproppred/dataset_pyg.py</code> <pre><code>def len(self) -&gt; int:\n\"\"\"\n    size of the dataset\n    Returns:\n        size: int\n    \"\"\"\nreturn self._src.shape[0]\n</code></pre>"},{"location":"api/tgb.nodeproppred/#tgb.nodeproppred.dataset_pyg.PyGNodePropPredDataset.process_data","title":"<code>process_data()</code>","text":"<p>convert data to pytorch tensors</p> Source code in <code>tgb/nodeproppred/dataset_pyg.py</code> <pre><code>def process_data(self):\n\"\"\"\n    convert data to pytorch tensors\n    \"\"\"\nsrc = torch.from_numpy(self.dataset.full_data[\"sources\"])\ndst = torch.from_numpy(self.dataset.full_data[\"destinations\"])\nt = torch.from_numpy(self.dataset.full_data[\"timestamps\"])\nedge_label = torch.from_numpy(self.dataset.full_data[\"edge_label\"])\nmsg = torch.from_numpy(self.dataset.full_data[\"edge_feat\"])\n# msg = torch.from_numpy(self.dataset.full_data[\"edge_feat\"]).reshape(\n#     [-1, 1]\n# ) \n# * check typing\nif src.dtype != torch.int64:\nsrc = src.long()\nif dst.dtype != torch.int64:\ndst = dst.long()\nif t.dtype != torch.int64:\nt = t.long()\nif msg.dtype != torch.float32:\nmsg = msg.float()\nself._src = src\nself._dst = dst\nself._ts = t\nself._edge_label = edge_label\nself._edge_feat = msg\n</code></pre>"},{"location":"api/tgb.nodeproppred/#tgb.nodeproppred.dataset_pyg.PyGNodePropPredDataset.reset_label_time","title":"<code>reset_label_time()</code>","text":"<p>reset the pointer for the node labels, should be done per epoch</p> Source code in <code>tgb/nodeproppred/dataset_pyg.py</code> <pre><code>def reset_label_time(self) -&gt; None:\n\"\"\"\n    reset the pointer for the node labels, should be done per epoch\n    \"\"\"\nself.dataset.reset_label_time()\n</code></pre>"},{"location":"api/tgb.nodeproppred/#tgb.nodeproppred.evaluate.Evaluator","title":"<code>Evaluator</code>","text":"<p>             Bases: <code>object</code></p> <p>Evaluator for Node Property Prediction</p> Source code in <code>tgb/nodeproppred/evaluate.py</code> <pre><code>class Evaluator(object):\n\"\"\"Evaluator for Node Property Prediction\"\"\"\ndef __init__(self, name: str):\nr\"\"\"\n        Parameters:\n            name: name of the dataset\n        \"\"\"\nself.name = name\nself.valid_metric_list = [\"mse\", \"rmse\", \"ndcg\"]\nif self.name not in DATA_EVAL_METRIC_DICT:\nraise NotImplementedError(\"Dataset not supported\")\ndef _parse_and_check_input(self, input_dict):\n\"\"\"\n        check whether the input has the required format\n        Parametrers:\n            -input_dict: a dictionary containing \"y_true\", \"y_pred\", and \"eval_metric\"\n            note: \"eval_metric\" should be a list including one or more of the followin metrics:\n                    [\"mse\"]\n        \"\"\"\n# valid_metric_list = ['ap', 'au_roc_score', 'au_pr_score', 'acc', 'prec', 'rec', 'f1']\nif \"eval_metric\" not in input_dict:\nraise RuntimeError(\"Missing key of eval_metric\")\nfor eval_metric in input_dict[\"eval_metric\"]:\nif eval_metric in self.valid_metric_list:\nif \"y_true\" not in input_dict:\nraise RuntimeError(\"Missing key of y_true\")\nif \"y_pred\" not in input_dict:\nraise RuntimeError(\"Missing key of y_pred\")\ny_true, y_pred = input_dict[\"y_true\"], input_dict[\"y_pred\"]\n# converting to numpy on cpu\nif torch is not None and isinstance(y_true, torch.Tensor):\ny_true = y_true.detach().cpu().numpy()\nif torch is not None and isinstance(y_pred, torch.Tensor):\ny_pred = y_pred.detach().cpu().numpy()\n# check type and shape\nif not isinstance(y_true, np.ndarray) or not isinstance(\ny_pred, np.ndarray\n):\nraise RuntimeError(\n\"Arguments to Evaluator need to be either numpy ndarray or torch tensor!\"\n)\nif not y_true.shape == y_pred.shape:\nraise RuntimeError(\"Shape of y_true and y_pred must be the same!\")\nelse:\nprint(\n\"ERROR: The evaluation metric should be in:\", self.valid_metric_list\n)\nraise ValueError(\"Undefined eval metric %s \" % (eval_metric))\nself.eval_metric = input_dict[\"eval_metric\"]\nreturn y_true, y_pred\ndef _compute_metrics(self, y_true, y_pred):\n\"\"\"\n        compute the performance metrics for the given true labels and prediction probabilities\n        Parameters:\n            -y_true: actual true labels\n            -y_pred: predicted probabilities\n        \"\"\"\nperf_dict = {}\nfor eval_metric in self.eval_metric:\nif eval_metric == \"mse\":\nperf_dict = {\n\"mse\": mean_squared_error(y_true, y_pred),\n\"rmse\": math.sqrt(mean_squared_error(y_true, y_pred)),\n}\nelif eval_metric == \"ndcg\":\nk = 10\nperf_dict = {\"ndcg\": ndcg_score(y_true, y_pred, k=k)}\nreturn perf_dict\ndef eval(self, input_dict, verbose=False):\n\"\"\"\n        evaluation for edge regression task\n        \"\"\"\ny_true, y_pred = self._parse_and_check_input(input_dict)\nperf_dict = self._compute_metrics(y_true, y_pred)\nif verbose:\nprint(\"INFO: Evaluation Results:\")\nfor eval_metric in input_dict[\"eval_metric\"]:\nprint(f\"\\t&gt;&gt;&gt; {eval_metric}: {perf_dict[eval_metric]:.4f}\")\nreturn perf_dict\n@property\ndef expected_input_format(self):\ndesc = \"==== Expected input format of Evaluator for {}\\n\".format(self.name)\nif \"mse\" in self.valid_metric_list:\ndesc += \"{'y_pred': y_pred}\\n\"\ndesc += \"- y_pred: numpy ndarray or torch tensor of shape (num_edges, ). Torch tensor on GPU is recommended for efficiency.\\n\"\ndesc += \"y_pred is the predicted weight for edges.\\n\"\nelse:\nraise ValueError(\"Undefined eval metric %s\" % (self.eval_metric))\nreturn desc\n@property\ndef expected_output_format(self):\ndesc = \"==== Expected output format of Evaluator for {}\\n\".format(self.name)\nif \"mse\" in self.valid_metric_list:\ndesc += \"{'mse': mse\\n\"\ndesc += \"- mse (float): mse score\\n\"\nelse:\nraise ValueError(\"Undefined eval metric %s\" % (self.eval_metric))\nreturn desc\n</code></pre>"},{"location":"api/tgb.nodeproppred/#tgb.nodeproppred.evaluate.Evaluator.__init__","title":"<code>__init__(name)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>name of the dataset</p> required Source code in <code>tgb/nodeproppred/evaluate.py</code> <pre><code>def __init__(self, name: str):\nr\"\"\"\n    Parameters:\n        name: name of the dataset\n    \"\"\"\nself.name = name\nself.valid_metric_list = [\"mse\", \"rmse\", \"ndcg\"]\nif self.name not in DATA_EVAL_METRIC_DICT:\nraise NotImplementedError(\"Dataset not supported\")\n</code></pre>"},{"location":"api/tgb.nodeproppred/#tgb.nodeproppred.evaluate.Evaluator.eval","title":"<code>eval(input_dict, verbose=False)</code>","text":"<p>evaluation for edge regression task</p> Source code in <code>tgb/nodeproppred/evaluate.py</code> <pre><code>def eval(self, input_dict, verbose=False):\n\"\"\"\n    evaluation for edge regression task\n    \"\"\"\ny_true, y_pred = self._parse_and_check_input(input_dict)\nperf_dict = self._compute_metrics(y_true, y_pred)\nif verbose:\nprint(\"INFO: Evaluation Results:\")\nfor eval_metric in input_dict[\"eval_metric\"]:\nprint(f\"\\t&gt;&gt;&gt; {eval_metric}: {perf_dict[eval_metric]:.4f}\")\nreturn perf_dict\n</code></pre>"},{"location":"api/tgb.nodeproppred/#tgb.nodeproppred.evaluate.main","title":"<code>main()</code>","text":"<p>simple test for evaluator</p> Source code in <code>tgb/nodeproppred/evaluate.py</code> <pre><code>def main():\n\"\"\"\n    simple test for evaluator\n    \"\"\"\nname = \"tgbn-trade\"\nevaluator = Evaluator(name=name)\nprint(evaluator.expected_input_format)\nprint(evaluator.expected_output_format)\ninput_dict = {\"y_true\": y_true, \"y_pred\": y_pred, \"eval_metric\": [\"mse\"]}\nresult_dict = evaluator.eval(input_dict)\nprint(result_dict)\n</code></pre>"},{"location":"api/tgb.utils/","title":"<code>tgb.utils</code>","text":"<p>script for generating statistics from the dataset</p>"},{"location":"api/tgb.utils/#tgb.utils.pre_process.clean_rows","title":"<code>clean_rows(fname, outname)</code>","text":"<p>clean the rows with comma in the name</p> <p>Parameters:</p> Name Type Description Default <code>fname</code> <code>str</code> <p>the path to the raw data</p> required <code>outname</code> <code>str</code> <p>the path to the cleaned data</p> required Source code in <code>tgb/utils/pre_process.py</code> <pre><code>def clean_rows(\nfname: str,\noutname: str,\n):\nr\"\"\"\n    clean the rows with comma in the name\n    args:\n        fname: the path to the raw data\n        outname: the path to the cleaned data\n    \"\"\"\noutf = open(outname, \"w\")\nwith open(fname) as f:\ns = next(f)\noutf.write(s)\nfor idx, line in enumerate(f):\nstrs = [\"China, Taiwan Province of\", \"China, mainland\"]\nfor str in strs:\nline = line.replace(\n\"China, Taiwan Province of\", \"Taiwan Province of China\"\n)\nline = line.replace(\"China, mainland\", \"China mainland\")\nline = line.replace(\"China, Hong Kong SAR\", \"China Hong Kong SAR\")\nline = line.replace(\"China, Macao SAR\", \"China Macao SAR\")\nline = line.replace(\n\"Saint Helena, Ascension and Tristan da Cunha\",\n\"Saint Helena Ascension and Tristan da Cunha\",\n)\ne = line.strip().split(\",\")\nif len(e) &gt; 4:\nprint(e)\nraise ValueError(\"line has more than 4 elements\")\noutf.write(line)\noutf.close()\n</code></pre>"},{"location":"api/tgb.utils/#tgb.utils.pre_process.convert_str2int","title":"<code>convert_str2int(in_str)</code>","text":"<p>convert strings to vectors of integers based on individual character each letter is converted as follows, a=10, b=11 numbers are still int</p> <p>Parameters:</p> Name Type Description Default <code>in_str</code> <code>str</code> <p>an input string to parse</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>np.ndarray</code> <p>a numpy integer array</p> Source code in <code>tgb/utils/pre_process.py</code> <pre><code>def convert_str2int(\nin_str: str,\n) -&gt; np.ndarray:\n\"\"\"\n    convert strings to vectors of integers based on individual character\n    each letter is converted as follows, a=10, b=11\n    numbers are still int\n    Parameters:\n        in_str: an input string to parse\n    Returns:\n        out: a numpy integer array\n    \"\"\"\nout = []\nfor element in in_str:\nif element.isnumeric():\nout.append(element)\nelif element == \"!\":\nout.append(-1)\nelse:\nout.append(ord(element.upper()) - 44 + 9)\nout = np.array(out, dtype=np.float32)\nreturn out\n</code></pre>"},{"location":"api/tgb.utils/#tgb.utils.pre_process.csv_to_pd_data","title":"<code>csv_to_pd_data(fname)</code>","text":"<p>currently used by open sky dataset convert the raw .csv data to pandas dataframe and numpy array input .csv file format should be: timestamp, node u, node v, attributes</p> <p>Parameters:</p> Name Type Description Default <code>fname</code> <code>str</code> <p>the path to the raw data</p> required Source code in <code>tgb/utils/pre_process.py</code> <pre><code>def csv_to_pd_data(\nfname: str,\n) -&gt; pd.DataFrame:\nr\"\"\"\n    currently used by open sky dataset\n    convert the raw .csv data to pandas dataframe and numpy array\n    input .csv file format should be: timestamp, node u, node v, attributes\n    Args:\n        fname: the path to the raw data\n    \"\"\"\nfeat_size = 16\nnum_lines = sum(1 for line in open(fname)) - 1\nprint(\"number of lines counted\", num_lines)\nu_list = np.zeros(num_lines)\ni_list = np.zeros(num_lines)\nts_list = np.zeros(num_lines)\nlabel_list = np.zeros(num_lines)\nfeat_l = np.zeros((num_lines, feat_size))\nidx_list = np.zeros(num_lines)\nw_list = np.zeros(num_lines)\nprint(\"numpy allocated\")\nTIME_FORMAT = \"%Y-%m-%d\"  # 2019-01-01\nnode_ids = {}\nunique_id = 0\nwith open(fname, \"r\") as csv_file:\ncsv_reader = csv.reader(csv_file, delimiter=\",\")\nidx = 0\n#'day','src','dst','callsign','typecode'\nfor row in tqdm(csv_reader):\nif idx == 0:\nidx += 1\ncontinue\nelse:\nts = row[0]\ndate_cur = datetime.strptime(ts, TIME_FORMAT)\nts = float(date_cur.timestamp())\nsrc = row[1]\ndst = row[2]\n# 'callsign' has max size 8, can be 4, 5, 6, or 7\n# 'typecode' has max size 8\n# use ! as padding\n# pad row[3] to size 7\nif len(row[3]) == 0:\nrow[3] = \"!!!!!!!!\"\nwhile len(row[3]) &lt; 8:\nrow[3] += \"!\"\n# pad row[4] to size 4\nif len(row[4]) == 0:\nrow[4] = \"!!!!!!!!\"\nwhile len(row[4]) &lt; 8:\nrow[4] += \"!\"\nif len(row[4]) &gt; 8:\nrow[4] = \"!!!!!!!!\"\nfeat_str = row[3] + row[4]\nif src not in node_ids:\nnode_ids[src] = unique_id\nunique_id += 1\nif dst not in node_ids:\nnode_ids[dst] = unique_id\nunique_id += 1\nu = node_ids[src]\ni = node_ids[dst]\nu_list[idx - 1] = u\ni_list[idx - 1] = i\nts_list[idx - 1] = ts\nidx_list[idx - 1] = idx\nw_list[idx - 1] = float(1)\nfeat_l[idx - 1] = convert_str2int(feat_str)\nidx += 1\nreturn (\npd.DataFrame(\n{\n\"u\": u_list,\n\"i\": i_list,\n\"ts\": ts_list,\n\"label\": label_list,\n\"idx\": idx_list,\n\"w\": w_list,\n}\n),\nfeat_l,\nnode_ids,\n)\n</code></pre>"},{"location":"api/tgb.utils/#tgb.utils.pre_process.csv_to_pd_data_rc","title":"<code>csv_to_pd_data_rc(fname)</code>","text":"<p>currently used by redditcomments dataset convert the raw .csv data to pandas dataframe and numpy array input .csv file format should be: timestamp, node u, node v, attributes</p> <p>Parameters:</p> Name Type Description Default <code>fname</code> <code>str</code> <p>the path to the raw data</p> required Source code in <code>tgb/utils/pre_process.py</code> <pre><code>def csv_to_pd_data_rc(\nfname: str,\n) -&gt; pd.DataFrame:\nr\"\"\"\n    currently used by redditcomments dataset\n    convert the raw .csv data to pandas dataframe and numpy array\n    input .csv file format should be: timestamp, node u, node v, attributes\n    Args:\n        fname: the path to the raw data\n    \"\"\"\nfeat_size = 2  # 1 for subreddit, 1 for num words\nnum_lines = sum(1 for line in open(fname)) - 1\n#print(\"number of lines counted\", num_lines)\nprint(\"there are \", num_lines, \" lines in the raw data\")\nu_list = np.zeros(num_lines)\ni_list = np.zeros(num_lines)\nts_list = np.zeros(num_lines)\nlabel_list = np.zeros(num_lines)\nfeat_l = np.zeros((num_lines, feat_size))\nidx_list = np.zeros(num_lines)\nw_list = np.zeros(num_lines)\nnode_ids = {}\nunique_id = 0\nmax_words = 5000  # counted form statistics\nwith open(fname, \"r\") as csv_file:\ncsv_reader = csv.reader(csv_file, delimiter=\",\")\nidx = 0\n# ['ts', 'src', 'dst', 'subreddit', 'num_words', 'score']\nfor row in tqdm(csv_reader):\nif idx == 0:\nidx += 1\ncontinue\nelse:\nts = int(row[0])\nsrc = row[1]\ndst = row[2]\nnum_words = int(row[3]) / max_words  # int number, normalize to [0,1]\nscore = int(row[4])  # int number\n# reindexing node and subreddits\nif src not in node_ids:\nnode_ids[src] = unique_id\nunique_id += 1\nif dst not in node_ids:\nnode_ids[dst] = unique_id\nunique_id += 1\nw = float(score)\nu = node_ids[src]\ni = node_ids[dst]\nu_list[idx - 1] = u\ni_list[idx - 1] = i\nts_list[idx - 1] = ts\nidx_list[idx - 1] = idx\nw_list[idx - 1] = w\nfeat_l[idx - 1] = np.array([num_words])\nidx += 1\nprint(\"there are \", len(node_ids), \" unique nodes\")\nreturn (\npd.DataFrame(\n{\n\"u\": u_list,\n\"i\": i_list,\n\"ts\": ts_list,\n\"label\": label_list,\n\"idx\": idx_list,\n\"w\": w_list,\n}\n),\nfeat_l,\nnode_ids,\n)\n</code></pre>"},{"location":"api/tgb.utils/#tgb.utils.pre_process.csv_to_pd_data_sc","title":"<code>csv_to_pd_data_sc(fname)</code>","text":"<p>currently used by stablecoin dataset convert the raw .csv data to pandas dataframe and numpy array input .csv file format should be: timestamp, node u, node v, attributes</p> <p>Parameters:</p> Name Type Description Default <code>fname</code> <code>str</code> <p>the path to the raw data</p> required <p>Returns:</p> Name Type Description <code>df</code> <code>pd.DataFrame</code> <p>a pandas dataframe containing the edgelist data</p> <code>feat_l</code> <code>pd.DataFrame</code> <p>a numpy array containing the node features</p> <code>node_ids</code> <code>pd.DataFrame</code> <p>a dictionary mapping node id to integer</p> Source code in <code>tgb/utils/pre_process.py</code> <pre><code>def csv_to_pd_data_sc(\nfname: str,\n) -&gt; pd.DataFrame:\nr\"\"\"\n    currently used by stablecoin dataset\n    convert the raw .csv data to pandas dataframe and numpy array\n    input .csv file format should be: timestamp, node u, node v, attributes\n    Parameters:\n        fname: the path to the raw data\n    Returns:\n        df: a pandas dataframe containing the edgelist data\n        feat_l: a numpy array containing the node features\n        node_ids: a dictionary mapping node id to integer\n    \"\"\"\nfeat_size = 1\nnum_lines = sum(1 for line in open(fname)) - 1\nprint(\"number of lines counted\", num_lines)\nu_list = np.zeros(num_lines)\ni_list = np.zeros(num_lines)\nts_list = np.zeros(num_lines)\nlabel_list = np.zeros(num_lines)\nfeat_l = np.zeros((num_lines, feat_size))\nidx_list = np.zeros(num_lines)\nw_list = np.zeros(num_lines)\nprint(\"numpy allocated\")\nnode_ids = {}\nunique_id = 0\nwith open(fname, \"r\") as csv_file:\ncsv_reader = csv.reader(csv_file, delimiter=\",\")\nidx = 0\n# time,src,dst,weight\n# 1648811421,0x27cbb0e6885ccb1db2dab7c2314131c94795fbef,0x8426a27add8dca73548f012d92c7f8f4bbd42a3e,800.0\nfor row in tqdm(csv_reader):\nif idx == 0:\nidx += 1\ncontinue\nelse:\nts = int(row[0])\nsrc = row[1]\ndst = row[2]\nif src not in node_ids:\nnode_ids[src] = unique_id\nunique_id += 1\nif dst not in node_ids:\nnode_ids[dst] = unique_id\nunique_id += 1\nw = float(row[3])\nif w == 0:\nw = 1\nu = node_ids[src]\ni = node_ids[dst]\nu_list[idx - 1] = u\ni_list[idx - 1] = i\nts_list[idx - 1] = ts\nidx_list[idx - 1] = idx\nw_list[idx - 1] = w\nfeat_l[idx - 1] = np.zeros(feat_size)\nidx += 1\n#! normalize by log 2 for stablecoin\nw_list = np.log2(w_list)\nreturn (\npd.DataFrame(\n{\n\"u\": u_list,\n\"i\": i_list,\n\"ts\": ts_list,\n\"label\": label_list,\n\"idx\": idx_list,\n\"w\": w_list,\n}\n),\nfeat_l,\nnode_ids,\n)\n</code></pre>"},{"location":"api/tgb.utils/#tgb.utils.pre_process.load_edgelist_datetime","title":"<code>load_edgelist_datetime(fname, label_size=514)</code>","text":"<p>load the edgelist into a pandas dataframe use numpy array instead of list for faster processing assume all edges are already sorted by time convert all time unit to unix time</p> <p>time, user_id, genre, weight</p> Source code in <code>tgb/utils/pre_process.py</code> <pre><code>def load_edgelist_datetime(fname, label_size=514):\n\"\"\"\n    load the edgelist into a pandas dataframe\n    use numpy array instead of list for faster processing\n    assume all edges are already sorted by time\n    convert all time unit to unix time\n    time, user_id, genre, weight\n    \"\"\"\nfeat_size = 1\nnum_lines = sum(1 for line in open(fname)) - 1\nprint(\"number of lines counted\", num_lines)\nu_list = np.zeros(num_lines)\ni_list = np.zeros(num_lines)\nts_list = np.zeros(num_lines)\nfeat_l = np.zeros((num_lines, feat_size))\nidx_list = np.zeros(num_lines)\nw_list = np.zeros(num_lines)\n#print(\"numpy allocated\")\nnode_ids = {}  # dictionary for node ids\nlabel_ids = {}  # dictionary for label ids\nnode_uid = label_size  # node ids start after the genre nodes\nlabel_uid = 0\nwith open(fname, \"r\") as csv_file:\ncsv_reader = csv.reader(csv_file, delimiter=\",\")\nidx = 0\nfor row in tqdm(csv_reader):\nif idx == 0:\nidx += 1\nelse:\nts = int(row[0])\nuser_id = row[1]\ngenre = row[2]\nw = float(row[3])\nif user_id not in node_ids:\nnode_ids[user_id] = node_uid\nnode_uid += 1\nif genre not in label_ids:\nlabel_ids[genre] = label_uid\nif label_uid &gt;= label_size:\nprint(\"id overlap, terminate\")\nlabel_uid += 1\nu = node_ids[user_id]\ni = label_ids[genre]\nu_list[idx - 1] = u\ni_list[idx - 1] = i\nts_list[idx - 1] = ts\nidx_list[idx - 1] = idx\nw_list[idx - 1] = w\nfeat_l[idx - 1] = np.asarray([w])\nidx += 1\nreturn (\npd.DataFrame(\n{\"u\": u_list, \"i\": i_list, \"ts\": ts_list, \"idx\": idx_list, \"w\": w_list}\n),\nfeat_l,\nnode_ids,\nlabel_ids,\n)\n</code></pre>"},{"location":"api/tgb.utils/#tgb.utils.pre_process.load_edgelist_sr","title":"<code>load_edgelist_sr(fname, label_size=2221)</code>","text":"<p>load the edgelist into pandas dataframe also outputs index for the user nodes and genre nodes</p> <p>Parameters:</p> Name Type Description Default <code>fname</code> <code>str</code> <p>str, name of the input file</p> required <code>label_size</code> <code>int</code> <p>int, number of genres</p> <code>2221</code> <p>Returns:</p> Name Type Description <code>df</code> <code>pd.DataFrame</code> <p>a pandas dataframe containing the edgelist data</p> Source code in <code>tgb/utils/pre_process.py</code> <pre><code>def load_edgelist_sr(\nfname: str,\nlabel_size: int = 2221,\n) -&gt; pd.DataFrame:\n\"\"\"\n    load the edgelist into pandas dataframe\n    also outputs index for the user nodes and genre nodes\n    Parameters:\n        fname: str, name of the input file\n        label_size: int, number of genres\n    Returns:\n        df: a pandas dataframe containing the edgelist data\n    \"\"\"\nfeat_size = 1 #2\nnum_lines = sum(1 for line in open(fname)) - 1\n#print(\"number of lines counted\", num_lines)\nprint(\"there are \", num_lines, \" lines in the raw data\")\nu_list = np.zeros(num_lines)\ni_list = np.zeros(num_lines)\nts_list = np.zeros(num_lines)\nlabel_list = np.zeros(num_lines)\nfeat_l = np.zeros((num_lines, feat_size))\nidx_list = np.zeros(num_lines)\nw_list = np.zeros(num_lines)\nnode_ids = {}\nrd_dict = {}\nnode_uid = label_size  # node ids start after all the genres\nsr_uid = 0\nwith open(fname, \"r\") as csv_file:\ncsv_reader = csv.reader(csv_file, delimiter=\",\")\nidx = 0\n# ['ts', 'src', 'subreddit', 'num_words', 'score']\nfor row in tqdm(csv_reader):\nif idx == 0:\nidx += 1\nelse:\nts = row[0]\nsrc = row[1]\nsubreddit = row[2]\n#num_words = int(row[3])\nscore = int(row[4])\nif src not in node_ids:\nnode_ids[src] = node_uid\nnode_uid += 1\nif subreddit not in rd_dict:\nrd_dict[subreddit] = sr_uid\nsr_uid += 1\nw = float(score)\nu = node_ids[src]\ni = rd_dict[subreddit]\nu_list[idx - 1] = u\ni_list[idx - 1] = i\nts_list[idx - 1] = ts\nidx_list[idx - 1] = idx\nw_list[idx - 1] = w\nfeat_l[idx - 1] = np.array([w])\nidx += 1\nreturn (\npd.DataFrame(\n{\n\"u\": u_list,\n\"i\": i_list,\n\"ts\": ts_list,\n\"label\": label_list,\n\"idx\": idx_list,\n\"w\": w_list,\n}\n),\nfeat_l,\nnode_ids,\nrd_dict,\n)\n</code></pre>"},{"location":"api/tgb.utils/#tgb.utils.pre_process.load_edgelist_token","title":"<code>load_edgelist_token(fname, label_size=1001)</code>","text":"<p>load the edgelist into pandas dataframe also outputs index for the user nodes and genre nodes</p> <p>Parameters:</p> Name Type Description Default <code>fname</code> <code>str</code> <p>str, name of the input file</p> required <code>label_size</code> <code>int</code> <p>int, number of genres</p> <code>1001</code> <p>Returns:</p> Name Type Description <code>df</code> <code>pd.DataFrame</code> <p>a pandas dataframe containing the edgelist data</p> Source code in <code>tgb/utils/pre_process.py</code> <pre><code>def load_edgelist_token(\nfname: str,\nlabel_size: int = 1001,\n) -&gt; pd.DataFrame:\n\"\"\"\n    load the edgelist into pandas dataframe\n    also outputs index for the user nodes and genre nodes\n    Parameters:\n        fname: str, name of the input file\n        label_size: int, number of genres\n    Returns:\n        df: a pandas dataframe containing the edgelist data\n    \"\"\"\nfeat_size = 2\nnum_lines = sum(1 for line in open(fname)) - 1\n#print(\"number of lines counted\", num_lines)\nprint(\"there are \", num_lines, \" lines in the raw data\")\nu_list = np.zeros(num_lines)\ni_list = np.zeros(num_lines)\nts_list = np.zeros(num_lines)\nlabel_list = np.zeros(num_lines)\nfeat_l = np.zeros((num_lines, feat_size))\nidx_list = np.zeros(num_lines)\nw_list = np.zeros(num_lines)\nnode_ids = {}\nrd_dict = {}\nnode_uid = label_size  # node ids start after all the genres\nsr_uid = 0\nwith open(fname, \"r\") as csv_file:\ncsv_reader = csv.reader(csv_file, delimiter=\",\")\nidx = 0\n# [timestamp,user_address,token_address,value,IsSender]\nfor row in tqdm(csv_reader):\nif idx == 0:\nidx += 1\nelse:\nts = row[0]\nsrc = row[1]\ntoken = row[2]\nw = float(row[3])\nattr = float(row[4])\nif src not in node_ids:\nnode_ids[src] = node_uid\nnode_uid += 1\nif token not in rd_dict:\nrd_dict[token] = sr_uid\nsr_uid += 1\nu = node_ids[src]\ni = rd_dict[token]\nu_list[idx - 1] = u\ni_list[idx - 1] = i\nts_list[idx - 1] = ts\nidx_list[idx - 1] = idx\nw_list[idx - 1] = w\nfeat_l[idx - 1] = np.array([w,attr])\nidx += 1\nreturn (\npd.DataFrame(\n{\n\"u\": u_list,\n\"i\": i_list,\n\"ts\": ts_list,\n\"label\": label_list,\n\"idx\": idx_list,\n\"w\": w_list,\n}\n),\nfeat_l,\nnode_ids,\nrd_dict,\n)\n</code></pre>"},{"location":"api/tgb.utils/#tgb.utils.pre_process.load_edgelist_trade","title":"<code>load_edgelist_trade(fname, label_size=255)</code>","text":"<p>load the edgelist into pandas dataframe</p> Source code in <code>tgb/utils/pre_process.py</code> <pre><code>def load_edgelist_trade(fname: str, label_size=255):\n\"\"\"\n    load the edgelist into pandas dataframe\n    \"\"\"\nfeat_size = 1\nnum_lines = sum(1 for line in open(fname)) - 1\nprint(\"number of lines counted\", num_lines)\nu_list = np.zeros(num_lines)\ni_list = np.zeros(num_lines)\nts_list = np.zeros(num_lines)\nfeat_l = np.zeros((num_lines, feat_size))\nidx_list = np.zeros(num_lines)\nw_list = np.zeros(num_lines)\n#print(\"numpy allocated\")\nnode_ids = {}  # dictionary for node ids\nnode_uid = 0\nwith open(fname, \"r\") as csv_file:\ncsv_reader = csv.reader(csv_file, delimiter=\",\")\nidx = 0\nfor row in tqdm(csv_reader):\nif idx == 0:\nidx += 1\nelse:\nts = int(row[0])\nu = row[1]\nv = row[2]\nw = float(row[3])\nif u not in node_ids:\nnode_ids[u] = node_uid\nnode_uid += 1\nif v not in node_ids:\nnode_ids[v] = node_uid\nnode_uid += 1\nu = node_ids[u]\ni = node_ids[v]\nu_list[idx - 1] = u\ni_list[idx - 1] = i\nts_list[idx - 1] = ts\nidx_list[idx - 1] = idx\nw_list[idx - 1] = w\nfeat_l[idx - 1] = np.array([w])\nidx += 1\nreturn (\npd.DataFrame(\n{\"u\": u_list, \"i\": i_list, \"ts\": ts_list, \"idx\": idx_list, \"w\": w_list}\n),\nfeat_l,\nnode_ids,\n)\n</code></pre>"},{"location":"api/tgb.utils/#tgb.utils.pre_process.load_edgelist_wiki","title":"<code>load_edgelist_wiki(fname)</code>","text":"<p>loading wikipedia dataset into pandas dataframe similar processing to https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/datasets/jodie.html</p> <p>Parameters:</p> Name Type Description Default <code>fname</code> <code>str</code> <p>str, name of the input file</p> required <p>Returns:</p> Name Type Description <code>df</code> <code>pd.DataFrame</code> <p>a pandas dataframe containing the edgelist data</p> Source code in <code>tgb/utils/pre_process.py</code> <pre><code>def load_edgelist_wiki(fname: str) -&gt; pd.DataFrame:\n\"\"\"\n    loading wikipedia dataset into pandas dataframe\n    similar processing to\n    https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/datasets/jodie.html\n    Parameters:\n        fname: str, name of the input file\n    Returns:\n        df: a pandas dataframe containing the edgelist data\n    \"\"\"\ndf = pd.read_csv(fname, skiprows=1, header=None)\nsrc = df.iloc[:, 0].values\ndst = df.iloc[:, 1].values\ndst += int(src.max()) + 1\nt = df.iloc[:, 2].values\nmsg = df.iloc[:, 4:].values\nidx = np.arange(t.shape[0])\nw = np.ones(t.shape[0])\nreturn pd.DataFrame({\"u\": src, \"i\": dst, \"ts\": t, \"idx\": idx, \"w\": w}), msg, None\n</code></pre>"},{"location":"api/tgb.utils/#tgb.utils.pre_process.load_genre_list","title":"<code>load_genre_list(fname)</code>","text":"<p>load the list of genres</p> Source code in <code>tgb/utils/pre_process.py</code> <pre><code>def load_genre_list(fname):\n\"\"\"\n    load the list of genres\n    \"\"\"\nif not osp.exists(fname):\nraise FileNotFoundError(f\"File not found at {fname}\")\nedgelist = open(fname, \"r\")\nlines = list(edgelist.readlines())\nedgelist.close()\ngenre_index = {}\nctr = 0\nfor i in range(1, len(lines)):\nvals = lines[i].split(\",\")\ngenre = vals[0]\nif genre not in genre_index:\ngenre_index[genre] = ctr\nctr += 1\nelse:\nraise ValueError(\"duplicate in genre_index\")\nreturn genre_index\n</code></pre>"},{"location":"api/tgb.utils/#tgb.utils.pre_process.load_label_dict","title":"<code>load_label_dict(fname, node_ids, rd_dict)</code>","text":"<p>load node labels into a nested dictionary instead of pandas dataobject {ts: {node_id: label_vec}}</p> <p>Parameters:</p> Name Type Description Default <code>fname</code> <code>str</code> <p>str, name of the input file</p> required <code>node_ids</code> <code>dict</code> <p>dictionary of user names mapped to integer node ids</p> required <code>rd_dict</code> <code>dict</code> <p>dictionary of subreddit names mapped to integer node ids</p> required Source code in <code>tgb/utils/pre_process.py</code> <pre><code>def load_label_dict(fname: str, node_ids: dict, rd_dict: dict) -&gt; dict:\n\"\"\"\n    load node labels into a nested dictionary instead of pandas dataobject\n    {ts: {node_id: label_vec}}\n    Parameters:\n        fname: str, name of the input file\n        node_ids: dictionary of user names mapped to integer node ids\n        rd_dict: dictionary of subreddit names mapped to integer node ids\n    \"\"\"\nif not osp.exists(fname):\nraise FileNotFoundError(f\"File not found at {fname}\")\n# day, user_idx, label_vec\nlabel_size = len(rd_dict)\nnode_label_dict = {}  # {ts: {node_id: label_vec}}\nwith open(fname, \"r\") as csv_file:\ncsv_reader = csv.reader(csv_file, delimiter=\",\")\nidx = 0\n# ['ts', 'src', 'dst', 'w']\nfor row in tqdm(csv_reader):\nif idx == 0:\nidx += 1\nelse:\nu = node_ids[row[1]]\nts = int(row[0])\nv = int(rd_dict[row[2]])\nweight = float(row[3])\nif (ts not in node_label_dict):\nnode_label_dict[ts] = {u:np.zeros(label_size)}\nif (u not in node_label_dict[ts]):\nnode_label_dict[ts][u] = np.zeros(label_size)\nnode_label_dict[ts][u][v] = weight\nidx += 1\nreturn node_label_dict\n</code></pre>"},{"location":"api/tgb.utils/#tgb.utils.pre_process.load_labels_sr","title":"<code>load_labels_sr(fname, node_ids, rd_dict)</code>","text":"<p>load the node labels for subreddit dataset</p> Source code in <code>tgb/utils/pre_process.py</code> <pre><code>def load_labels_sr(\nfname,\nnode_ids,\nrd_dict,\n):\n\"\"\"\n    load the node labels for subreddit dataset\n    \"\"\"\nif not osp.exists(fname):\nraise FileNotFoundError(f\"File not found at {fname}\")\n# day, user_idx, label_vec\nlabel_size = len(rd_dict)\nlabel_vec = np.zeros(label_size)\nts_prev = 0\nprev_user = 0\nts_list = []\nnode_id_list = []\ny_list = []\nwith open(fname, \"r\") as csv_file:\ncsv_reader = csv.reader(csv_file, delimiter=\",\")\nidx = 0\n# ['ts', 'src', 'subreddit', 'num_words', 'score']\nfor row in tqdm(csv_reader):\nif idx == 0:\nidx += 1\nelse:\nuser_id = node_ids[int(row[1])]\nts = int(row[0])\nsr_id = int(rd_dict[row[2]])\nweight = float(row[3])\nif idx == 1:\nts_prev = ts\nprev_user = user_id\n# the next day\nif ts != ts_prev:\nts_list.append(ts_prev)\nnode_id_list.append(prev_user)\ny_list.append(label_vec)\nlabel_vec = np.zeros(label_size)\nts_prev = ts\nprev_user = user_id\nelse:\nlabel_vec[sr_id] = weight\nif user_id != prev_user:\nts_list.append(ts_prev)\nnode_id_list.append(prev_user)\ny_list.append(label_vec)\nprev_user = user_id\nlabel_vec = np.zeros(label_size)\nidx += 1\nreturn pd.DataFrame({\"ts\": ts_list, \"node_id\": node_id_list, \"y\": y_list})\n</code></pre>"},{"location":"api/tgb.utils/#tgb.utils.pre_process.load_trade_label_dict","title":"<code>load_trade_label_dict(fname, node_ids)</code>","text":"<p>load node labels into a nested dictionary instead of pandas dataobject {ts: {node_id: label_vec}}</p> <p>Parameters:</p> Name Type Description Default <code>fname</code> <code>str</code> <p>str, name of the input file</p> required <code>node_ids</code> <code>dict</code> <p>dictionary of user names mapped to integer node ids</p> required <p>Returns:</p> Name Type Description <code>node_label_dict</code> <code>dict</code> <p>a nested dictionary of node labels</p> Source code in <code>tgb/utils/pre_process.py</code> <pre><code>def load_trade_label_dict(\nfname: str,\nnode_ids: dict,\n) -&gt; dict:\n\"\"\"\n    load node labels into a nested dictionary instead of pandas dataobject\n    {ts: {node_id: label_vec}}\n    Parameters:\n        fname: str, name of the input file\n        node_ids: dictionary of user names mapped to integer node ids\n    Returns:\n        node_label_dict: a nested dictionary of node labels\n    \"\"\"\nif not osp.exists(fname):\nraise FileNotFoundError(f\"File not found at {fname}\")\nlabel_size = len(node_ids)\n#label_vec = np.zeros(label_size)\nnode_label_dict = {}  # {ts: {node_id: label_vec}}\nwith open(fname, \"r\") as csv_file:\ncsv_reader = csv.reader(csv_file, delimiter=\",\")\nidx = 0\nfor row in tqdm(csv_reader):\nif idx == 0:\nidx += 1\nelse:\nts = int(row[0])\nu = node_ids[row[1]]\nv = node_ids[row[2]]\nweight = float(row[3])\nif (ts not in node_label_dict):\nnode_label_dict[ts] = {u:np.zeros(label_size)}\nif (u not in node_label_dict[ts]):\nnode_label_dict[ts][u] = np.zeros(label_size)\nnode_label_dict[ts][u][v] = weight\nidx += 1\nreturn node_label_dict\n</code></pre>"},{"location":"api/tgb.utils/#tgb.utils.pre_process.process_node_feat","title":"<code>process_node_feat(fname, node_ids)</code>","text":"<ol> <li>need to have the same node id as csv_to_pd_data</li> <li>process the various node features into a vector</li> <li>return a numpy array of node features with index corresponding to node id</li> </ol> <p>airport_code,type,continent,iso_region,longitude,latitude type: onehot encoding continent: onehot encoding iso_region: alphabet encoding same as edge feat longitude: float divide by 180 latitude: float divide by 90</p> Source code in <code>tgb/utils/pre_process.py</code> <pre><code>def process_node_feat(\nfname: str,\nnode_ids,\n):\n\"\"\"\n    1. need to have the same node id as csv_to_pd_data\n    2. process the various node features into a vector\n    3. return a numpy array of node features with index corresponding to node id\n    airport_code,type,continent,iso_region,longitude,latitude\n    type: onehot encoding\n    continent: onehot encoding\n    iso_region: alphabet encoding same as edge feat\n    longitude: float divide by 180\n    latitude: float divide by 90\n    \"\"\"\nfeat_size = 20\nnode_feat = np.zeros((len(node_ids), feat_size))\ntype_dict = {}\ntype_idx = 0\ncontinent_dict = {}\ncont_idx = 0\nwith open(fname, \"r\") as csv_file:\ncsv_reader = csv.reader(csv_file, delimiter=\",\")\nidx = 0\n# airport_code,type,continent,iso_region,longitude,latitude\nfor row in tqdm(csv_reader):\nif idx == 0:\nidx += 1\ncontinue\nelse:\ncode = row[0]\nif code not in node_ids:\ncontinue\nelse:\nnode_id = node_ids[code]\nairport_type = row[1]\nif airport_type not in type_dict:\ntype_dict[airport_type] = type_idx\ntype_idx += 1\ncontinent = row[2]\nif continent not in continent_dict:\ncontinent_dict[continent] = cont_idx\ncont_idx += 1\nwith open(fname, \"r\") as csv_file:\ncsv_reader = csv.reader(csv_file, delimiter=\",\")\nidx = 0\n# airport_code,type,continent,iso_region,longitude,latitude\nfor row in tqdm(csv_reader):\nif idx == 0:\nidx += 1\ncontinue\nelse:\ncode = row[0]\nif code not in node_ids:\ncontinue\nelse:\nnode_id = node_ids[code]\nairport_type = type_dict[row[1]]\ntype_vec = np.zeros(type_idx)\ntype_vec[airport_type] = 1\ncontinent = continent_dict[row[2]]\ncont_vec = np.zeros(cont_idx)\ncont_vec[continent] = 1\nwhile len(row[3]) &lt; 7:\nrow[3] += \"!\"\niso_region = convert_str2int(row[3])  # numpy float array\nlng = float(row[4])\nlat = float(row[5])\ncoor_vec = np.array([lng, lat])\nfinal = np.concatenate(\n(type_vec, cont_vec, iso_region, coor_vec), axis=0\n)\nnode_feat[node_id] = final\nreturn node_feat\n</code></pre>"},{"location":"api/tgb.utils/#tgb.utils.pre_process.reindex","title":"<code>reindex(df, bipartite=False)</code>","text":"<p>reindex the nodes especially if the node ids are not integers</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pd.DataFrame</code> <p>the pandas dataframe containing the graph</p> required <code>bipartite</code> <code>Optional[bool]</code> <p>whether the graph is bipartite</p> <code>False</code> Source code in <code>tgb/utils/pre_process.py</code> <pre><code>def reindex(\ndf: pd.DataFrame,\nbipartite: Optional[bool] = False,\n):\nr\"\"\"\n    reindex the nodes especially if the node ids are not integers\n    Args:\n        df: the pandas dataframe containing the graph\n        bipartite: whether the graph is bipartite\n    \"\"\"\nnew_df = df.copy()\nif bipartite:\nassert df.u.max() - df.u.min() + 1 == len(df.u.unique())\nassert df.i.max() - df.i.min() + 1 == len(df.i.unique())\nupper_u = df.u.max() + 1\nnew_i = df.i + upper_u\nnew_df.i = new_i\nnew_df.u += 1\nnew_df.i += 1\nnew_df.idx += 1\nelse:\nnew_df.u += 1\nnew_df.i += 1\nnew_df.idx += 1\nreturn new_df\n</code></pre>"},{"location":"api/tgb.utils/#tgb.utils.utils.load_pkl","title":"<code>load_pkl(fname)</code>","text":"<p>load a python object from a pickle file</p> Source code in <code>tgb/utils/utils.py</code> <pre><code>def load_pkl(fname: str) -&gt; Any:\nr\"\"\"\n    load a python object from a pickle file\n    \"\"\"\nwith open(fname, \"rb\") as handle:\nreturn pickle.load(handle)\n</code></pre>"},{"location":"api/tgb.utils/#tgb.utils.utils.save_pkl","title":"<code>save_pkl(obj, fname)</code>","text":"<p>save a python object as a pickle file</p> Source code in <code>tgb/utils/utils.py</code> <pre><code>def save_pkl(obj: Any, fname: str) -&gt; None:\nr\"\"\"\n    save a python object as a pickle file\n    \"\"\"\nwith open(fname, \"wb\") as handle:\npickle.dump(obj, handle, protocol=pickle.HIGHEST_PROTOCOL)\n</code></pre>"},{"location":"api/tgb.utils/#tgb.utils.utils.save_results","title":"<code>save_results(new_results, filename)</code>","text":"<p>save (new) results into a json file :param: new_results (dictionary): a dictionary of new results to be saved :filename: the name of the file to save the (new) results</p> Source code in <code>tgb/utils/utils.py</code> <pre><code>def save_results(new_results: dict, filename: str):\nr\"\"\"\n    save (new) results into a json file\n    :param: new_results (dictionary): a dictionary of new results to be saved\n    :filename: the name of the file to save the (new) results\n    \"\"\"\nif os.path.isfile(filename):\n# append to the file\nwith open(filename, 'r+') as json_file:\nfile_data = json.load(json_file)\n# convert file_data to list if not\nif type(file_data) is dict:\nfile_data = [file_data]\nfile_data.append(new_results)\njson_file.seek(0)\njson.dump(file_data, json_file, indent=4)\nelse:\n# dump the results\nwith open(filename, 'w') as json_file:\njson.dump(new_results, json_file, indent=4)\n</code></pre>"},{"location":"api/tgb.utils/#tgb.utils.utils.set_random_seed","title":"<code>set_random_seed(seed)</code>","text":"<p>setting random seed for reproducibility</p> Source code in <code>tgb/utils/utils.py</code> <pre><code>def set_random_seed(seed: int):\nr\"\"\"\n    setting random seed for reproducibility\n    \"\"\"\nnp.random.seed(seed)\nrandom.seed(seed)\nos.environ[\"PYTHONHASHSEED\"] = str(seed)\n</code></pre>"},{"location":"api/tgb.utils/#tgb.utils.info.BColors","title":"<code>BColors</code>","text":"<p>A class to change the colors of the strings.</p> Source code in <code>tgb/utils/info.py</code> <pre><code>class BColors:\n\"\"\"\n    A class to change the colors of the strings.\n    \"\"\"\nHEADER = \"\\033[95m\"\nOKBLUE = \"\\033[94m\"\nOKCYAN = \"\\033[96m\"\nOKGREEN = \"\\033[92m\"\nWARNING = \"\\033[93m\"\nFAIL = \"\\033[91m\"\nENDC = \"\\033[0m\"\nBOLD = \"\\033[1m\"\nUNDERLINE = \"\\033[4m\"\n</code></pre>"},{"location":"api/tgb.utils/#tgb.utils.stats.plot_curve","title":"<code>plot_curve(y, outname)</code>","text":"<p>plot the training curve given y</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>np.ndarray</code> <p>np.ndarray, the training curve</p> required <code>outname</code> <code>str</code> <p>str, the output name</p> required Source code in <code>tgb/utils/stats.py</code> <pre><code>def plot_curve(y: np.ndarray, outname: str) -&gt; None:\n\"\"\"\n    plot the training curve given y\n    Parameters:\n        y: np.ndarray, the training curve\n        outname: str, the output name\n    \"\"\"\nplt.plot(y, color=\"#fc4e2a\")\nplt.savefig(outname + \".pdf\")\nplt.close()\n</code></pre>"},{"location":"tutorials/Edge_data_numpy/","title":"Access edge data as numpy arrays","text":"In\u00a0[1]: Copied! <pre>from tgb.linkproppred.dataset import LinkPropPredDataset\n</pre> from tgb.linkproppred.dataset import LinkPropPredDataset <p>specifying the name of the dataset</p> In\u00a0[2]: Copied! <pre>name = \"tgbl-wiki\"\n</pre> name = \"tgbl-wiki\"  In\u00a0[3]: Copied! <pre>dataset = LinkPropPredDataset(name=name, root=\"datasets\", preprocess=True)\ntype(dataset)\n</pre> dataset = LinkPropPredDataset(name=name, root=\"datasets\", preprocess=True) type(dataset) <pre>Will you download the dataset(s) now? (y/N)\ny\nDownload started, this might take a while . . . \nDataset title: tgbl-wiki\nDownload completed \nDataset directory is  /mnt/f/code/TGB/tgb/datasets/tgbl_wiki\nfile not processed, generating processed file\n</pre> Out[3]: <pre>tgb.linkproppred.dataset.LinkPropPredDataset</pre> In\u00a0[4]: Copied! <pre>data = dataset.full_data  #a dictioinary stores all the edge data\ntype(data)\n</pre> data = dataset.full_data  #a dictioinary stores all the edge data type(data)  Out[4]: <pre>dict</pre> In\u00a0[5]: Copied! <pre>type(data['sources'])\ntype(data['destinations'])\ntype(data['timestamps'])\ntype(data['edge_feat'])\ntype(data['w'])\ntype(data['edge_label']) #just all one array as all edges in the dataset are positive edges\ntype(data['edge_idxs']) #just index of the edges increment by 1 for each edge\n</pre> type(data['sources']) type(data['destinations']) type(data['timestamps']) type(data['edge_feat']) type(data['w']) type(data['edge_label']) #just all one array as all edges in the dataset are positive edges type(data['edge_idxs']) #just index of the edges increment by 1 for each edge Out[5]: <pre>numpy.ndarray</pre> In\u00a0[6]: Copied! <pre>train_mask = dataset.train_mask\nval_mask = dataset.val_mask\ntest_mask = dataset.test_mask\n\ntype(train_mask)\ntype(val_mask)\ntype(test_mask)\n</pre> train_mask = dataset.train_mask val_mask = dataset.val_mask test_mask = dataset.test_mask  type(train_mask) type(val_mask) type(test_mask) Out[6]: <pre>numpy.ndarray</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"tutorials/Edge_data_numpy/#access-edge-data-as-numpy-arrays","title":"Access edge data as numpy arrays\u00b6","text":"<p>This tutorial will show you how to access various datasets and their corresponding edgelists in <code>tgb</code></p> <p>You can directly retrieve the edge data as <code>numpy</code> arrays, <code>PyG</code> and <code>Pytorch</code> dependencies are not necessary</p> <p>The logic is implemented in <code>dataset.py</code> under <code>tgb/linkproppred/</code> and <code>tgb/nodeproppred/</code> folders respectively</p>"},{"location":"tutorials/Edge_data_numpy/#process-and-loading-the-dataset","title":"process and loading the dataset\u00b6","text":"<p>if the dataset has been processed, it will be loaded from disc for fast access</p> <p>if the dataset has not been downloaded, it will be processed automatically</p>"},{"location":"tutorials/Edge_data_numpy/#accessing-the-edge-data","title":"Accessing the edge data\u00b6","text":"<p>the edge data can be easily accessed via the property of the method as <code>numpy</code> arrays</p>"},{"location":"tutorials/Edge_data_numpy/#accessing-the-train-test-val-split","title":"Accessing the train, test, val split\u00b6","text":"<p>the masks for training, validation, and test split can be accessed directly from the <code>dataset</code> as well</p>"},{"location":"tutorials/Edge_data_pyg/","title":"Access edge data in Pytorch Geometric","text":"In\u00a0[1]: Copied! <pre>from tgb.linkproppred.dataset_pyg import PyGLinkPropPredDataset\n</pre> from tgb.linkproppred.dataset_pyg import PyGLinkPropPredDataset <p>specifying the name of the dataset</p> In\u00a0[2]: Copied! <pre>name = \"tgbl-wiki\"\n</pre> name = \"tgbl-wiki\" In\u00a0[3]: Copied! <pre>dataset = PyGLinkPropPredDataset(name=name, root=\"datasets\")\ntype(dataset)\n</pre> dataset = PyGLinkPropPredDataset(name=name, root=\"datasets\") type(dataset) <pre>file found, skipping download\nDataset directory is  /mnt/f/code/TGB/tgb/datasets/tgbl_wiki\nloading processed file\n</pre> Out[3]: <pre>tgb.linkproppred.dataset_pyg.PyGLinkPropPredDataset</pre> In\u00a0[4]: Copied! <pre>data = dataset.get_TemporalData()\ntype(data)\n</pre> data = dataset.get_TemporalData() type(data) Out[4]: <pre>torch_geometric.data.temporal.TemporalData</pre> In\u00a0[5]: Copied! <pre>type(data.src)\ntype(data.dst)\ntype(data.t)\ntype(data.msg)\n</pre> type(data.src) type(data.dst) type(data.t) type(data.msg) Out[5]: <pre>torch.Tensor</pre> In\u00a0[6]: Copied! <pre>type(dataset.src)  #same as src from above\ntype(dataset.dst)  #same as dst\ntype(dataset.ts)  #same as t\ntype(dataset.edge_feat) #same as msg\ntype(dataset.edge_label) #same as label used in tgn\n</pre> type(dataset.src)  #same as src from above type(dataset.dst)  #same as dst type(dataset.ts)  #same as t type(dataset.edge_feat) #same as msg type(dataset.edge_label) #same as label used in tgn Out[6]: <pre>torch.Tensor</pre> In\u00a0[7]: Copied! <pre>train_mask = dataset.train_mask\nval_mask = dataset.val_mask\ntest_mask = dataset.test_mask\n\ntype(train_mask)\ntype(val_mask)\ntype(test_mask)\n</pre> train_mask = dataset.train_mask val_mask = dataset.val_mask test_mask = dataset.test_mask  type(train_mask) type(val_mask) type(test_mask) Out[7]: <pre>torch.Tensor</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"tutorials/Edge_data_pyg/#access-edge-data-in-pytorch-geometric","title":"Access edge data in Pytorch Geometric\u00b6","text":"<p>This tutorial will show you how to access various datasets and their corresponding edgelists in <code>tgb</code></p> <p>The logic for PyG data is stored in <code>dataset_pyg.py</code> in <code>tgb/linkproppred</code> and <code>tgb/nodeproppred</code> folders</p> <p>This tutorial requires <code>Pytorch</code> and <code>PyG</code>, refer to <code>README.md</code> for installation instructions</p>"},{"location":"tutorials/Edge_data_pyg/#process-and-load-the-dataset","title":"Process and load the dataset\u00b6","text":"<p>if the dataset has been processed, it will be loaded from disc for fast access</p> <p>if the dataset has not been downloaded, it will be processed automatically</p>"},{"location":"tutorials/Edge_data_pyg/#access-edge-data-from-temporaldata-object","title":"Access edge data from TemporalData object\u00b6","text":"<p>You can retrieve <code>torch_geometric.data.temporal.TemporalData</code> directly from <code>PyGLinkPropPredDataset</code></p>"},{"location":"tutorials/Edge_data_pyg/#directly-access-edge-data-as-pytorch-tensors","title":"Directly access edge data as Pytorch tensors\u00b6","text":"<p>the edge data can be easily accessed via the property of the method, these are converted into pytorch tensors (from <code>PyGLinkPropPredDataset</code>)</p>"},{"location":"tutorials/Edge_data_pyg/#accessing-the-train-test-val-split","title":"Accessing the train, test, val split\u00b6","text":"<p>the masks for training, validation, and test split can be accessed directly from the <code>dataset</code> as well</p>"},{"location":"tutorials/Node_label_tutorial/","title":"Access node labels for Dynamic Node Property Prediction","text":"In\u00a0[1]: Copied! <pre>from tgb.nodeproppred.dataset_pyg import PyGNodePropPredDataset\nfrom torch_geometric.loader import TemporalDataLoader\n</pre> from tgb.nodeproppred.dataset_pyg import PyGNodePropPredDataset from torch_geometric.loader import TemporalDataLoader <p>specifying the name of the dataset</p> In\u00a0[2]: Copied! <pre>name = \"tgbn-genre\"\n</pre> name = \"tgbn-genre\" In\u00a0[3]: Copied! <pre>dataset = PyGNodePropPredDataset(name=name, root=\"datasets\")\ntype(dataset)\n</pre> dataset = PyGNodePropPredDataset(name=name, root=\"datasets\") type(dataset) <pre>file found, skipping download\nDataset directory is  /mnt/f/code/TGB/tgb/datasets/tgbn_genre\nloading processed file\n</pre> Out[3]: <pre>tgb.nodeproppred.dataset_pyg.PyGNodePropPredDataset</pre> In\u00a0[4]: Copied! <pre>train_mask = dataset.train_mask\nval_mask = dataset.val_mask\ntest_mask = dataset.test_mask\n\n\ndata = dataset.get_TemporalData()\n\ntrain_data = data[train_mask]\nval_data = data[val_mask]\ntest_data = data[test_mask]\n\nbatch_size = 200\ntrain_loader = TemporalDataLoader(train_data, batch_size=batch_size)\nval_loader = TemporalDataLoader(val_data, batch_size=batch_size)\ntest_loader = TemporalDataLoader(test_data, batch_size=batch_size)\n</pre> train_mask = dataset.train_mask val_mask = dataset.val_mask test_mask = dataset.test_mask   data = dataset.get_TemporalData()  train_data = data[train_mask] val_data = data[val_mask] test_data = data[test_mask]  batch_size = 200 train_loader = TemporalDataLoader(train_data, batch_size=batch_size) val_loader = TemporalDataLoader(val_data, batch_size=batch_size) test_loader = TemporalDataLoader(test_data, batch_size=batch_size)  In\u00a0[5]: Copied! <pre>#query the timestamps for the first node labels\nlabel_t = dataset.get_label_time()\n\nfor batch in train_loader:\n    #access the edges in this batch\n    src, dst, t, msg = batch.src, batch.dst, batch.t, batch.msg\n    query_t = batch.t[-1]\n    # check if this batch moves to the next day\n    if query_t &gt; label_t:\n        # find the node labels from the past day\n        label_tuple = dataset.get_node_label(query_t)\n        # node labels are structured as a tuple with (timestamps, source node, label) format, label is a vector\n        label_ts, label_srcs, labels = (\n            label_tuple[0],\n            label_tuple[1],\n            label_tuple[2],\n        )\n        label_t = dataset.get_label_time()\n\n        #insert your code for backproping with node labels here\n</pre> #query the timestamps for the first node labels label_t = dataset.get_label_time()  for batch in train_loader:     #access the edges in this batch     src, dst, t, msg = batch.src, batch.dst, batch.t, batch.msg     query_t = batch.t[-1]     # check if this batch moves to the next day     if query_t &gt; label_t:         # find the node labels from the past day         label_tuple = dataset.get_node_label(query_t)         # node labels are structured as a tuple with (timestamps, source node, label) format, label is a vector         label_ts, label_srcs, labels = (             label_tuple[0],             label_tuple[1],             label_tuple[2],         )         label_t = dataset.get_label_time()          #insert your code for backproping with node labels here"},{"location":"tutorials/Node_label_tutorial/#access-node-labels-for-dynamic-node-property-prediction","title":"Access node labels for Dynamic Node Property Prediction\u00b6","text":"<p>This tutorial will show you how to access node labels and edge data for the node property prediction datasets in <code>tgb</code>.</p> <p>The source code is stored in <code>dataset_pyg.py</code> in <code>tgb/nodeproppred</code> folder</p> <p>This tutorial requires <code>Pytorch</code> and <code>PyG</code>, refer to <code>README.md</code> for installation instructions</p> <p>This tutorial uses <code>PyG TemporalData</code> object, however it is possible to use <code>numpy</code> arrays as well.</p> <p>see examples in <code>examples/nodeproppred</code> folder for more details.</p>"},{"location":"tutorials/Node_label_tutorial/#process-and-load-the-dataset","title":"Process and load the dataset\u00b6","text":"<p>if the dataset has been processed, it will be loaded from disc for fast access</p> <p>if the dataset has not been downloaded, it will be processed automatically</p>"},{"location":"tutorials/Node_label_tutorial/#train-validation-and-test-splits-with-dataloaders","title":"Train, Validation and Test splits with dataloaders\u00b6","text":"<p>spliting the edges into train, val, test sets and construct dataloader for each</p>"},{"location":"tutorials/Node_label_tutorial/#access-node-label-data","title":"Access node label data\u00b6","text":"<p>In <code>tgb</code>, the node label data are queried based on the nearest edge observed so far and retrieves the node label data for the corresponding day.</p> <p>Note that this is because the node labels often have different timestamps from the edges thus should be processed at the correct time in the edge stream.</p> <p>In the example below, we show how to iterate through the edges and retrieve the node labels of the corresponding time.</p>"}]}